import csv
import os
from tkinter import messagebox, scrolledtext
import pandas as pd
import ast
import re
import math
import unicodedata
import pyperclip
import tkinter as tk
from tkinter import ttk
import threading
from rapidfuzz import fuzz
import numpy as np
import textwrap


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GLOBAL HELPER  ‚Äì  build live noun-morphology lookup
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from functools import lru_cache

# Helper to determine whether a given string is a full Punjabi word
def is_full_word(s: str) -> bool:
    """Return ``True`` if *s* looks like a complete Punjabi word."""
    s = str(s).strip()
    # Words starting with a vowel matra are generally suffixes
    return len(s) > 1 and not ("\u0A3E" <= s[0] <= "\u0A4C")

# ‚îÄ‚îÄ Canonical ending-class labels for the dropdown ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CANONICAL_ENDINGS = [
    "NA",
    "‡®Æ‡©Å‡®ï‡®§‡®æ Ending",      # bare consonant
    "‡®ï‡©∞‡®®‡®æ Ending",       # ‚Äì‡®æ
    "‡®∏‡®ø‡®π‡®æ‡®∞‡©Ä Ending",     # ‚Äì‡®ø
    "‡®¨‡®ø‡®π‡®æ‡®∞‡©Ä Ending",     # ‚Äì‡©Ä
    "‡®π‡©ã‡®∞‡®æ Ending",       # ‚Äì‡©ã / ‚Äì‡®ì poetic
    "‡®â Ending",          # ‚Äì‡©Å
    "‡©Ç Ending",          # ‚Äì‡©Ç
]

# ------------------------------------------------------------------
#  FULL-WORD EXEMPLARS FOR EACH ENDING-CLASS
#  (trim / extend these lists whenever you like)
# ------------------------------------------------------------------

# ‚îÄ‚îÄ‚îÄ Canonical ‚Äúkeep‚Äù vowel for each ending-class ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
KEEP_CHAR = {
    "‡®Æ‡©Å‡®ï‡®§‡®æ Ending": "",
    "‡®ï‡©∞‡®®‡®æ Ending": ("‡®æ", "‡®Ü", "‡®ø‡®Ü"),
    "‡®∏‡®ø‡®π‡®æ‡®∞‡©Ä Ending": "‡®ø",
    "‡®¨‡®ø‡®π‡®æ‡®∞‡©Ä Ending": "‡©Ä",
    "‡®π‡©ã‡®∞‡®æ Ending": "‡©ã",
    "‡®â Ending": "‡©Å",
    "‡©Ç Ending": "‡©Ç",
}

ENDING_EXAMPLES = {
    "‡®Æ‡©Å‡®ï‡®§‡®æ Ending": [
        "‡®â‡®¶‡®ø‡®Ü‡®®‡©à","‡®â‡®™‡®æ‡®µ‡©Ä","‡®ì‡®Ö‡©∞‡®ï‡®æ‡®∞‡®ø","‡®Ö‡®ñ‡©Ä","‡®Ö‡®ñ‡®∞‡®æ","‡®Ü‡®π‡®∞",
        "‡®Ö‡®Æ‡©Å‡®≤","‡®Ö‡®Æ‡©Å‡®≤‡©Å","‡®Ö‡®µ‡®ø‡®ó‡®§‡©ã","‡®Ö‡©∞‡®ß‡©á","‡®Ö‡®π‡©∞‡®ï‡®æ‡®∞‡©Ä","‡®Ü‡®∏","‡®Ü‡®∏‡©à",
        "‡®â‡®§‡®Æ","‡®â‡®™‡®æ‡®á","‡®â‡®¶‡®Æ","‡®ï‡®¶‡®∞","‡®ú‡®π‡®æ‡®ú", "‡®¶‡®∞‡®¶","‡®Ö‡®®‡®æ‡®•‡®π",
        "‡®ï‡®∞‡®Æ","‡®ï‡®â‡®§‡®ï","‡®ö‡®∞‡®£","‡®ö‡®ø‡®§","‡®ß‡®∞‡®Æ","‡®®‡®¶‡®∞","‡®®‡®ø‡®∏‡®º‡®æ‡®®","‡®™‡®¶‡®Æ"
    ],

    "‡®ï‡©∞‡®®‡®æ Ending": [
        "‡®Ü‡®ó‡®ø‡®Ü","‡®§‡©ç‡®∞‡®ø‡®∏‡®®‡®æ","‡®¶‡©Å‡®¨‡®ø‡®ß‡®æ","‡®®‡®ø‡©∞‡®¶‡®æ","‡®∞‡®∏‡®®‡®æ","‡®∏‡®ñ‡©Ä‡®Ü","‡®∏‡®ø‡®∞‡©Ä‡®Ü","‡®ú‡®ø‡®π‡®¨‡®æ",
        "‡®ú‡®ø‡®π‡®µ‡©á","‡®Æ‡®æ‡®á‡®Ü","‡®≠‡®æ‡®à‡®Ü","‡®¨‡®π‡©Å‡®∞‡©Ä‡®Ü","‡®Æ‡®®‡©Ç‡®Ü","‡®®‡®ø‡®Æ‡®æ‡®£‡®ø‡®Ü","‡®®‡®ø‡®ó‡©Å‡®∞‡®ø‡®Ü",
        "‡®µ‡®°‡®≠‡®æ‡®ó‡©Ä‡®Ü","‡®µ‡®°‡®ø‡®Ü‡®à‡®Ü","‡®ö‡©∞‡®ó‡®ø‡®Ü‡®à‡®Ü","‡®ó‡©ã‡®™‡©Ä‡®Ü","‡®ï‡®π‡®æ‡®£‡©Ä‡®Ü","‡®ï‡©ú‡®õ‡©Ä‡®Ü","‡®ö‡®æ‡®ü‡©ú‡®ø‡®Ü",
        "‡®ñ‡®ü‡©Ä‡®Ü","‡®ó‡©Å‡®™‡®§‡®ß‡®æ","‡®¶‡©Å‡®π‡®æ‡®à‡®Ü","‡®ö‡©ú‡©ç‡®π‡®æ‡®à‡®Ü","‡®ò‡©ú‡©Ä‡®Ü","‡®∏‡®•‡®æ‡®∏‡©Ä‡®Ü","‡®ï‡®π‡®æ‡®£‡©Ä‡®Ü"
    ],

    "‡®∏‡®ø‡®π‡®æ‡®∞‡©Ä Ending": [
        "‡®ï‡®ø‡®∞‡®§‡®ø","‡®ö‡®ø‡®§‡®ø","‡®≠‡®ó‡®§‡®ø","‡®ó‡©ç‡®∞‡®π‡®ø","‡®™‡®∞‡®Æ‡®æ‡®§‡®Æ‡®ø","‡®ï‡®≤‡®™‡®ø","‡®∞‡®ø‡®¶‡®ø",
        "‡®ñ‡®∞‡®ö‡®ø","‡®®‡®∞‡®∏‡®ø","‡®ö‡®æ‡®∞‡®ø‡®§‡©ç‡®∞‡®ø","‡®Ö‡®ö‡®∞‡®ú‡®ø","‡®≤‡®π‡®ø‡®∞‡®ø","‡®¶‡©ç‡®∞‡®ø‡®∏‡®ü‡®ø","‡®∏‡©∞‡®ú‡©Ä‡®µ‡®®‡®ø",
        "‡®®‡®µ‡®ú‡®æ‡®§‡®ø","‡®Ö‡®ï‡®∏‡®º‡®ø","‡®Ö‡®∞‡®∏‡®ø‡®Ö","‡®∏‡®ø‡®ñ‡®ø","‡®∏‡®ø‡®ñ‡®ø‡®Ü","‡®ú‡®™‡®§‡®ø","‡®∏‡©ç‡®∞‡®ø‡®∏‡®ü‡®ø","‡®®‡®ø‡®∞‡®Æ‡®§‡®ø",
        "‡®¶‡©á‡®µ‡®§‡®ø","‡®Ü‡®¶‡®ø‡®∏‡®ü‡®ø","‡®Ü‡®∏‡®ï‡®§‡®ø","‡®â‡®∞‡®ß‡®ø‡®ï‡®ø","‡®ï‡®≤‡®Æ‡®ø","‡®®‡®ø‡®ú‡®Æ‡®ø","‡®∏‡©∞‡®ó‡®§‡®ø"
    ],

    "‡®¨‡®ø‡®π‡®æ‡®∞‡©Ä Ending": [
        "‡®®‡®ø‡®∞‡®ó‡©Å‡®£‡©Ä","‡®∏‡©Å‡®ú‡®æ‡®£‡©Ä","‡®≠‡®ó‡®§‡©Ä","‡®¶‡®ø‡®≤‡®ó‡©Ä","‡®¨‡©Ä‡®¨‡©Ä","‡®∏‡®æ‡®ï‡©Ä","‡®ï‡®π‡®æ‡®£‡©Ä",
        "‡®ï‡®¨‡©Ä‡®∞‡©Ä","‡®∏‡®¶‡©Ä‡®ï‡©Ä","‡®™‡©ç‡®∞‡©Ä‡®§‡©Ä","‡®Æ‡®π‡®ø‡®≤‡©Ä","‡®Æ‡®æ‡®§‡©Ä","‡®¨‡®≤‡®µ‡©Ä","‡®°‡©∞‡®°‡©Ä","‡®Æ‡®ø‡®≤‡®®‡©Ä",
        "‡®∏‡®ö‡®æ‡®à","‡®∞‡©Å‡®∏‡®º‡®§‡©Ä","‡®Ö‡®≤‡®∏‡©Ä","‡®¶‡®ø‡©∞‡®¶‡©Ä","‡®≤‡®ø‡®ñ‡®§‡©Ä‡®Ç","‡®ß‡©Ä‡®∞‡®ú‡©Ä","‡®ï‡©ç‡®∞‡®ø‡®™‡®æ‡®≤‡©Ä",
        "‡®ï‡®ø‡®∞‡®™‡®æ‡®à","‡®ó‡©ç‡®∞‡®π‡®£‡©Ä","‡®®‡®ø‡®Æ‡®æ‡®£‡©Ä"
    ],

    "‡®π‡©ã‡®∞‡®æ Ending": [
        "‡®ì‡®π‡©Å","‡®ì‡®π","‡®ì‡®π‡©Ä","‡®ì‡®π‡©ã","‡®ì‡®Ü","‡®ì‡®Ü‡®π","‡®ì‡®à‡®è","‡®ì‡®á","‡®ì‡®à","‡®ì‡®è"
    ],

    "‡®â Ending": [
        "‡®≤‡®ñ‡©Å","‡®≤‡®õ‡©Å","‡®≤‡®æ‡®ñ‡©Å","‡®Ö‡©∞‡®∏‡©Å","‡®ï‡®≤‡®§‡©Å","‡®ñ‡®æ‡®ï‡©Å","‡®Ö‡®ï‡®§‡©Å","‡®Ö‡®Æ‡®§‡©Å","‡®§‡®™‡©Å",
        "‡®∞‡®ï‡®§‡©Å","‡®≠‡®µ‡®®‡©Å","‡®ï‡©∞‡®§‡©Å","‡®∏‡®§‡©Å","‡®∏‡®§‡©Å","‡®®‡®ø‡®∏‡©Å","‡®ï‡®â‡®®‡©Å","‡®Æ‡®®‡©Å","‡®∏‡®®‡©Å",
        "‡®â‡®§‡®™‡®§‡©Å","‡®Ü‡®¶‡®§‡©Å","‡®¶‡®Ø‡©Å","‡®¶‡®®‡©Å","‡®ï‡®∞‡®Æ‡©Å","‡®ï‡®∞‡®§‡©Å","‡®∞‡®â","‡®ó‡®â","‡®ò‡®â","‡®ö‡®π‡©Å"
    ],

    "‡©Ç Ending": [
        "‡®Æ‡©Ç‡®≤‡©Ç","‡®∏‡©Ç‡®≤‡©Ç","‡®≠‡©Ç‡®≤‡©Ç","‡®∂‡©Ç‡®≤‡©Ç","‡®∞‡©Ç‡®™‡©Ç","‡®π‡®ø‡®∞‡®¶‡©Ç","‡®¶‡®ø‡®≤‡©Ç","‡®Æ‡®ø‡®§‡©ç‡®∞‡©Ç","‡®ß‡®∞‡®§‡©Ç",
        "‡®∏‡®µ‡®æ‡®∞‡©Ç"
    ],
}

# ‚îÄ‚îÄ‚îÄ Function that turns ENDING_EXAMPLES into (Full, Base, Suffix) tuples ‚îÄ‚îÄ

def build_example_bases(
    csv_path: str = "1.1.1_birha.csv",
    ending_examples: dict[str, list[str]] = None,
    keep_char: dict[str, str] = None,
) -> dict[str, list[tuple[str, str, str]]]:
    if ending_examples is None or keep_char is None:
        raise ValueError("Pass ENDING_EXAMPLES and KEEP_CHAR")


    df = (pd.read_csv(csv_path).rename(columns={"Vowel Ending": "\ufeffVowel Ending", "Word Type": "Type"}).fillna("")
            .assign(**{
                "Word Root": lambda d: (
                    d["Word Root"]
                      .str.replace("‡®ï‡®®‡®æ‡©± Ending","‡®ï‡©∞‡®®‡®æ Ending", regex=False)
                      .str.replace("‡®ï‡®®‡®æ Ending","‡®ï‡©∞‡®®‡®æ Ending", regex=False)
                )
            }))

    # map: same 5-feature key ‚Üí list of 1-glyph endings
    suffix_lookup = {}
    small = df[~df["\ufeffVowel Ending"].apply(is_full_word)]
    for _, r in small.iterrows():
        k = (r["Word Root"], r["Type"], r["Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£"],
             r["Gender / ‡®≤‡®ø‡©∞‡®ó"], r["Number / ‡®µ‡®ö‡®®"])
        suffix_lookup.setdefault(k, []).append(r["\ufeffVowel Ending"].strip())

    result = {}
    for label, wordlist in ending_examples.items():
        canon = keep_char.get(label, "")
        canon_set = set(canon) if isinstance(canon, (list, tuple, set)) else {canon}
        triples = []
        for full in wordlist:
            row = df[(df["\ufeffVowel Ending"].str.strip() == full) &
                     (df["Word Root"] == label)]
            if row.empty:
                triples.append((full, full, ""))
                continue
            r = row.iloc[0]
            k = (r["Word Root"], r["Type"], r["Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£"],
                 r["Gender / ‡®≤‡®ø‡©∞‡®ó"], r["Number / ‡®µ‡®ö‡®®"])
            base, suf = full, ""
            for cand in suffix_lookup.get(k, []):
                cand = cand.strip()
                if cand in canon_set or cand == "":
                    continue
                if full.endswith(cand):
                    base = full[:-len(cand)]
                    suf = cand
                    break
               
            if label == "‡®Æ‡©Å‡®ï‡®§‡®æ Ending" and base == full and len(full) > 1:
                last = full[-1]
                # Unicode range for Gurmukhi matras (U+0A3E‚ÄìU+0A4C)
                if "\u0A3E" <= last <= "\u0A4C":
                    # strip that final matra as a true detachment
                    base, suf = full[:-1], last
            
            triples.append((full, base, suf))
        result[label] = triples
    return result

EXAMPLE_BASES = build_example_bases(
    csv_path="1.1.1_birha.csv",
    ending_examples=ENDING_EXAMPLES,
    keep_char=KEEP_CHAR,
)


@lru_cache(maxsize=1)
def build_noun_map(csv_path="1.1.1_birha.csv"):
    """
    Returns a nested dict:
        noun_map[ending][gender][number][case] -> [list of attested forms]
    The loader also normalises stray spaces & typo-variants so look-ups never fail
    due to invisible characters.
    """
    df = (
        pd.read_csv(csv_path)
          .query("Type.str.startswith('Noun')", engine="python")
          .fillna("NA")
          .rename(columns={
              "Vowel Ending"        : "ending",
              "Number / ‡®µ‡®ö‡®®"         : "num",
              "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£"     : "case",
              "Gender / ‡®≤‡®ø‡©∞‡®ó"         : "gender",
              "Word Root"           : "root",
          })
    )

    # --- normalise whitespace & common misspellings -----------------
    for c in ("ending", "gender", "num", "case", "root"):
        df[c] = (
            df[c].astype(str)
                 .str.replace(r"\s+", " ", regex=True)   # collapse weird spaces
                 .str.strip()                            # trim front/back
        )

    # unify the Kanna spelling
    df["root"] = df["root"].str.replace("‡®ï‡®®‡®æ‡©± Ending", "‡®ï‡©∞‡®®‡®æ Ending")

    # --- build nested dictionary ------------------------------------
    by_end = {}
    for ending, g1 in df.groupby("ending"):
        g_dict = {}
        for gender, g2 in g1.groupby("gender"):
            n_dict = {}
            for num, g3 in g2.groupby("num"):
                case_dict = (
                    g3.groupby("case")["ending"]     # store the surface form
                      .apply(list)                   # list of forms
                      .to_dict()
                )
                n_dict[num] = case_dict
            g_dict[gender] = n_dict
        by_end[ending] = g_dict
    return by_end


class GrammarApp:
    def __init__(self, root):
        """
        Initialize the application and display the dashboard as the main window.
        """
        # ------------------------------------------------------------------
        # ‚îÄ‚îÄ‚îÄ 1.  BASIC ROOT‚ÄëWINDOW SETUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ------------------------------------------------------------------
        self.root = root
        self.root.title("Dashboard")
        self.root.configure(bg="light gray")
        self.root.state("zoomed")        # maximise on Windows
      
        # ------------------------------------------------------------------
        # ‚îÄ‚îÄ‚îÄ 2.  APP‚ÄëWIDE STATE VARIABLES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ------------------------------------------------------------------
        self.number_var  = tk.StringVar(value="NA")
        self.gender_var  = tk.StringVar(value="NA")
        self.pos_var     = tk.StringVar(value="NA")

        self.new_entries                   = []
        self.accumulated_pankti            = ""
        self.accumulated_meanings          = []
        self.accumulated_grammar_matches   = []
        self.accumulated_finalized_matches = []
        self.current_pankti                = ""
        self.match_vars                    = []
        self.all_matches                   = []
        self.all_new_entries               = []   # global accumulator

        # word‚Äëby‚Äëword navigation
        self.current_word_index = 0
        self.pankti_words       = []

        # per-verse repeat-word note tracking
        self._repeat_note_shown = set()
        self._suppress_repeat_notes_for_verse = False
        self._use_inline_literal_banner = True
        self._always_show_literal_banner_frame = False
        self._last_literal_verse_key = None
        self._first_repeat_token = None
        self._last_dropdown_verse_key = None

        self._LITERAL_NOTE_TEXT = (
            "In literal analysis: This word appears multiple times in this verse. "
            "The highlighted grammar options reflect your past selections for this word "
            "(or close matches) to encourage consistency. They‚Äôre suggestions, not mandates‚Äî"
            "adjust if the current context differs."
        )

        # ------------------------------------------------------------------
        # ‚îÄ‚îÄ‚îÄ 3.  DATA LOAD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ------------------------------------------------------------------
        self.grammar_data   = self.load_grammar_data("1.1.1_birha.csv")
        self.dictionary_data = pd.read_csv(
            "1.1.2 Grammatical Meanings Dictionary.csv",
            encoding="utf-8"
        )

        # ------------------------------------------------------------------
        # ‚îÄ‚îÄ‚îÄ 4.  LAUNCH DASHBOARD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        # ------------------------------------------------------------------
        self.show_dashboard()
    def _norm_get(self, d, key):
        """Unified getter that tolerates legacy field names."""
        if key == "\ufeffVowel Ending" or key == "Vowel Ending":
            return d.get("\ufeffVowel Ending") or d.get("Vowel Ending")
        if key == "Type" or key == "Word Type":
            return d.get("Type") or d.get("Word Type")
        return d.get(key)

    # TODO: Reuse in user_input(...) and prompt_save_results(...) for consistent comparisons.
    def _norm_tok(self, t: str) -> str:
        """Normalize token via NFC; drop dandas, zero-width spaces, ZWJ/ZWNJ, trailing digits & punctuation."""
        t = unicodedata.normalize("NFC", t.strip())
        t = re.sub(r"[‡•§‡••]", "", t)  # danda/double-danda
        # remove ZERO WIDTH SPACE, ZWNJ, ZWJ
        t = t.replace("\u200b", "").replace("\u200c", "").replace("\u200d", "")
        t = re.sub(r"[\d\u0A66-\u0A6F.,;:!?\"'‚Äî‚Äì-]+$", "", t)  # trailing digits (Latin+Gurmukhi) & punct
        return t

    def _verse_key(self, verse_text: str) -> str:
        """NFC + collapse spaces + remove danda variations; used for verse-scoped de-dupe keys."""
        cleaned = re.sub(r"[‡•§‡••]", "", verse_text).strip()
        cleaned = re.sub(r"\s+", " ", cleaned)
        return unicodedata.normalize("NFC", cleaned)

    def _banner_wraplength(self, win=None) -> int:
        """Return a wraplength tuned to the window width (clamped 600‚Äì900)."""
        try:
            target = win or (self.match_window if hasattr(self, "match_window") else None)
            if target and target.winfo_exists():
                target.update_idletasks()
                w = target.winfo_width()
                return max(600, min(900, w - 120))
        except Exception:
            pass
        return 900

    def _modal_wraplength(self, win=None) -> int:
        """Return a wraplength tuned for the small modal (clamped 360‚Äì520)."""
        try:
            target = win or (self.root if hasattr(self, "root") else getattr(self, "match_window", None))
            if target and target.winfo_exists():
                target.update_idletasks()
                w = target.winfo_width()
                return max(360, min(520, w - 200))
        except Exception:
            pass
        return 400

    def _on_match_window_resize(self, event=None):
        """Resize handler to reflow the inline banner text, if present."""
        try:
            if hasattr(self, "literal_note_body") and self.literal_note_body and self.literal_note_body.winfo_exists():
                self.literal_note_body.config(wraplength=self._banner_wraplength(self.match_window))
        except Exception:
            pass

    def _has_repeat(self, norm_words, norm_target: str) -> bool:
        """Return True iff *norm_target* appears at least twice in ``norm_words``.

        ``norm_words`` is expected to be a list of pre-normalized tokens so this
        helper can operate without re-normalizing each time it is called.
        """
        if not norm_target:
            return False
        return norm_words.count(norm_target) >= 2

    def _maybe_show_repeat_important_note(self, word, occurrence_idx, verse_norm):
        """Show an explanatory note for repeated words within a verse."""
        if occurrence_idx < 1 or self._suppress_repeat_notes_for_verse:
            return
        norm_word = self._norm_tok(word)
        if not norm_word:
            return
        norm_verse = self._verse_key(verse_norm)
        key = (norm_verse, norm_word, "second")
        if key in self._repeat_note_shown:
            return
        self._repeat_note_shown.add(key)

        top = tk.Toplevel(self.root)
        top.title("Important Note ‚Äî Literal Analysis")
        top.configure(bg='AntiqueWhite')
        top.transient(self.root)
        top.grab_set()

        body_lbl = tk.Label(
            top,
            text=self._LITERAL_NOTE_TEXT,
            bg='AntiqueWhite',
            wraplength=self._modal_wraplength(top),
            justify=tk.LEFT,
            font=('Arial', 12)
        )
        body_lbl.pack(padx=20, pady=(15,5))
        # Reflow text on modal resize
        try:
            top.bind("<Configure>", lambda e: body_lbl.config(wraplength=self._modal_wraplength(top)))
        except Exception:
            pass

        dont_var = tk.BooleanVar(value=False)
        tk.Checkbutton(
            top,
            text="Don't show again for this verse",
            variable=dont_var,
            bg='AntiqueWhite',
            font=('Arial', 11)
        ).pack(pady=(0,10))

        def _commit_and_close():
            try:
                if dont_var.get():
                    self._suppress_repeat_notes_for_verse = True
            except Exception:
                pass
            top.destroy()

        ok_btn = tk.Button(
            top,
            text="OK",
            command=_commit_and_close,
            font=('Arial', 12, 'bold'),
            bg='navy',
            fg='white',
            padx=10,
            pady=5
        )
        ok_btn.pack(pady=(0,15))
        try:
            ok_btn.focus_set()
            top.bind("<Return>", lambda e: _commit_and_close(), add="+")
            top.bind("<Escape>", lambda e: _commit_and_close(), add="+")
        except Exception:
            pass

        def _on_close():
            _commit_and_close()
        top.protocol("WM_DELETE_WINDOW", _on_close)

        top.update_idletasks()
        w, h = top.winfo_width(), top.winfo_height()
        x = self.root.winfo_x() + (self.root.winfo_width() - w)//2
        y = self.root.winfo_y() + (self.root.winfo_height() - h)//2
        top.geometry(f"{w}x{h}+{x}+{y}")
        top.wait_window()

    def show_dashboard(self):
        """Creates the dashboard interface directly in the main root window."""
        # Clear any existing widgets from the root
        for widget in self.root.winfo_children():
            widget.destroy()

        # Set up the dashboard appearance in the root window
        self.root.title("Dashboard")
        self.root.configure(bg='light gray')
        self.root.state("zoomed")  # Maximize the window

        # Dashboard header label
        header = tk.Label(
            self.root,
            text="Welcome to Gurbani Software Dashboard",
            font=('Arial', 18, 'bold'),
            bg='dark slate gray',
            fg='white'
        )
        header.pack(fill=tk.X, pady=20)

        # Create a frame to hold dashboard buttons
        button_frame = tk.Frame(self.root, bg='light gray')
        button_frame.pack(expand=True)

        # New Button to open the Verse Analysis Dashboard
        verse_analysis_btn = tk.Button(
            button_frame,
            text="Verse Analysis Dashboard",
            font=('Arial', 14, 'bold'),
            bg='dark cyan',
            fg='white',
            padx=20,
            pady=10,
            command=self.launch_verse_analysis_dashboard
        )
        verse_analysis_btn.pack(pady=10)

        # Button to open the Grammar‚ÄëDB Update window
        grammar_update_btn = tk.Button(
            button_frame,
            text="Grammar DB Update",
            font=('Arial', 14, 'bold'),
            bg='teal',
            fg='white',
            padx=20,
            pady=10,
            command=self.launch_grammar_update_dashboard
        )
        grammar_update_btn.pack(pady=10)

        # Placeholder for future features (e.g., Grammar Correction)
        future_btn = tk.Button(
            button_frame,
            text="Upcoming Feature: Grammar Correction",
            font=('Arial', 14, 'bold'),
            bg='gray',
            fg='white',
            padx=20,
            pady=10,
            state=tk.DISABLED
        )
        future_btn.pack(pady=10)

    def launch_grammar_update_dashboard(self):
        win = tk.Toplevel(self.root)
        win.title("Grammar Database Update")
        win.configure(bg='#e0e0e0')  # light neutral background
        win.state("zoomed")

        # ‚Äî Header Bar ‚Äî
        header = tk.Frame(win, bg='#2f4f4f', height=60)
        header.pack(fill=tk.X)
        tk.Label(
            header,
            text="Grammar Database Update",
            font=('Arial', 20, 'bold'),
            bg='#2f4f4f',
            fg='white'
        ).place(relx=0.5, rely=0.5, anchor='center')

        # ‚Äî Separator ‚Äî
        sep = tk.Frame(win, bg='#cccccc', height=2)
        sep.pack(fill=tk.X)

        # ‚Äî Navigation Buttons ‚Äî
        nav = tk.Frame(win, bg='#e0e0e0')
        nav.pack(pady=30)
        btn_kwargs = dict(
            font=('Arial', 14, 'bold'),
            width=20,
            padx=10, pady=10,
            relief='flat',
            activebackground='#007d7d',
            bg='#008c8c', fg='white'
        )

        btn_verse = tk.Button(
            nav, text="Assess by Verse", **btn_kwargs,
            command=self.launch_verse_assessment
        )
        btn_verse.grid(row=0, column=0, padx=20)

        btn_word = tk.Button(
            nav, text="Assess by Word (coming soon)", **btn_kwargs,
            state=tk.DISABLED, disabledforeground='#666666'
        )
        btn_word.grid(row=0, column=1, padx=20)

        # ‚Äî Instruction / Description ‚Äî
        instr = (
            "Choose ‚ÄúAssess by Verse‚Äù to look up verses and refine grammar entries.\n"
            "The ‚ÄúAssess by Word‚Äù workflow is coming in the next release."
        )
        tk.Label(
            win, text=instr,
            font=('Arial', 16),
            bg='#e0e0e0', fg='#333333',
            justify='center', wraplength=800
        ).pack(pady=20)

        # ‚Äî Bottom Back Button ‚Äî
        bottom = tk.Frame(win, bg='#e0e0e0')
        bottom.pack(side=tk.BOTTOM, pady=30)
        back_btn = tk.Button(
            bottom,
            text="‚Üê Back to Dashboard",
            font=('Arial', 14),
            bg='#2f4f4f', fg='white',
            activebackground='#3f6f6f',
            padx=20, pady=10,
            command=self.show_dashboard
        )
        back_btn.pack()

        # Optional: make ESC key close this window
        win.bind("<Escape>", lambda e: win.destroy())

    def launch_verse_assessment(self):
        """Window for searching & selecting verses to assess grammar using a 2‚Äëcolumn card layout."""
        win = tk.Toplevel(self.root)
        win.title("Assess by Verse")
        win.configure(bg='light gray')
        win.state("zoomed")
        
        # ‚Äî Optional page‚Äêwide heading ‚Äî
        tk.Label(
            win,
            text="Select a Verse to Refine Grammar Entries",
            font=("Arial", 20, "bold"),
            bg="dark slate gray",
            fg="white",
            pady=10
        ).pack(fill=tk.X)

        # keep track of which card is selected
        self._selected_verse_idx = tk.IntVar(value=-1)

        # ‚Äî Top frame: entry + Search button ‚Äî
        top = tk.Frame(win, bg='light gray')
        top.pack(fill=tk.X, padx=20, pady=15)
        tk.Label(top, text="Enter Verse:", font=("Arial", 16), bg='light gray').pack(side=tk.LEFT)
        self._verse_var = tk.StringVar()
        tk.Entry(top, textvariable=self._verse_var, font=("Arial", 16))\
        .pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(10,10))
        tk.Button(
            top, text="Search", font=("Arial", 16, "bold"),
            bg='dark cyan', fg='white',
            command=self._populate_cards
        ).pack(side=tk.LEFT)

        # ‚Äî Middle frame: scrollable canvas + 2‚Äëcolumn grid of ‚Äúcards‚Äù ‚Äî
        middle = tk.Frame(win, bg='light gray')
        middle.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)

        canvas = tk.Canvas(middle, bg='light gray', highlightthickness=0)
        vsb    = tk.Scrollbar(middle, orient="vertical", command=canvas.yview)
        canvas.configure(yscrollcommand=vsb.set)
        vsb.pack(side="right", fill="y")
        canvas.pack(side="left", fill="both", expand=True)

        # This frame goes inside the canvas and will hold our cards
        self._cards_frame = tk.Frame(canvas, bg='light gray')

        # create_window with anchor="n" so its x coordinate is the top-center of cards_frame
        cards_window = canvas.create_window((0, 0), window=self._cards_frame, anchor="n")

        # configure two equal‚Äëweight columns for 2‚Äëcolumn layout
        self._cards_frame.grid_columnconfigure(0, weight=1, minsize=450)
        self._cards_frame.grid_columnconfigure(1, weight=1, minsize=450)

        # keep scrollregion up to date
        def _on_cards_configure(event):
            canvas.configure(scrollregion=canvas.bbox("all"))
        self._cards_frame.bind("<Configure>", _on_cards_configure)

        # **New:** whenever the canvas resizes, recenter cards_frame horizontally
        def _on_canvas_resize(event):
            canvas.coords(cards_window, event.width // 2, 0)
        canvas.bind("<Configure>", _on_canvas_resize)

        # ‚Äî Bottom frame: navigation buttons ‚Äî
        bottom = tk.Frame(win, bg='light gray')
        bottom.pack(fill=tk.X, padx=20, pady=15)
        tk.Button(
            bottom, text="‚Äπ Back", font=("Arial", 14),
            bg='gray', fg='white', command=win.destroy
        ).pack(side=tk.LEFT)
        tk.Button(
            bottom, text="Back to Dashboard", font=("Arial", 14),
            bg='gray', fg='white', command=self.show_dashboard
        ).pack(side=tk.LEFT, padx=5)
        tk.Button(
            bottom, text="Next ‚Üí", font=("Arial", 14, "bold"),
            bg='dark cyan', fg='white',
            command=lambda: self.proceed_to_word_assessment(self._selected_verse_idx.get())
        ).pack(side=tk.RIGHT)

    def _populate_cards(self):
        """Perform the verse search, filter & then render up to 10 cards in two columns."""
        # first, clear any existing cards
        for w in self._cards_frame.winfo_children():
            w.destroy()

        # 1) run search & filter
        query = self._verse_var.get().strip()
        headers, all_matches = self.match_sggs_verse(query)
        filtered = [m for m in all_matches if m.get("Score",0) >= 60.0][:10]
        # remember these for the ‚ÄúNext ‚Üí‚Äù step
        self._last_filtered = filtered

        # reset selection
        self._selected_verse_idx.set(-1)

        # 2) render each card
        for idx, m in enumerate(filtered):
            row, col = divmod(idx, 2)
            card = tk.Frame(
                self._cards_frame,
                bd=1,
                relief="solid",
                bg="white",
                padx=8,
                pady=8
            )
            card.grid(
                row=row,
                column=col,
                padx=10, pady=10,
                sticky="nsew"
            )

            # a little radiobutton at top-left for selection
            rb = tk.Radiobutton(
                card,
                variable=self._selected_verse_idx,
                value=idx,
                bg="white",
                activebackground="white"
            )
            rb.place(x=4, y=4)

            # the verse itself, wrapped
            tk.Label(
                card,
                text=m.get("Verse","").strip(),
                font=("Arial", 14, "bold"),
                wraplength=500,
                justify="center",
                bg="white"
            ).pack(pady=(8,4))

            # metadata line
            # build a list of (label, key) pairs
            fields = [
                ("Raag",   "Raag (Fixed)"),
                ("Writer", "Writer (Fixed)"),
                ("Bani",   "Bani Name"),
                ("Page",   "Page Number"),
            ]

            meta_parts = []
            for label, key in fields:
                v = m.get(key)
                # skip if missing or NaN
                if v is None or (isinstance(v, float) and math.isnan(v)):
                    continue
                meta_parts.append(f"{label}: {v}")

            # always include the match%
            meta_parts.append(f"Match: {m.get('Score',0):.1f}%")

            # join with separators
            meta = "   |   ".join(meta_parts)

            tk.Label(
                card,
                text=meta,
                font=("Arial", 12),
                bg="white"
            ).pack()

        # 3) force a canvas update of its scroll region
        self._cards_frame.update_idletasks()
        self._cards_frame.master.configure(
            scrollregion=self._cards_frame.master.bbox("all")
        )

    def show_translation_input(self):
        win = tk.Toplevel(self.root)
        win.title("Paste Darpan Translation")
        win.configure(bg='light gray')
        # bump default size up so buttons are always visible
        win.state("zoomed")
        win.transient(self.root)
        win.grab_set()

        # ‚Äî Heading ‚Äî
        tk.Label(
            win,
            text=self.selected_verse_text,
            font=("Arial", 20, "bold"),
            bg="light gray",
            wraplength=900,
            justify="center",
            pady=10
        ).pack(fill=tk.X, padx=20, pady=(15,10))

        # ‚Äî Translation area ‚Äî
        tf = tk.LabelFrame(
            win,
            text="Established Darpan Translation",
            font=("Arial", 14, "bold"),
            bg='light gray',
            fg='black',
            padx=10, pady=10
        )
        tf.pack(fill=tk.BOTH, expand=False, padx=20, pady=(0,15))

        self._translation_text = tk.Text(
            tf, wrap=tk.WORD, font=("Arial", 13),
            height=8, padx=5, pady=5
        )
        self._translation_text.pack(fill=tk.BOTH, expand=False)

        # ‚Äî Word‚Äêselection area ‚Äî
        wf = tk.LabelFrame(
            win,
            text="Select Words to Assess Grammar",
            font=("Arial", 14, "bold"),
            bg='light gray',
            fg='black',
            padx=10, pady=10
        )
        wf.pack(fill=tk.BOTH, expand=False, padx=20, pady=(0,15))

        # select/deselect all
        self._select_all_words_var = tk.BooleanVar(value=False)
        tk.Checkbutton(
            wf,
            text="Select/Deselect All Words",
            variable=self._select_all_words_var,
            bg="light gray",
            font=("Arial", 12, "italic"),
            command=self._toggle_all_word_selection
        ).pack(anchor="w", pady=(0,10))

        # scrollable word grid
        canvas = tk.Canvas(wf, bg='light gray', highlightthickness=0)
        scrollbar = tk.Scrollbar(wf, orient="vertical", command=canvas.yview)
        word_frame = tk.Frame(canvas, bg='light gray')
        canvas.configure(yscrollcommand=scrollbar.set)

        scrollbar.pack(side="right", fill="y")
        canvas.pack(side="left", fill="both", expand=False)
        canvas.create_window((0,0), window=word_frame, anchor="nw")

        def _on_wf_resize(event):
            canvas.configure(scrollregion=canvas.bbox("all"))
        word_frame.bind("<Configure>", _on_wf_resize)

        # lay out each word
        self._word_selection_vars = []

        # 1) grab the verse text, remove any trailing danda symbols:
        verse_text = self.selected_verse_text.strip().rstrip('‡•• ').strip()

        # 2) split into words (now ‚Äú‡••‚Äù won‚Äôt appear as its own token)
        words = verse_text.split()

        # 3) build your checkboxes off `words` instead of the raw text:
        for i, w in enumerate(words):
            var = tk.BooleanVar(value=False)
            chk = tk.Checkbutton(
                word_frame,
                text=w,
                variable=var,
                bg='light gray',
                font=('Arial', 12),
                wraplength=120,
                anchor='w',
                justify='left'
            )
            chk.grid(row=i//4, column=i%4, sticky='w', padx=5, pady=3)
            self._word_selection_vars.append((var, w))

        # ‚Äî Bottom buttons ‚Äî
        btn_frame = tk.Frame(win, bg="light gray")
        btn_frame.pack(fill=tk.X, padx=20, pady=20)

        tk.Button(
            btn_frame,
            text="‚Üê Back to Verse Search",
            font=("Arial", 12),
            bg="gray",
            fg="white",
            command=win.destroy,
            padx=15, pady=8
        ).pack(side=tk.LEFT)

        tk.Button(
            btn_frame,
            text="Submit Translation ‚Üí",
            font=("Arial", 12, "bold"),
            bg="dark cyan",
            fg="white",
            command=lambda: self._on_translation_submitted(win),
            padx=15, pady=8
        ).pack(side=tk.RIGHT)

    def proceed_to_word_assessment(self, idx):
        # grab the metadata dict from the last search
        self.selected_verse_meta = self._last_filtered[idx]
        self.selected_verse_text = self.selected_verse_meta["Verse"]
        # now pop up the translation‚Äêpaste window
        self.show_translation_input()

    def process_next_word_assessment(self):
        if self.current_queue_pos >= len(self.grammar_queue):
            return self.finish_and_prompt_save()

        idx, word = self.grammar_queue[self.current_queue_pos]
        self.current_word_index = idx
        self.user_input_grammar(word, self.current_translation, idx)

    def _on_translation_submitted(self, win):
        # 1) grab and validate the translation itself
        text = self._translation_text.get("1.0", tk.END).strip()
        if not text:
            messagebox.showwarning("No Translation", "Please paste a translation before submitting.")
            return
        self.current_translation = text

        # 2) capture exactly which indices the user checked
        #    (you built self._word_selection_vars = [(var, word), ...] in show_translation_input)
        selected_idxs = [
            idx for idx, (var, _) in enumerate(self._word_selection_vars)
            if var.get()
        ]
        if not selected_idxs:
            messagebox.showwarning("Nothing Selected", "Please select at least one word to assess.")
            return
        self._selected_word_indices = selected_idxs

        # 3) tear down and hand off to your queue initializer
        win.destroy()
        self.initialize_grammar_queue()
        # ‚Üê NO MORE direct call to process_next_word_assessment() here,
        #     initialize_grammar_queue() will immediately invoke it.

    def initialize_grammar_queue(self):
        """
        After the user has pasted their translation, split the verse
        into words and build the queue of those words the user selected
        for grammar assessment. Then immediately start the first word.
        """
        # split the verse text
        words = self.selected_verse_text.strip().split()

        # collect exactly those indices the user checked in show_translation_input()
        # (you should have created a tk.BooleanVar list self._word_vars there)
        selected_indices = self._selected_word_indices

        # build the queue
        self.grammar_queue = [
            (i, words[i]) for i in selected_indices
        ]
        self.grammar_meanings = []        # ‚Üê NEW: clear out any old entries
        self.current_queue_pos = 0

        if not self.grammar_queue:
            messagebox.showinfo("Nothing Selected",
                "You didn‚Äôt select any words for grammar assessment.")
            return

        # **IMMEDIATELY** start your per-word flow
        self.process_next_word_assessment()

    def _toggle_all_word_selection(self):
        """Called by the top ‚ÄòSelect/Deselect All Words‚Äô checkbox."""
        val = self._select_all_words_var.get()
        for var, _ in getattr(self, "_word_selection_vars", []):
            var.set(val)

    def user_input_grammar(self, word, translation, index):
        """
        Pop up a window to collect grammar info for one word:
        - shows full verse with the `index`th word highlighted
        - shows the Darpan translation
        - left pane: dictionary meanings
        - right pane: Number/Gender/POS radio buttons + Expert-Prompt button
        - bottom row: Back / Skip / Submit
        """
        win = tk.Toplevel(self.root)
        win.title(f"Assess Grammar: {word}")
        win.configure(bg='light gray')
        # give a reasonable size so buttons show up
        win.state("zoomed")
        win.resizable(True, True)

        # 1) Verse display + highlight
        vf = tk.Frame(win, bg='light gray')
        vf.pack(fill=tk.X, padx=20, pady=(20,10))
        td = tk.Text(vf, wrap=tk.WORD, bg='light gray',
                     font=('Arial', 24), height=1, bd=0)
        td.pack(fill=tk.X)
        td.insert('1.0', self.selected_verse_text)
        td.tag_add('center', '1.0', 'end')
        td.tag_configure('center', justify='center')
        # highlight the word
        words = self.selected_verse_text.split()
        start = sum(len(w)+1 for w in words[:index])
        end   = start + len(words[index])
        td.tag_add('highlight', f'1.{start}', f'1.{end}')
        td.tag_configure('highlight',
                         font=('Arial',24,'bold'),
                         foreground='blue')
        td.config(state=tk.DISABLED)

        # 2) Translation LabelFrame
        tf = tk.LabelFrame(win, text="Darpan Translation",
                           font=('Arial',16,'bold'),
                           bg='light gray', fg='black',
                           padx=10, pady=10)
        tf.pack(fill=tk.BOTH, padx=20, pady=(0,15))
        trans = tk.Text(tf, wrap=tk.WORD, font=('Arial',14),
                        height=2, bd=0)
        trans.insert('1.0', translation)
        trans.config(state=tk.DISABLED)
        trans.pack(fill=tk.BOTH, expand=False)

        # Prepare vars for grammar options
        # Default to ‚ÄúUnknown‚Äù (NA)
        self.number_var = tk.StringVar(value="NA")
        self.gender_var = tk.StringVar(value="NA")
        self.pos_var    = tk.StringVar(value="NA")

        # 3+4) Split pane: left=meanings, right=options
        split = tk.PanedWindow(win, orient=tk.HORIZONTAL, bg='light gray')
        split.pack(fill=tk.BOTH, expand=False, padx=20, pady=(0,15))

        # ‚Äî Left: Dictionary Meanings in 5 columns with scrollbar ‚Äî
        left = tk.LabelFrame(split,
                            text=f"Meanings for ‚Äú{word}‚Äù",
                            font=('Arial',16,'bold'),
                            bg='light gray', fg='black',
                            padx=10, pady=10)

        self.meanings_canvas = tk.Canvas(left, bg='light gray', borderwidth=0)
        scrollbar = tk.Scrollbar(left, orient=tk.VERTICAL, command=self.meanings_canvas.yview)
        self.meanings_canvas.configure(yscrollcommand=scrollbar.set)

        scrollbar.pack(side='right', fill='y')
        self.meanings_canvas.pack(side='left', fill='both', expand=True)
        self.meanings_inner_frame = tk.Frame(self.meanings_canvas, bg='light gray')
        self.meanings_canvas.create_window((0,0), window=self.meanings_inner_frame, anchor='nw')

        def _on_meanings_configure(evt):
            self.meanings_canvas.configure(scrollregion=self.meanings_canvas.bbox("all"))
        self.meanings_inner_frame.bind("<Configure>", _on_meanings_configure)

        split.add(left, stretch="always")

        self.current_word = word   # ‚Üê NEW: remember which word we‚Äôre looking up
        threading.Thread(
            target=lambda: self.lookup_grammar_meanings_thread(word),
            daemon=True
        ).start()


        # ‚Äî Right: Grammar Options + Expert Prompt ‚Äî
        right = tk.LabelFrame(split,
                            text="Select Grammar Options",
                            font=("Arial", 16, "bold"),
                            bg="light gray", fg="black",
                            padx=10, pady=10)
        split.add(right, stretch="never")

        # prepare your choices
        nums = [
            ("Singular", "Singular / ‡®á‡®ï"),
            ("Plural",   "Plural / ‡®¨‡®π‡©Å"),
            ("Unknown",  "NA")
        ]
        gends = [
            ("Masculine", "Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó"),
            ("Feminine",  "Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä"),
            ("Neuter",    "Trans / ‡®®‡®™‡©Å‡©∞‡®∏‡®ï"),
            ("Unknown",   "NA")
        ]
        pos_choices = [
            ("Noun",        "Noun / ‡®®‡®æ‡®Ç‡®µ"),
            ("Adjective",   "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£"),
            ("Adverb",      "Adverb / ‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡©á‡®∂‡®£"),
            ("Verb",        "Verb / ‡®ï‡®ø‡®∞‡®ø‡®Ü"),
            ("Pronoun",     "Pronoun / ‡®™‡©ú‡®®‡®æ‡®Ç‡®µ"),
            ("Postposition","Postposition / ‡®∏‡©∞‡®¨‡©∞‡®ß‡®ï"),
            ("Conjunction", "Conjunction / ‡®Ø‡©ã‡®ú‡®ï"),
            ("Interjection", "Interjection / ‡®µ‡®ø‡®∏‡®Æ‡®ø‡®ï"),
            ("Unknown",     "NA")
        ]

        # Number & Gender side-by-side
        frame_ng = tk.Frame(right, bg="light gray")
        frame_ng.pack(fill=tk.X)

        # Number frame in col0
        num_frame = tk.LabelFrame(frame_ng, text="Number",
                                font=("Arial", 14, "bold"),
                                bg="light gray", padx=8, pady=8)
        num_frame.grid(row=0, column=0, sticky="nsew", padx=5)
        for txt, val in nums:
            tk.Radiobutton(
                num_frame, text=txt, variable=self.number_var, value=val,
                bg="light gray", font=("Arial", 12),
                anchor="w", justify="left"
            ).pack(anchor="w", pady=2)

        # Gender frame in col1, split into two columns
        gend_frame = tk.LabelFrame(frame_ng, text="Gender",
                                font=("Arial", 14, "bold"),
                                bg="light gray", padx=8, pady=8)
        gend_frame.grid(row=0, column=1, sticky="nsew", padx=5)

        # two sub-frames for the two columns
        gf_col1 = tk.Frame(gend_frame, bg="light gray")
        gf_col2 = tk.Frame(gend_frame, bg="light gray")
        gf_col1.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0,5))
        gf_col2.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(5,0))

        # split the list in half
        half = (len(gends)+1)//2
        for i, (txt, val) in enumerate(gends):
            parent = gf_col1 if i < half else gf_col2
            tk.Radiobutton(
                parent, text=txt, variable=self.gender_var, value=val,
                bg="light gray", font=("Arial", 12),
                anchor="w", justify="left"
            ).pack(anchor="w", pady=2)

        # Part-of-Speech in two columns
        pos_frame = tk.LabelFrame(right, text="Part of Speech",
                                font=("Arial", 14, "bold"),
                                bg="light gray", padx=8, pady=8)
        pos_frame.pack(fill=tk.X, pady=5)

        # sub-frames for POS
        p1 = tk.Frame(pos_frame, bg="light gray")
        p2 = tk.Frame(pos_frame, bg="light gray")
        p1.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(0,5))
        p2.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=(5,0))

        half_pos = (len(pos_choices)+1)//2
        for i, (txt, val) in enumerate(pos_choices):
            parent = p1 if i < half_pos else p2
            tk.Radiobutton(
                parent, text=txt, variable=self.pos_var, value=val,
                bg="light gray", font=("Arial", 12),
                anchor="w", justify="left"
            ).pack(anchor="w", pady=2)

        # Expert-prompt builder
        def ask_suggestion():
            verse = self.selected_verse_text
            trans = self.current_translation
            word  = self.current_word
            num   = self.number_var.get() or "‚Äì"
            gen   = self.gender_var.get() or "‚Äì"
            pos   = self.pos_var.get()    or "‚Äì"

            # --------------------------------------------------------------
            # Surface existing matches from the grammar database.  We mimic
            # perform_search_and_finish_reanalysis() to determine whether to
            # call search_by_criteria() or search_by_inflections().
            # --------------------------------------------------------------
            try:
                search_num = self.number_var.get()
                search_gen = self.gender_var.get()
                search_pos = self.pos_var.get()

                if (
                    search_num == "NA" and
                    search_gen == "NA" and
                    search_pos == "NA"
                ):
                    matches = self.search_by_inflections(word)
                else:
                    matches = self.search_by_criteria(
                        word, search_num, search_gen, search_pos
                    )
                    if not matches:
                        matches = self.search_by_inflections(word)

                rows = []
                for result, _count, _perc in matches[:5]:
                    parts = [p.strip() for p in result.split("|")]
                    if len(parts) < 7:
                        parts += [""] * (7 - len(parts))

                    highlight = parts[0] == parts[1] and is_full_word(parts[0])
                    if highlight:
                        parts = [f"**{p}**" for p in parts]
                        parts[0] = "‚úÖ " + parts[0]

                    rows.append(
                        "| "
                        + " | ".join(parts + [str(_count), f"{_perc:.1f}%"])
                        + " |"
                    )

                if rows:
                    headers = [
                        "Word under Analysis",
                        "Vowel Ending / Word Matches",
                        "Number / ‡®µ‡®ö‡®®",
                        "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",
                        "Gender / ‡®≤‡®ø‡©∞‡®ó",
                        "Word Root",
                        "Type",
                        "Match Count",
                        "Match %",
                    ]
                    table_lines = [
                        "**Top Grammar Matches**",
                        "| " + " | ".join(headers) + " |",
                        "| " + " | ".join(["---"] * len(headers)) + " |",
                        *rows,
                    ]
                    matches_block = "\n".join(table_lines)
                else:
                    matches_block = ""
            except Exception as exc:
                print(f"search for matches failed: {exc}")
                matches_block = ""

            # pull the meanings we stored for this word
            meanings = next(
                (e["meanings"] for e in self.grammar_meanings if e["word"] == word),
                []
            )
            meanings_block = "\n".join(f"- {m}" for m in meanings) or "- (no dictionary meanings found)"

            prompt = textwrap.dedent(f"""
            You are a Punjabi grammar expert trained in the grammatical framework of Sri Guru Granth Sahib (SGGS). I will provide:

            1. **Verse** (in Gurmukhi)  
            2. **Established Darpan Translation** (by Prof. Sahib Singh)  
            3. **Word under scrutiny**, along with my selected values for Number, Gender, and Part of Speech  
            4. **Dictionary Meanings** of that word (as a secondary reference)

            Your job is to confirm or correct my selections based on the **Darpan Translation and its contextual meaning**, which is the **primary reference**. Override my input only if the Darpan explanation makes it grammatically, semantically, or functionally incorrect within the SGGS grammatical framework.

           ---

            ## üîÑ Two-Pass Analysis Workflow
            **Phase 1 ‚Äì Functional Tagging**  
            1 a. Locate every occurrence of the stem in the verse.  
            1 b. Assign provisional POS to each occurrence from context.  

            **Phase 2 ‚Äì Morphological Reconciliation**  
            2 a. Compare endings of all identical stems found in 1 a.  
            2 b. If endings differ ‚Üí mark the stem **declinable** and align each form with its noun/pronoun.  
            2 c. If endings never differ ‚Üí note ‚ÄúNo declension detected.‚Äù  

            If Phase 2 detects a declinable pattern but any token fails to agree with its noun/pronoun, **STOP** and return ‚ÄúAgreement Error ‚Äì Review Needed.‚Äù

            ---

            ## üìò Reference Framework ‚Äì SGGS Grammar Definitions

            ### üß© Implicit Case Logic in Gurbani Grammar
            Many case roles in SGGS are conveyed through **inflection or contextual meaning**, not modern postpositions. Refer to the gloss clues (‚Äúof‚Äù, ‚Äúby‚Äù, ‚Äúwith‚Äù, etc.) to infer case correctly.

            ### 1. **Noun (‡®®‡®æ‡®Ç‡®µ)**  
            A noun is a word that names a person, place, thing, quality, or idea.

            #### üîπ Types:
            - **Proper Noun (‡®µ‡®ø‡®∏‡®º‡©á‡®∏‡®º ‡®®‡®æ‡®Ç‡®µ)** ‚Äì e.g., ‡®ó‡©Å‡®∞‡©Ç ‡®®‡®æ‡®®‡®ï
            - **Common Noun (‡®∏‡®ß‡®æ‡®∞‡®® ‡®®‡®æ‡®Ç‡®µ)** ‚Äì e.g., ‡®™‡®æ‡®£‡©Ä, ‡®∞‡©ã‡®ü‡©Ä
            - **Abstract Noun (‡®≠‡®æ‡®µ ‡®®‡®æ‡®Ç‡®µ)** ‚Äì e.g., ‡®™‡®ø‡®Ü‡®∞, ‡®ó‡®ø‡®Ü‡®®
            - **Material Noun (‡®¶‡©ç‡®∞‡®µ ‡®®‡®æ‡®Ç‡®µ)** ‚Äì e.g., ‡®∏‡©ã‡®®‡®æ, ‡®ú‡®≤
            - **Collective Noun (‡®∏‡®Æ‡©Ç‡®π‡®ï ‡®®‡®æ‡®Ç‡®µ)** ‚Äì e.g., ‡®∏‡©∞‡®ó‡®§, ‡®´‡©å‡®ú

            #### üîπ Cases in Gurbani Grammar:
            Nouns in Gurbani may appear in the following **grammatical cases** (*vibhakti*), sometimes **without explicit post-positions**:

            | Case         | Helper (Gloss Clue)             | Modern Marker    | When to Use                                                       |
            |--------------|----------------------------------|------------------|-------------------------------------------------------------------|
            | **Nominative**     | No helper, subject role         | None             | Default when noun is subject of verb                              |
            | **Accusative**     | No helper, object role          | None             | Default when noun is object of verb                               |
            | **Genitive**       | ‚Äúof‚Äù, ‚Äú‡®¶‡©á/‡®¶‡©Ä/‡®¶‡®æ‚Äù                | `‡®¶‡©á`, `‡®¶‡©Ä`, `‡®¶‡®æ` | Use when gloss adds ownership/association                         |
            | **Instrumental**   | ‚Äúby‚Äù, ‚Äúwith‚Äù, ‚Äúunder‚Äù           | `‡®®‡®æ‡®≤`, `‡®Ö‡®ß‡©Ä‡®®`     | Use when gloss suggests means/manner (even if unstated in verse)  |
            | **Dative**         | ‚Äúto‚Äù, ‚Äúfor‚Äù                     | `‡®®‡©Ç‡©∞`, `‡®≤‡®à`       | When gloss implies recipient/beneficiary                          |
            | **Locative**       | ‚Äúin‚Äù, ‚Äúon‚Äù, ‚Äúat‚Äù                | `‡®µ‡®ø‡©±‡®ö`, `‡®§‡©á`      | When gloss places noun in space/context                           |
            | **Ablative**       | ‚Äúfrom‚Äù, ‚Äúout of‚Äù                | `‡®§‡©ã‡®Ç`, `‡®â‡®§‡©ã‡®Ç`      | When gloss implies source                                         |
            | **Vocative**       | ‚ÄúO‚Äù, ‚ÄúHey‚Äù                      | *(address)*       | Used for direct address (e.g., *‡®π‡©á ‡®≠‡®æ‡®à!*)                          |

            > üî∏ **Implicit Post-Positions:** If Darpan adds ‚Äú‡®®‡®æ‡®≤, ‡®¶‡©á, ‡®µ‡®ø‡©±‡®ö, ‡®§‡©ã‡®Ç‚Äù etc., treat it as a **helper** for inferring the noun‚Äôs **grammatical case**, even if the verse lacks a marker.
            >
            > üî∏ **Indeclinable Loan Nouns:** Sanskrit-based nouns (like *‡®¨‡®ø‡®ß‡®ø*, *‡®Æ‡®§‡©Ä*) may not show visible inflection. Their case must be inferred from semantic role and Darpan gloss, not suffix alone.

            > üîπ **Fallback Rule:**  
            > When the gloss offers no helper and the noun does not visibly decline, default to **Nominative or Accusative**, then refine based on sentence structure and implied role in the Darpan explanation.

            ### 2. **Pronoun (‡®™‡©ú‡®®‡®æ‡®Ç‡®µ)**  
            Used in place of nouns. Types include:  
            - **Personal**, **Demonstrative**, **Reflexive**, **Possessive**, **Relative**, **Indefinite**, **Interrogative**

            ### 3. **Adjective (‡®µ‡®ø‡®∏‡®º‡©á‡®∏‡®º‡®£) ‚Äì Agreement Framework**
            Describes or qualifies a noun or pronoun only. Must be directly linked to one.  
            Adjectives include: **Qualitative**, **Demonstrative**, **Indefinite**, **Pronominal**, **Numeral**, and **Interrogative**.
            Examples include: ‡®ö‡©∞‡®ó‡®æ ‡®Æ‡®®‡©Å, ‡®ö‡©∞‡®ó‡©Ä ‡®¨‡®æ‡®£‡©Ä, ‡®ö‡©∞‡®ó‡©á ‡®¨‡®ö‡®®, ‡®∏‡®æ‡®∞‡®æ ‡®¶‡©Å‡®ñ, ‡®â‡®π ‡®Æ‡®æ‡®á‡®Ü, ‡®ï‡©ã‡®à ‡®Æ‡®®‡©Å‡©±‡®ñ

            üî¥ **GURBANI RULE (STRICT)**  
            ‚ñ∂Ô∏è **All adjectives in Gurbani MUST agree in Number and Gender with the noun or pronoun they qualify.**  
            This is a **non-negotiable rule** confirmed by both **Sikh Research Institute (SikhRi)** and **Prof. Sahib Singh‚Äôs Gurbani Vyakaran**.  
            The agreement must be:
            - **Semantic** (referring to the correct noun/pronoun)
            - **Morphological** (adjective form visibly matches Number & Gender)

            üëâ *In Gurbani, adjectives are always **declined** to match the Number and Gender of the noun or pronoun they describe. This means adjectives **change form** based on their grammatical role. They are not fixed or invariable by default.*

            If the adjective‚Äôs form appears fixed (e.g., ending in ‚Äò≈ç‚Äô or ‚Äòau‚Äô), consult its grammatical root ending (MuktƒÅ, KannƒÅ, AunkƒÅr, HorƒÅ, BihƒÅrƒ´) to verify its role and alignment.

            üîç *Do not assume that any adjective is morphologically invariable unless **Gurbani Vyakaran** explicitly identifies it as a poetic variant that still maintains grammatical agreement.* **Do not conclude invariance merely because the same form appears with multiple nouns.**
            **Many adjectives follow internal paradigms that are consistent across different contexts, even if they *look* fixed.**

            üß† *If the adjective‚Äôs ending appears unchanged, it must still be evaluated against known adjective paradigms (e.g., h≈çrƒÅ-ending, kannƒÅ-ending). Only when those forms confirm invariance through grammatical structure‚Äînot intuition‚Äîshould it be marked as ‚Äòinvariable‚Äô in the agreement table.*

            > **Cross-token check ** ‚Äì If the same stem re-appears with a different ending in the *line*, treat that as conclusive evidence it is **declinable**; do not invoke ‚Äúindeclinable‚Äù unless all tokens are identical in form *and* no paradigm lists inflected endings.

            ---

            **üõë Mandatory Adjective Agreement Table**
            ‚ö†Ô∏è **Caution:**  
            Do **not** classify a word as an Adjective merely because it appears near a noun.  
            Carefully check whether the word is:
            - Acting as the **object of a postposition** (e.g., "‡®¶‡©á ‡®Ö‡®ß‡©Ä‡®®", "‡®µ‡®ø‡©±‡®ö", "‡®§‡©ã‡®Ç", "‡®â‡©±‡®§‡©á"), in which case it is a **noun**, not an adjective.
            - Part of an **oblique noun phrase** and not qualifying the noun directly.
            - Functioning as a **noun in instrumental case** (e.g., ‡®§‡©ç‡®∞‡®ø‡®¨‡®ø‡®ß‡®ø ‚Äì by/with threefold means); these may **appear** descriptive but are **semantically instrumental nouns**, not adjectives.
            
            These constructions often create **false links**. Always confirm grammatical agreement and functional relationship before assigning Adjective.

            If a word is confirmed as an adjective, this table is required:

            | Step | Requirement | Observation | Result |
            |------|-------------|-------------|--------|
            | 1 | Identify the qualified noun/pronoun | (e.g., ‡®∏‡©Å‡®ñ‡©Å ‚Äì masculine singular) | ... |
            | 2 | Show matching Number & Gender in adjective form | (e.g., ‡®Ö‡®ó‡®≤‡©ã = masculine singular form of ‡®π‡©å‡®∞‡®æ-ending adjective) | ‚úÖ / ‚ùå |
            | 3 | Stem-variation observed? | e.g. ‡®´‡®ï‡©ú / ‡®´‡®ï‡©ú‡©Å | ‚úÖ / ‚ùå |

            ‚ùå *Responses that skip this table or assume invariable adjectives will be treated as incomplete.*
            *(skip the table entirely if final POS ‚â† Adjective)*

            ### 4. **Verb (‡®ï‡®ø‡®∞‡®ø‡®Ü)**  
            Expresses an action, state, or condition. Includes forms like transitive/intransitive, passive, causative, auxiliary, etc.

            ### 5. **Adverb (‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡®º‡©á‡®∏‡®º‡®£)**  
            Modifies verbs only. Never nouns. Categories include Time, Place, Manner, Degree, Frequency, etc.

            ### 6. **Postposition (‡®∏‡®ø‡©∞‡®¨‡©∞‡®ß‡®ï)** ‚Äì e.g., ‡®®‡®æ‡®≤, ‡®µ‡®ø‡©±‡®ö, ‡®â‡©±‡®§‡©á  
            ### 7. **Conjunction (‡®Ø‡©ã‡®ó‡®ï)** ‚Äì e.g., ‡®Ö‡®§‡©á, ‡®ú‡©á‡®ï‡®∞, ‡®™‡®∞  
            ### 8. **Interjection (‡®µ‡®ø‡®∏‡®Æ‡©Ä‡®ï)** ‚Äì e.g., ‡®µ‡®æ‡®π ‡®µ‡®æ‡®π!, ‡®π‡®æ‡®è!

            ---

            ## üéØ Evaluation Guidelines

            1. Use **Darpan Translation** to determine the word‚Äôs semantic role.  
            2. Confirm **Part of Speech**:  
            - Modifies noun/pronoun ‚Üí Adjective (**triggers the agreement check**)  
            - Modifies verb/adjective/adverb ‚Üí Adverb  
            - If noun/pronoun ‚Üí classify accordingly  
            3. For Adjectives:
            - Confirm Number & Gender based on the noun/pronoun the adjective qualifies. If the adjective form appears fixed, verify its grammatical alignment using its root ending.
            - If adjective doesn‚Äôt change form (invariable), still list target noun and declare this explicitly 
            - ‚ö†Ô∏è The **noun‚Äôs gender and number** must be derived from **Gurbani Grammar definitions** (as per Darpan and Vyakaran), not from modern Punjabi intuition or pronunciation. For example, abstract nouns like **‡®∏‡©á‡®µ‡®æ** are feminine singular by SGGS convention.
            ‚úÖ *Trigger Adjective Agreement Table only if:*  
            - Word semantically modifies a noun/pronoun (confirmed in Darpan gloss)  
            - Is not the subject/object of a helper-preposition  
            - Does not serve as the head of a noun phrase or abstract concept (e.g., ‡®§‡©ç‡®∞‡®ø‡®¨‡®ø‡®ß‡®ø = by/through threefold mode)  
            4. Do not guess based on spelling or intuition‚Äî**rely on function and context from translation**  
            5. Output is **incomplete** if POS = Adjective and Adjective Agreement Table is missing

            ---

            ## üì• Inputs

            **Verse (Gurmukhi):**  
            {verse}

            **Darpan Translation:**  
            {trans}

            **Word under scrutiny:**  
            {word}

            **My Selections:**  
            - Number: {num}  
            - Gender: {gen}  
            - Part of Speech: {pos}

            **Dictionary Meanings (Secondary Aid):**
            {meanings_block}

            {matches_block}

            ---

            ## üìã Response Format (Follow exactly)

            1. **Feature Confirmation**  
            - Number: (Correct / Incorrect) ‚Äì based on Darpan gloss and noun agreement  
            - Gender: (Correct / Incorrect) ‚Äì based on noun gender  
            - Part of Speech: (Correct / Incorrect) ‚Äì based on function and Darpan context  

            2. **Corrections (if needed)**  
            - Number: <correct value> ‚Äì with rationale  
            - Gender: <correct value> ‚Äì with rationale  
            - Part of Speech: <correct value> ‚Äì with rationale  

            3. **Commentary**  
            - Explain briefly how the Darpan translation and noun/pronoun connection led to your decision  
            - If adjective form is invariable, name the adjective group (e.g., **Horaa** ending or **Poetic variation**)

            4. **Adjective-Agreement Table (REQUIRED if POS = Adjective)**  
            | Step | Requirement              | Observation                    | Result        |
            |------|--------------------------|--------------------------------|---------------|
            | 1    | Qualified noun/pronoun   | (e.g., ‡®∏‡©Å‡®ñ‡©Å ‚Äì masculine-singular) | (Identified) |
            | 2    | Number & Gender match    | (e.g., adjective ends with -≈ç, matches masculine singular noun; or declare as invariable) | ‚úÖ/‚ùå |
            
            ---

            üìò **Quick Reference: Common Adjective Endings in Gurbani**

            | Ending      | Number & Gender         | Example           |
            |-------------|--------------------------|-------------------|
            | **-≈ç**      | Masculine singular        | ‡®Ö‡®ó‡®≤‡©ã, ‡®®‡®ø‡®µ‡©ç‡®∞‡®§‡©ã       |
            | **-ƒì / ‡®è**  | Masculine plural          | ‡®Ö‡®ó‡®≤‡©á, ‡®ö‡©∞‡®ó‡©á         |
            | **-ƒ´**      | Feminine singular         | ‡®ö‡©∞‡®ó‡©Ä, ‡®Ö‡®ó‡®≤‡©Ä         |
            | **-ƒ´ƒÅ·πÅ / ‡®ø‡®Ü‡®Ç** | Feminine plural         | ‡®ö‡©∞‡®ó‡©Ä‡®Ü‡®Ç, ‡®Ö‡®ó‡®≤‡©Ä‡®Ü‡®Ç      |

            These endings are drawn from adjective groups described in Prof. Sahib Singh‚Äôs *Gurbani Vyakaran*, e.g., h≈çrƒÅ-samƒÅpt adjectives. Always match these with the gender and number of the qualified noun.
            üîπ *Tatsam Words (Sanskrit-Derived)*:  
            Many Sanskrit-origin words in Gurbani‚Äîsuch as **‡®§‡©ç‡®∞‡®ø‡®¨‡®ø‡®ß‡®ø**, **‡®ó‡©Å‡®π‡®ú**, **‡®§‡®§**‚Äîoften appear morphologically fixed and may superficially resemble adjectives. However, they frequently function as **abstract nouns** or appear in **instrumental** or other oblique grammatical cases.

            > üî∏ **Tatsam Adjectives vs Indeclinable Nouns:**  
            > Do **not** classify such words as adjectives unless the **Darpan gloss clearly shows them qualifying a noun**, with **visible agreement in Number and Gender**.  
            > ‚ñ∂Ô∏è If the gloss inserts a helper like *‚Äúby,‚Äù ‚Äúwith,‚Äù ‚Äúin,‚Äù or ‚Äúof‚Äù*, this usually signals a **noun in an oblique case**‚Äînot an adjective.  
            > ‚ûï For example, **‡®§‡©ç‡®∞‡®ø‡®¨‡®ø‡®ß‡®ø** may mean *‚Äúby threefold means‚Äù* or *‚Äúthrough the three qualities‚Äù*, serving a **functional role** rather than describing a noun.

            üîç *Key Insight:*  
            Words like **‡®§‡©ç‡®∞‡®ø‡®¨‡®ø‡®ß‡®ø**, despite their descriptive appearance, often act as **instrumental-case nouns** or form part of a **compound abstract expression** (e.g., *‡®§‡©ç‡®∞‡®ø‡®ó‡©Å‡®£‡©Ä ‡®Æ‡®æ‡®á‡®Ü*). Always validate their role against the **Darpan translation** and **Gurbani grammar definitions**, not surface resemblance.

            ---

            ### üìë Stem-Variation Check üÜï
            *(Fill this mini-grid during Phase 2 if you detected more than one token of the same stem)*  
            | Token | Ending | Nearby noun/pronoun | Expected agreement | Matches? |
            |-------|--------|---------------------|--------------------|----------|

            ---

            üõ† **Debug Trace** üÜï (single line at the very end):  
            `[TokensChecked:X | Declined:Yes/No | FinalPOS:___ | AgreementOK:Yes/No]`

            """).strip()

            # copy to clipboard
            self.root.clipboard_clear()
            self.root.clipboard_append(prompt)
            messagebox.showinfo(
                "Prompt Ready",
                "Expert-level prompt (with secondary dictionary meanings) has been copied to your clipboard.\n"
                "Paste it into ChatGPT for its recommendation."
            )

        tk.Button(
            right,
            text="üìã Build Expert Prompt",
            font=("Arial", 14, "italic"),
            bg="white",
            fg="dark cyan",
            padx=6, pady=4,
            command=ask_suggestion
        ).pack(pady=(10,0))

        # 5) Bottom separator + buttons
        sep = tk.Frame(win, bg='#cccccc', height=2)
        sep.pack(side=tk.BOTTOM, fill=tk.X, padx=20, pady=(0,5))

        btns = tk.Frame(win, bg='light gray')
        btns.pack(side=tk.BOTTOM, fill=tk.X, padx=20, pady=(0,46))
        tk.Button(btns, text="‚Äπ Back to Translation",
                  font=('Arial',12), bg='gray', fg='white',
                  padx=20, pady=8,
                  command=lambda: [win.destroy(), self.show_translation_input()]
        ).pack(side=tk.LEFT)
        tk.Button(btns, text="Skip Word",
                  font=('Arial',12), bg='orange', fg='white',
                  padx=20, pady=8,
                  command=lambda: [win.destroy(), self.skip_word_grammar()]
        ).pack(side=tk.LEFT, padx=10)
        tk.Button(btns, text="Submit",
                  font=('Arial',12,'bold'),
                  bg='dark cyan', fg='white',
                  padx=20, pady=8,
                  command=lambda: self.submit_input_grammar(word, index)
        ).pack(side=tk.RIGHT)

        # Modal
        win.transient(self.root)
        win.grab_set()
        self.root.wait_window(win)

    def lookup_grammar_meanings_thread(self, word):
        """
        Look up dictionary meanings for ‚Äòword‚Äô on a background thread,
        then schedule update into the grammar UI.
        """
        meanings = self.lookup_word_in_dictionary(word)
        # schedule into mainloop
        self.root.after(0, lambda: self.update_grammar_meanings_ui(meanings))

    def update_grammar_meanings_ui(self, meanings):
        """
        Populate the meanings_inner_frame into N columns (now 7).
        """
        # 1) Clear any old widgets
        for w in self.meanings_inner_frame.winfo_children():
            w.destroy()

        # 2) Decide on how many columns
        num_cols = 5
        total   = len(meanings)
        # Ceil division so each column has at most ceil(total/num_cols) entries
        per_col = -(-total // num_cols)

        # 3) Grid each meaning into (row, column)
        for idx, m in enumerate(meanings):
            col = idx // per_col
            row = idx % per_col
            tk.Label(
                self.meanings_inner_frame,
                text=f"‚Ä¢ {m}",
                bg='light gray',
                font=('Arial', 12),
                wraplength=350,   # adjust if you need narrower columns
                justify='left'
            ).grid(
                row=row,
                column=col,
                sticky='nw',
                padx=8, pady=2
            )
        
        # 4) NEW: stash into a growing list of dicts:
        entry = {
            "word": getattr(self, "current_word", None),
            "meanings": meanings
        }
        self.grammar_meanings.append(entry)

    def submit_input_grammar(self, word, index):
        """
        Collects grammar input and transitions to the dropdown step.
        """
        # 1) Extract the basic Number/Gender/POS the user just picked:
        number = self.number_var.get()
        gender = self.gender_var.get()
        pos    = self.pos_var.get()

        # 2) Gather verse + translation context:
        verse       = self.selected_verse_text
        translation = self.current_translation

        # 3) Pull the previously looked‚Äêup meanings out of self.grammar_meanings:
        meanings = next(
            (e["meanings"] for e in self.grammar_meanings if e["word"] == word),
            []
        )

        # 4) Build the initial "detailed" entry dict:
        entry = {
            "\ufeffVowel Ending":       word,
            "Number / ‡®µ‡®ö‡®®":       number,
            "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£":    "",   # to be filled in dropdown step
            "Gender / ‡®≤‡®ø‡©∞‡®ó":       gender,
            "Word Root":           "",   # to be filled next
            "Type":                pos,
            "Evaluation":          "Derived",
            "Reference Verse":     verse,
            "Darpan Translation":  translation,
            "Darpan Meaning":      "| ".join(m.strip() for m in meanings),
            "ChatGPT Commentary":  ""    # to be pasted later
        }

        # 5) Store it so the next window can read & update it:
        self.current_detailed_entry = entry

        # 6) Hand off to your dropdown‚ÄêUI:
        self.open_final_grammar_dropdown(word, entry["Type"], index)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # MAIN METHOD  ‚Äì  drop-in replacement
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def open_final_grammar_dropdown(self, word, pos, index):
        """
        After the user has chosen a Part-of-Speech, pop up a Toplevel
        with dropdowns for the detailed grammar fields _and_ a place
        to paste ChatGPT‚Äôs commentary.
        """

        # 1) --------------  Load & filter your CSV  -----------------
        self.grammar_db = pd.read_csv("1.1.1_birha.csv")
        df = self.grammar_db[self.grammar_db["Type"] == pos]

        # option lists
        num_opts  = sorted(df["Number / ‡®µ‡®ö‡®®"].dropna().unique().tolist())
        gram_opts = sorted(df["Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£"].dropna().unique().tolist())
        gen_opts  = sorted(df["Gender / ‡®≤‡®ø‡©∞‡®ó"].dropna().unique().tolist())
        
        # pull the saved entry first
        entry = self.current_detailed_entry
        # Extract the POS type
        pos_type = entry["Type"]

        # Choose how to build root_opts based on whether it's a Noun
        if pos_type == "Noun / ‡®®‡®æ‡®Ç‡®µ":
            # Option-1: For Nouns, use hard-wired canonical endings
            root_opts = CANONICAL_ENDINGS.copy()
            for lst in (num_opts, gram_opts, gen_opts):
                if "NA" not in lst:
                    lst.insert(0, "NA")
        else:
            # Option-2: For all other types (e.g., Pronoun), use actual values from database
            root_opts = sorted(df["Word Root"].dropna().unique().tolist())
            for lst in (num_opts, gram_opts, gen_opts, root_opts):
                if "NA" not in lst:
                    lst.insert(0, "NA")

        # ---- Repeat-word banner bookkeeping ----
        verse_text = getattr(self, "current_pankti", "")
        verse_key = self._verse_key(verse_text)
        if getattr(self, "_last_dropdown_verse_key", None) != verse_key:
            self._first_repeat_token = None
            self._last_dropdown_verse_key = verse_key

        words = list(getattr(self, "pankti_words", []))
        if not words and verse_text:
            words = verse_text.split()
        norm_words = getattr(self, "_norm_words_cache", None)
        if norm_words is None or len(norm_words) != len(words):
            norm_words = [self._norm_tok(w) for w in words]
            # keep cache in sync for other flows that read it
            self._norm_words_cache = norm_words

        idx = index
        display_word = words[idx] if idx < len(words) else word
        word_norm = norm_words[idx] if idx < len(norm_words) else self._norm_tok(display_word)

        # If the normalized token vanished (punctuation/ZW space), never treat it as a repeat.
        has_repeat = self._has_repeat(norm_words, word_norm) if word_norm else False
        if has_repeat and self._first_repeat_token is None and word_norm:
            self._first_repeat_token = word_norm

        seen_before = norm_words[:idx].count(word_norm)
        key = (verse_key, word_norm, "second")
        is_special_hit = (
            has_repeat
            and word_norm == self._first_repeat_token
            and seen_before == 1
            and key not in self._repeat_note_shown
        )

        inline_allowed = (
            getattr(self, "_use_inline_literal_banner", True)
            and not getattr(self, "_suppress_repeat_notes_for_verse", False)
        )

        suppress_first_occurrence_of_first_token = (
            has_repeat
            and self._first_repeat_token is not None
            and word_norm == self._first_repeat_token
            and seen_before == 0
        )

        if inline_allowed and is_special_hit:
            self._repeat_note_shown.add(key)
            reuse_ok = (
                hasattr(self, "literal_note_frame") and self.literal_note_frame
                and self.literal_note_frame.winfo_exists()
                and self.literal_note_frame.master is self.match_window
            )
            if not reuse_ok:
                if (
                    hasattr(self, "literal_note_frame")
                    and self.literal_note_frame
                    and self.literal_note_frame.winfo_exists()
                ):
                    self.literal_note_frame.destroy()
                self.literal_note_frame = tk.Frame(
                    self.match_window, bg="AntiqueWhite", relief="groove", bd=2
                )
                self.literal_note_title = tk.Label(
                    self.literal_note_frame,
                    text="Important Note ‚Äî Literal Analysis",
                    bg="AntiqueWhite",
                    font=("Arial", 14, "bold"),
                )
                self.literal_note_title.pack(anchor="w", padx=10, pady=(5, 0))
                self.literal_note_body = tk.Label(
                    self.literal_note_frame,
                    bg="AntiqueWhite",
                    wraplength=self._banner_wraplength(self.match_window),
                    justify=tk.LEFT,
                    font=("Arial", 12),
                )
            else:
                if not self.literal_note_title.winfo_ismapped():
                    self.literal_note_title.pack(anchor="w", padx=10, pady=(5, 0))
                if not self.literal_note_body.winfo_ismapped():
                    self.literal_note_body.pack(anchor="w", padx=10, pady=(0, 5))
            if not self.literal_note_frame.winfo_ismapped():
                self.literal_note_frame.pack(fill=tk.X, padx=20, pady=(5, 10))
            banner_text = (
                f"In literal analysis: The word ‚Äú{display_word}‚Äù appears multiple times in this verse. "
                "The highlighted grammar options reflect your past selections for this word (or close matches) "
                "to encourage consistency. They‚Äôre suggestions, not mandates‚Äîadjust if the current context differs."
            )
            body = self.literal_note_body
            if body and body.winfo_exists():
                body.config(
                    text=banner_text,
                    wraplength=self._banner_wraplength(self.match_window),
                )
            try:
                self._on_match_window_resize()
            except Exception:
                pass
            try:
                if not getattr(self, "_inline_resize_bound", False):
                    self.match_window.bind(
                        "<Configure>", self._on_match_window_resize, add="+"
                    )
                    self._inline_resize_bound = True
            except Exception:
                pass
        elif inline_allowed and has_repeat and not suppress_first_occurrence_of_first_token:
            reuse_ok = (
                hasattr(self, "literal_note_frame") and self.literal_note_frame
                and self.literal_note_frame.winfo_exists()
                and self.literal_note_frame.master is self.match_window
            )
            if not reuse_ok:
                if (
                    hasattr(self, "literal_note_frame")
                    and self.literal_note_frame
                    and self.literal_note_frame.winfo_exists()
                ):
                    self.literal_note_frame.destroy()
                self.literal_note_frame = tk.Frame(
                    self.match_window, bg="AntiqueWhite", relief="groove", bd=2
                )
                self.literal_note_title = tk.Label(
                    self.literal_note_frame,
                    text="Important Note ‚Äî Literal Analysis",
                    bg="AntiqueWhite",
                    font=("Arial", 14, "bold"),
                )
                self.literal_note_title.pack(anchor="w", padx=10, pady=(5, 0))
                self.literal_note_body = tk.Label(
                    self.literal_note_frame,
                    bg="AntiqueWhite",
                    wraplength=self._banner_wraplength(self.match_window),
                    justify=tk.LEFT,
                    font=("Arial", 12),
                )
            if not self.literal_note_frame.winfo_ismapped():
                self.literal_note_frame.pack(fill=tk.X, padx=20, pady=(5, 10))
            body = self.literal_note_body
            if body and body.winfo_exists():
                body.config(
                    text=self._LITERAL_NOTE_TEXT,
                    wraplength=self._banner_wraplength(self.match_window),
                )
        else:
            if hasattr(self, "literal_note_frame") and self.literal_note_frame:
                if (
                    self.literal_note_frame.winfo_exists()
                    or self.literal_note_frame.master is not self.match_window
                ):
                    self.literal_note_frame.destroy()
                self.literal_note_frame = None
                self.literal_note_title = None
                self.literal_note_body = None

        # 2) --------------  Build the window  -----------------------
        win = tk.Toplevel(self.root)
        win.title(f"Detail Grammar for ‚Äò{word}‚Äô")
        win.configure(bg="light gray")
        win.state("zoomed")

        frm = tk.LabelFrame(
            win, text="Finalize Detailed Grammar",
            font=("Arial", 16, "bold"), bg="light gray",
            padx=10, pady=10
        )
        frm.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)
        frm.grid_columnconfigure(1, weight=1)

        def _add_dropdown(row, label, var, options, colspan=1):
            ttk.Label(frm, text=label, font=("Arial", 12),
                    background="light gray").grid(
                row=row, column=0, sticky="w", padx=5, pady=8)
            cb = ttk.Combobox(
                frm, textvariable=var, values=options,
                state="readonly", font=("Arial", 12))
            cb.grid(row=row, column=1, columnspan=colspan,
                    sticky="ew", padx=5, pady=8)
            return cb

        # 3) --------------  Five dropdowns  -------------------------
        self.detailed_ve_var      = tk.StringVar(value=self._norm_get(entry, "\ufeffVowel Ending"))
        self.detailed_number_var  = tk.StringVar(value=entry["Number / ‡®µ‡®ö‡®®"])
        self.detailed_grammar_var = tk.StringVar(value=entry["Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£"])
        self.detailed_gender_var  = tk.StringVar(value=entry["Gender / ‡®≤‡®ø‡©∞‡®ó"])
        self.detailed_root_var    = tk.StringVar(value=entry["Word Root"])

        _add_dropdown(0, "Word Under Analysis:", self.detailed_ve_var, [word], colspan=2)
        _add_dropdown(1, "Number / ‡®µ‡®ö‡®®:",        self.detailed_number_var,  num_opts)
        _add_dropdown(2, "Grammar Case / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£:", self.detailed_grammar_var, gram_opts)
        _add_dropdown(3, "Gender / ‡®≤‡®ø‡©∞‡®ó:",        self.detailed_gender_var,   gen_opts)
        _add_dropdown(4, "Word Root:",            self.detailed_root_var,     root_opts)

        # 4) --------------  Commentary box  -------------------------
        cm_frame = tk.LabelFrame(
            frm, text="ChatGPT Commentary", font=("Arial", 14, "bold"),
            bg="light gray", padx=8, pady=8
        )
        cm_frame.grid(row=5, column=0, columnspan=2,
                    sticky="nsew", padx=5, pady=(10, 0))
        self.detailed_commentary = tk.Text(
            cm_frame, wrap=tk.WORD, font=("Arial", 12),
            height=6, bd=1, relief="sunken", padx=5, pady=5
        )
        self.detailed_commentary.pack(fill=tk.BOTH, expand=True)

        # ---------- dynamic noun-map in self ----------
        if not hasattr(self, "noun_map"):
            self.noun_map = build_noun_map()

        def build_examples_footer():
            """
            Return a Markdown block that lists every ending-class with its full word,
            base form, and detachable suffix, taken from EXAMPLE_BASES.
            """
            lines = ["*Ending-class examples*"]
            for label in CANONICAL_ENDINGS:
                if label == "NA":
                    continue

                triples = EXAMPLE_BASES.get(label, [])
                if not triples:
                    continue

                # Build ‚Äú‡®â‡®¶‡®ø‡®Ü‡®®‡©à ‚Üí ‡®â‡®¶‡®ø‡®Ü‡®® + ‡©à‚Äù style strings
                rendered = [
                    f"{full} ‚Üí {base}{' + ' + suf if suf else ''}"
                    for full, base, suf in triples
                ]
                lines.append(f"- **{label}** ‚Üí " + ", ".join(rendered))

            return "\n".join(lines)

        # helper ‚Äì build cheat-sheet table from noun_map
        def make_cheat_sheet(word: str, gender: str, number: str) -> str:
            """
            Progressive right-edge matcher, now bounded by len(word):
            ‚Ä¢ For L = 1 ‚Ä¶ len(word):
                    slice_w = word[-L:]
                    for every ending key E in noun_map:
                        if E[-L:] == slice_w  ‚Üí collect E
            ‚Ä¢ Merge all collected endings‚Äô case tables (deduped), build Markdown.
            """

            word_len = len(word)                              # new upper bound
            matched: list[str] = []

            # 1) -------- gather every ending with the same right-edge ------------
            for L in range(1, word_len + 1):                  # 1 ‚Ä¶ len(word)
                slice_w = word[-L:]
                for ending in self.noun_map:
                    if ending[-L:] == slice_w and ending not in matched:
                        matched.append(ending)

            if not matched:
                return ""                                     # nothing found

            # 2) -------- merge case ‚Üí suffix lists for gender & number ----------
            merged: dict[str, list[str]] = {}
            for end in matched:
                cases = (
                    self.noun_map[end]
                        .get(gender or "NA", {})
                        .get(number or "NA", {})
                )
                for case, forms in cases.items():
                    merged.setdefault(case, []).extend(forms)

            if not merged:
                return ""                                     # no data for this combo

            # Deduplicate each list while preserving order
            for case, forms in merged.items():
                seen = set()
                merged[case] = [f for f in forms if not (f in seen or seen.add(f))]

            # 3) -------- build the mini-table -----------------------------------
            rows = [
                f"| {case:11} | {', '.join(forms)} |"
                for case, forms in merged.items()
            ]
            ending_list = ", ".join(matched)

            # build the core table but DON‚ÄôT return yet
            table_rows = "\n".join(rows)
            table_markdown = textwrap.dedent(f"""
                **Morphology map ‚Äì endings matched: {ending_list}
                ({gender.split()[0]}/{number.split()[0]})**
                | Case         | Attested suffix(es) |
                |--------------|----------------------|
                {table_rows}
                _Table shows **attested** suffixes.
                If you need an unlisted case, propose a plausible form and justify._
            """).strip()

            # --- build a footer that shows EVERY ending-class with examples -------------
            footer = "\n" + build_examples_footer()
            return table_markdown + footer + "\n\n"

        # 5) --------------  Prompt-builder button  ------------------
        def build_detailed_prompt(num_opts=num_opts,
                                gram_opts=gram_opts,
                                gen_opts=gen_opts,
                                root_opts=root_opts):

            ve    = self.detailed_ve_var.get()      or "(please choose)"
            num   = self.detailed_number_var.get()  or "(please choose)"
            gram  = self.detailed_grammar_var.get() or "(please choose)"
            gen   = self.detailed_gender_var.get()  or "(please choose)"
            root  = self.detailed_root_var.get()    or "(please choose)"
            verse = entry["Reference Verse"]
            trans = entry["Darpan Translation"]
            dm    = entry["Darpan Meaning"]

            def make_block(title, items):
                lines = [f"- **{title}**"]
                for it in items:
                    lines.append(f"  ‚Äì {it}")
                return "\n".join(lines)

            # ------------------------------------------------------------------
            # Use the existing search_by_criteria helper to surface any grammar
            # matches that align with the current selections.  This gives the
            # language model extra context about how the form appears in the
            # database.  If no match is found or the search fails, we simply
            # omit the block from the prompt.
            # ------------------------------------------------------------------
            try:
                crit_num = num if num != "(please choose)" else "NA"
                crit_gen = gen if gen != "(please choose)" else "NA"
                crit_matches = self.search_by_criteria(word, crit_num, crit_gen, pos)

                rows = []
                for result, _count, _perc in crit_matches[:5]:
                    parts = [p.strip() for p in result.split("|")]
                    if len(parts) < 7:
                        parts += [""] * (7 - len(parts))

                    highlight = parts[0] == parts[1] and is_full_word(parts[0])
                    if highlight:
                        parts = [f"**{p}**" for p in parts]
                        parts[0] = "‚úÖ " + parts[0]

                    rows.append("| " + " | ".join(
                        parts + [str(_count), f"{_perc:.1f}%"]
                    ) + " |")

                if rows:
                    headers = [
                        "Word under Analysis",
                        "Vowel Ending / Word Matches",
                        "Number / ‡®µ‡®ö‡®®",
                        "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",
                        "Gender / ‡®≤‡®ø‡©∞‡®ó",
                        "Word Root",
                        "Type",
                        "Match Count",
                        "Match %",
                    ]
                    table_lines = [
                        "**Top Grammar Matches**",
                        "| " + " | ".join(headers) + " |",
                        "| " + " | ".join(["---"] * len(headers)) + " |",
                        *rows,
                    ]
                    matches_block = "\n".join(table_lines)
                else:
                    matches_block = ""
            except Exception as exc:
                print(f"search_by_criteria failed: {exc}")
                matches_block = ""

            opts_block = "\n\n".join([
                make_block("Word Under Analysis", [ve]),
                make_block("Number / ‡®µ‡®ö‡®® options",   num_opts),
                make_block("Grammar Case / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£ options", gram_opts),
                make_block("Gender / ‡®≤‡®ø‡©∞‡®ó options",  gen_opts),
                make_block("Word-Root options",      root_opts),
            ])

            # noun-specific notes
            ending_cheat_sheet = ""
            implicit_note      = ""
            common_sense_note  = ""

            if entry["Type"] == "Noun / ‡®®‡®æ‡®Ç‡®µ":
                ending_cheat_sheet = make_cheat_sheet(ve, gen, num)

                implicit_note = textwrap.dedent("""\
                    **IMPLICIT POST-POSITIONS & CASE DECLENSIONS**  
                    In GurbƒÅ·πáƒ´, relationships such as *to, from, with, of, in* are conveyed
                    by **inflected endings** rather than modern post-positions (`‡®®‡©Ç‡©∞`, `‡®®‡®æ‡®≤`
                    ‚Ä¶). A noun may appear unmarked while the Darpan gloss supplies a helper.

                    **How to read the gloss**  
                    ‚Ä¢ If the gloss inserts **to / for / of / by / with / from / in / on / at / O / Hey**
                    that is absent in the verse, treat it as an **implicit post-position**
                    and pick the matching **case**.  
                    ‚Ä¢ If the gloss repeats the word without a helper, default to
                    **Nominative / Accusative** and let context refine the choice.

                    | Helper | Punjabi marker | Case |
                    |--------|----------------|------|
                    | to / for   | `‡®®‡©Ç‡©∞`, `‡®≤‡®à`     | **Dative** |
                    | of         | `‡®¶‡®æ/‡®¶‡©á/‡®¶‡©Ä`      | **Genitive** |
                    | by / with  | `‡®®‡®æ‡®≤`, `‡®®‡®æ‡®≤‡©ã‡®Ç`  | **Instrumental** |
                    | from / out of | `‡®§‡©ã‡®Ç`, `‡®â‡®§‡©ã‡®Ç` | **Ablative** |
                    | in / on / at | `‡®µ‡®ø‡©±‡®ö`, `‡®â‡©±‡®§‡©á`, `‡®§‡©á` | **Locative** |
                    | O / Hey    | *(address)*     | **Vocative** |

                    _Endings overlap: Nom‚âàAcc, Gen‚âàDat, Inst‚âàLoc ‚Äì use semantics to decide._
                """).strip() + "\n\n"

                common_sense_note = textwrap.dedent("""\
                    **SEMANTIC SANITY CHECK ‚Äì DOES THE LABEL REALLY FIT?**  
                    Match the case to the *role* the noun plays.

                    **Quick Meanings**  Nom=subject | Acc=object | Inst=by/with | Dat=to/for |
                    Gen=of | Abl=from | Loc=in/on | Voc=address

                    ‚Ä¢ Instrumental ‚Äì means, agency, tool  
                    ‚Ä¢ Locative     ‚Äì spatial/temporal setting  
                    ‚Ä¢ Dative       ‚Äì recipient, purpose  
                    ‚Ä¢ Genitive     ‚Äì ownership, relation  
                    ‚Ä¢ Ablative     ‚Äì source, cause  
                    ‚Ä¢ Nom / Acc    ‚Äì subject vs. direct object (no helper)  
                    ‚Ä¢ Vocative     ‚Äì direct address

                    **Ambiguity reminder** ‚Äì If **one suffix stands for two cases**
                    (e.g., ‚Äì‡®à = Nom *and* Acc), *explain your semantic reason* for choosing.

                    **Oblique + Post-position lines** ‚Äì GurbƒÅ·πáƒ´ occasionally stacks a
                    post-position **after** an oblique form **and** after a direct form
                    (see examples with *‡®®‡®á‡®Ü‡®Ç*, *‡®∏‡®¨‡®¶‡©à*).  Either is valid‚Äîchoose the case
                    that best reflects the combined meaning.
                """).strip() + "\n\n"
                
            elif entry["Type"] == "Pronoun / ‡®™‡©ú‡®®‡®æ‡®Ç‡®µ":
                # ‚îÄ‚îÄ‚îÄ Pronoun block with enriched cross-category logic ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                implicit_note = textwrap.dedent("""\
                    **PRONOUNS ‚Äì INFLECTIONS, IDENTITY & IMPLIED MEANINGS**  
                    In GurbƒÅ·πáƒ´, pronouns diverge from noun patterns and inflect by **person, number, and gender**.  
                    Their meaning is sometimes explicit (like ‡®Æ‡©à‡®Ç = I), but often **derived from Darpan's gloss**.

                    **Core Steps to Identify the Case**  
                    1. **Read the gloss literally.**  
                    If it adds a helper like *to, from, with, in*, this signals an **implicit post-position**.  
                    Match it with:  
                    ‚Ä¢ `‡®®‡©Ç‡©∞`, `‡®≤‡®à` ‚Üí Dative  
                    ‚Ä¢ `‡®¶‡®æ/‡®¶‡©Ä/‡®¶‡©á`, `‡®ï‡®æ/‡®ï‡©Ä/‡®ï‡©á` ‚Üí Genitive  
                    ‚Ä¢ `‡®§‡©ã‡®Ç`, `‡®â‡®§‡©ã‡®Ç`, `‡®∏‡©á`, `‡®Ö‡®§‡©á` ‚Üí Ablative  
                    ‚Ä¢ `‡®®‡®æ‡®≤`, `‡®µ‡®ø‡©±‡®ö`, `‡®â‡©±‡®§‡©á`, `‡®ï‡©ã‡®≤`, `‡®Ö‡©∞‡®¶‡®∞`, etc. ‚Üí Instrumental / Locative  
                    ‚Ä¢ `O`, `Hey` ‚Üí Vocative

                    2. **Check form compatibility.**  
                    Every person/gender/number has a finite set of endings (see below).  
                    Match the surface form to a standard **canonical pronoun**.

                    3. **For Relative / Interrogative / Reflexive / Indefinite types**,  
                    blend case logic with **semantic roles**: e.g.,  
                    ‚Ä¢ ‡®ï‡®ø‡®∏ ‡®®‡©Ç‡©∞ ‚Üí ‚Äúto whom‚Äù ‚Üí Dative  
                    ‚Ä¢ ‡®ú‡®ø‡®∏ ‡®§‡©á ‚Üí ‚Äúon whom‚Äù ‚Üí Locative  
                    ‚Ä¢ ‡®Ü‡®™‡®£‡©á ‡®π‡©Ä ‡®Ü‡®™ ‚Üí Reflexive emphatic  
                    ‚Ä¢ ‡®ú‡®ø‡®∏ ‡®¶‡©Ä, ‡®ú‡®ø‡®∏ ‡®¶‡®æ ‚Üí Genitive relative

                    _Postpositions are often absent but implied‚Äîyour judgment is key._  
                    Also note: **GurbƒÅ·πáƒ´ often uses plural pronouns to show respect.**
                """).strip() + "\n\n"

                common_sense_note = textwrap.dedent("""\
                    **PRONOUN SEMANTIC CHECK ‚Äì ROLE IN MEANINGFUL CONTEXT**  
                    Pronouns are **not just replacements for nouns**‚Äîthey carry personhood, humility, or divinity.

                    ‚úÖ Use this test logic:  
                    - **Is the pronoun the subject?** ‚Üí Nom  
                    - **Receiving the action?** ‚Üí Acc  
                    - **Belonging to someone?** ‚Üí Gen  
                    - **Given to someone?** ‚Üí Dat  
                    - **Means or tool or ‚Äúwith‚Äù sense?** ‚Üí Inst  
                    - **Place or inner state?** ‚Üí Loc  
                    - **Directly addressed?** ‚Üí Voc  

                    ‚ö†Ô∏è For overlapping forms:  
                    - Use the Darpan helper (e.g., "to me", "from them", "by whom")  
                    - Ask what semantic role the pronoun plays **in that line**  
                    - e.g., ‚Äú‡®Æ‡©à‚Äù may be Nom or Acc depending on meaning

                    **Special Guidance per Category**  
                    - **Reflexive** (‡®Ü‡®™, ‡®Ü‡®™‡®£‡©á): Self-reference or emphasis  
                    - **Relative/Correlative** (‡®ú‡©ã...‡®∏‡©ã): Link two ideas (doer/result, condition/result)  
                    - **Interrogative** (‡®ï‡©å‡®£, ‡®ï‡®ø‡®∏): Structure question  
                    - **Indefinite** (‡®ï‡©ã‡®à, ‡®∏‡®≠): Ambiguous subject  
                    - **Honorific 2nd Person** (‡®§‡©Å‡®∏‡©Ä‡®Ç, ‡®§‡©Å‡®Æ): May appear plural but refer to one Divine

                    **Final Tip**: Plural/oblique/abstract usage may reflect poetic or spiritual nuance more than grammar. Follow meaning.
                """).strip() + "\n\n"

                ending_cheat_sheet = textwrap.dedent("""\
                    **PRONOUN CASE ENDINGS ‚Äì EXAMPLES ACROSS CATEGORIES**

                    üîπ **Valid Number / Gender Combinations per Category**  
                    *(Use this to cross-check if your feature choices are logically possible)*

                    - **1st Person / ‡®â‡©±‡®§‡®Æ ‡®™‡©Å‡®∞‡®ñ**  
                    ‚Äì Number: Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å  
                    ‚Äì Gender: Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï

                    - **2nd Person / ‡®Æ‡®ß‡®Æ ‡®™‡©Å‡®∞‡®ñ**  
                    ‚Äì Number: Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å  
                    ‚Äì Gender: Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï

                    - **3rd Person / ‡®Ö‡®®‡®Ø ‡®™‡©Å‡®∞‡®ñ**  
                    ‚Äì Number: Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å  
                    ‚Äì Gender: Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó, Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä, Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï

                    - **CoRelative / ‡®Ö‡®®‡©Å‡®∏‡©∞‡®¨‡©∞‡®ß**  
                    ‚Äì Number: Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å  
                    ‚Äì Gender: Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó, Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä, Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï

                    - **Relative / ‡®∏‡©∞‡®¨‡©∞‡®ß**  
                    ‚Äì Number: Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å  
                    ‚Äì Gender: Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó, Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä, Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï

                    - **Interrogative / ‡®™‡©ç‡®∞‡®∂‡®® ‡®µ‡®æ‡®ö‡®ï**  
                    ‚Äì Number: Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å  
                    ‚Äì Gender: Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó, Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä, Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï

                    - **Reflexive / ‡®®‡®ø‡®ú ‡®µ‡®æ‡®ö‡®ï**  
                    ‚Äì Number: Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å  
                    ‚Äì Gender: Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó, Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä, Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï

                    - **Indefinite / ‡®Ö‡®®‡®ø‡®∏‡®ö‡©á ‡®µ‡®æ‡®ö‡®ï**  
                    ‚Äì Number: Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å  
                    ‚Äì Gender: Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó, Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä, Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï

                    _‚ú≥ Note: ‚ÄúTrans‚Äù (‡®®‡®™‡©Å‡®Ç‡®∏‡®ï) appears for most categories due to universal/neutral references or poetic plurality._

                    **1st Person / ‡®â‡©±‡®§‡®Æ ‡®™‡©Å‡®∞‡®ñ Pronouns ‚Äì Case Examples**
                    - Ablative ‡®Ö‡®™‡®æ‡®¶‡®æ‡®®: ‡®Æ‡©à / ‡®Æ‡©∞‡®ù‡®π‡©Å / ‡®π‡®Æ ‡®§‡©á
                    - Accusative ‡®ï‡®∞‡®Æ: ‡®Æ‡©à / ‡®Æ‡©à‡®®‡©ã / ‡®Æ‡©ã ‡®ï‡®â / ‡®Æ‡©ã‡®ï‡®â / ‡®Æ‡©ã‡®π‡®ø / ‡®Æ‡©∞‡®û‡©Å / ‡®π‡®Æ / ‡®π‡®Æ‡®π‡®ø
                    - Dative ‡®∏‡©∞‡®™‡©ç‡®¶‡®æ‡®®: ‡®Æ‡®æ‡®ù‡©à / ‡®Æ‡©Å‡®ù‡®π‡®ø / ‡®Æ‡©Å‡®ù‡©à / ‡®Æ‡©Å‡®π‡®ø / ‡®Æ‡©Ç / ‡®Æ‡©à / ‡®Æ‡©à‡®®‡©ã / ‡®Æ‡©ã ‡®ï‡®â / ‡®Æ‡©ã‡®π‡®ø / ‡®π‡®Æ (‡®ï‡®â) / ‡®π‡®Æ‡®π‡©Å / ‡®π‡®Æ‡®æ‡®∞‡©à
                    - Genitive ‡®∏‡©∞‡®¨‡©∞‡®ß: ‡®Ö‡®∏‡®æ / ‡®Ö‡®∏‡®æ‡®°‡©ú‡®æ / ‡®Ö‡®∏‡®æ‡®π / ‡®Ö‡®∏‡®æ‡©ú‡®æ / ‡®Æ‡®π‡®ø‡©∞‡®ú‡®æ / ‡®Æ‡®π‡®ø‡©∞‡®°‡®æ / ‡®Æ‡®æ / ‡®Æ‡©Ç / ‡®Æ‡©á‡®∞‡®â / ‡®Æ‡©á‡®∞‡®æ / ‡®Æ‡©á‡®∞‡©Ä / ‡®Æ‡©à / ‡®Æ‡©à‡®°‡®æ / ‡®Æ‡©ã‡®∞ / ‡®Æ‡©ã‡®∞‡®≤‡®æ / ‡®Æ‡©ã‡®∞‡®≤‡©ã / ‡®Æ‡©ã‡®∞‡®æ / ‡®Æ‡©ã‡®∞‡©Ä / ‡®Æ‡©ã‡®∞‡©á / ‡®Æ‡©ã‡®π‡®ø / ‡®Æ‡©∞‡®û‡©Å / ‡®π‡®Æ‡®∞‡®æ / ‡®π‡®Æ‡®∞‡©à / ‡®π‡®Æ‡®∞‡©ã / ‡®π‡®Æ‡®æ‡®∞‡®æ
                    - Locative ‡®Ö‡®ß‡®ø‡®ï‡®∞‡®£: ‡®Æ‡©Å‡®ù ‡®Æ‡®π‡®ø / ‡®Æ‡©Å‡®ù‡®π‡®ø ‡®™‡®π‡®ø / ‡®Æ‡©Å‡®ù‡©Å / ‡®Æ‡©Å‡®ù‡©à / ‡®Æ‡©á‡®∞‡©à / ‡®Æ‡©à ‡®Ö‡©∞‡®§‡®∞‡®ø / ‡®Æ‡©à ‡®µ‡®ø‡®ö‡®ø / ‡®Æ‡©ã ‡®Æ‡®π‡®ø / ‡®Æ‡©∞‡®ù‡©Å / ‡®π‡®Æ / ‡®π‡®Æ‡®∞‡©à / ‡®π‡®Æ‡®æ‡®∞‡©à
                    - Nominative ‡®ï‡®∞‡®§‡®æ: ‡®Ö‡®∏‡®æ / ‡®Ö‡®∏‡©Ä / ‡®Æ‡©Ç / ‡®Æ‡©Ç‡®Ç / ‡®Æ‡©à / ‡®Æ‡©ã‡®π‡®ø / ‡®π‡®â / ‡®π‡®Æ / ‡®π‡®Æ‡®π‡©Å

                    **2nd Person / ‡®Æ‡®ß‡®Æ ‡®™‡©Å‡®∞‡®ñ Pronouns ‚Äì Case Examples**
                    - Ablative ‡®Ö‡®™‡®æ‡®¶‡®æ‡®®: ‡®§‡©Å‡®ù ‡®§‡©á / ‡®§‡©Å‡®ù‡©à / ‡®§‡©Å‡®ù‡©à ‡®§‡©á / ‡®§‡©Å‡®ù‡©à ‡®™‡®π‡®ø / ‡®§‡©Å‡®ß‡®π‡©Å / ‡®§‡©Å‡®ß‡©à ‡®§‡©á / ‡®§‡©Å‡®Æ ‡®§‡©á
                    - Accusative ‡®ï‡®∞‡®Æ: ‡®§‡®â / ‡®§‡©Å‡®ù / ‡®§‡©Å‡®ù‡®π‡®ø / ‡®§‡©Å‡®ù‡©Å / ‡®§‡©Å‡®ù‡©à / ‡®§‡©Å‡®ß / ‡®§‡©Å‡®ß ‡®®‡©ã / ‡®§‡©Å‡®ß‡©Å / ‡®§‡©Å‡®ß‡©Å‡®®‡©ã / ‡®§‡©Å‡®ß‡©à / ‡®§‡©Å‡®Æ / ‡®§‡©Å‡®Æ‡®π‡®ø / ‡®§‡©Å‡®π‡®®‡©ã / ‡®§‡©Å‡®π‡®ø / ‡®§‡©Ç / ‡®§‡©Ç‡©∞ / ‡®§‡©ã‡®π‡®ø / ‡®§‡©ã‡®π‡©Ä
                    - Dative ‡®∏‡©∞‡®™‡©ç‡®¶‡®æ‡®®: ‡®§‡®â / ‡®§‡©Å‡®ù‡®π‡®ø / ‡®§‡©Å‡®ù‡©Å / ‡®§‡©Å‡®ù‡©à / ‡®§‡©Å‡®ß / ‡®§‡©Å‡®ß‡©Å / ‡®§‡©Å‡®Æ / ‡®§‡©Å‡®Æ ‡®ï‡®â / ‡®§‡©Å‡®∏‡®æ / ‡®§‡©Å‡®π‡®ø / ‡®§‡©à / ‡®§‡©à ‡®ï‡©Ç‡©∞ / ‡®§‡©ã‡®π‡®ø / ‡®•‡©á / ‡®•‡©à‡®Ç
                    - Genitive ‡®∏‡©∞‡®¨‡©∞‡®ß: ‡®§‡®â / ‡®§‡®µ / ‡®§‡®π‡®ø‡©∞‡®ú‡©Ä / ‡®§‡®ø‡®π‡®æ‡®∞‡©à / ‡®§‡©Å / ‡®§‡©Å‡®Ö / ‡®§‡©Å‡®ù‡®π‡®ø / ‡®§‡©Å‡®Æ‡®∞‡®æ / ‡®§‡©Å‡®Æ‡®∞‡©Ä / ‡®§‡©Å‡®Æ‡®∞‡©á / ‡®§‡©Å‡®Æ‡®æ‡®∞‡©Ä / ‡®§‡©Å‡®π‡®æ‡®∞‡©á / ‡®§‡©Ç / ‡®§‡©á‡®∞‡®â / ‡®§‡©á‡®∞‡®æ / ‡®§‡©á‡®∞‡®ø‡®Ü / ‡®§‡©á‡®∞‡©Ä / ‡®§‡©á‡®∞‡©á / ‡®§‡©á‡®∞‡©ã / ‡®§‡©à‡®°‡®æ / ‡®§‡©ã‡®∞ / ‡®§‡©ã‡®π‡®ø / ‡®•‡®æ‡®∞‡©Ä / ‡®•‡®æ‡®∞‡©á
                    - Locative ‡®Ö‡®ß‡®ø‡®ï‡®∞‡®£: ‡®§‡©Å‡®ù / ‡®§‡©Å‡®ù ‡®π‡©Ä / ‡®§‡©Å‡®ù‡®π‡®ø / ‡®§‡©Å‡®ù‡©à / ‡®§‡©Å‡®ù‡©à ‡®∏‡®æ‡®ù‡®∞‡®ø / ‡®§‡©Å‡®ß‡©Å / ‡®§‡©Å‡®ß‡©à / ‡®§‡©Å‡®Æ / ‡®§‡©Å‡®Æ‡®π‡®ø / ‡®§‡©ã‡®π‡®ø
                    - Nominative ‡®ï‡®∞‡®§‡®æ: ‡®§‡®â / ‡®§‡©Å ‡®π‡©Ä / ‡®§‡©Å‡®ù / ‡®§‡©Å‡®ù‡®π‡®ø / ‡®§‡©Å‡®ù‡©à / ‡®§‡©Å‡®ß‡©Å / ‡®§‡©Å‡®ß‡©à / ‡®§‡©Å‡®Æ / ‡®§‡©Å‡®Æ ‡®π‡©Ä / ‡®§‡©Å‡®Æ‡®π‡®ø / ‡®§‡©Å‡®Æ‡©à / ‡®§‡©Å‡®∏‡©Ä / ‡®§‡©Å‡®π‡©Ä / ‡®§‡©Ç / ‡®§‡©Ç ‡®π‡©à / ‡®§‡©Ç‡®Ç / ‡®§‡©Ç‡®π‡©à / ‡®§‡©à / ‡®§‡©à‡®Ç / ‡®§‡©ã‡®π‡®ø

                    **3rd Person / ‡®Ö‡®®‡®Ø ‡®™‡©Å‡®∞‡®ñ Pronouns ‚Äì Case Examples**
                    - Ablative ‡®Ö‡®™‡®æ‡®¶‡®æ‡®®: ‡®á‡®® / ‡®á‡®∏ (‡®§‡©á) / ‡®â‡®Ü / ‡®â‡®® (‡®§‡©á) / ‡®â‡®®‡®æ / ‡®â‡®∏ / ‡®ì‡®®‡®æ‡©ç
                    - Accusative ‡®ï‡®∞‡®Æ: ‡®á‡®∏‡®π‡®ø / ‡®á‡®∏‡©Å / ‡®á‡®π / ‡®á‡®π‡©Å / ‡®â‡®Ü‡®π‡®ø / ‡®â‡®á / ‡®â‡®® / ‡®â‡®∏ / ‡®â‡®∏‡©Å / ‡®â‡®π / ‡®è‡®∏ / ‡®è‡®π‡®æ / ‡®è‡®π‡®ø / ‡®ì‡®á / ‡®ì‡®à / ‡®ì‡®®‡®æ / ‡®ì‡®∏ / ‡®ì‡®∏‡©Å / ‡®ì‡®π‡©Å / ‡®§‡®ø‡®® / ‡®§‡©á / ‡®µ‡®æ / ‡®µ‡®æ‡®π‡©Ä / ‡®∏‡©á / ‡®∏‡©ã‡®ä
                    - Dative ‡®∏‡©∞‡®™‡©ç‡®¶‡®æ‡®®: ‡®á‡®∏ / ‡®á‡®∏‡©Å / ‡®â‡®Ü / ‡®â‡®® (‡®ï‚Äå‡®â) / ‡®â‡®®‡®æ / ‡®â‡®∏ / ‡®â‡®∏‡©Å / ‡®è‡®∏ / ‡®ì‡®®‡®æ‡©ç / ‡®ì‡®∏ / ‡®ì‡®∏‡©Å
                    - Genitive ‡®∏‡©∞‡®¨‡©∞‡®ß: ‡®Ö‡®∏‡®ó‡®æ / ‡®á‡®® / ‡®á‡®∏ / ‡®â‡®Ü / ‡®â‡®Ü (‡®ï‡®æ) / ‡®â‡®® (‡®ï‡©Ä) / ‡®â‡®®‡®æ / ‡®â‡®∏ (‡®ï‡®æ) / ‡®â‡®∏‡®ó‡®æ / ‡®â‡®∏‡©Å / ‡®ì‡®®‡®æ / ‡®ì‡®∏‡©Å / ‡®§‡®ø‡®® / ‡®§‡®ø‡®®‡®æ / ‡®§‡®ø‡®∏‡©Å / ‡®µ‡®æ (‡®ï‡®æ) (‡®ï‡©à) (‡®ï‡©á)
                    - Instrumental ‡®ï‡®∞‡®£: ‡®á‡®§‡©Å (‡®ï‡®∞‡®ø)
                    - Locative ‡®Ö‡®ß‡®ø‡®ï‡®∞‡®£: ‡®á‡®∏ / ‡®á‡®∏‡©Å (‡®Ü‡®ó‡©à) / ‡®â‡®∏‡©Å / ‡®ì‡®®‡®æ (‡®™‡®ø‡®õ‡©à) / ‡®ì‡®∏‡©Å / ‡®µ‡®æ‡®π‡©Ç
                    - Nominative ‡®ï‡®∞‡®§‡®æ: ‡®á‡®® / ‡®á‡®®‡®ø / ‡®á‡®π / ‡®á‡®π‡©Å / ‡®â‡®® / ‡®â‡®®‡®ø / ‡®â‡®π / ‡®â‡®π‡©Å / ‡®è‡®π / ‡®è‡®π‡®ø / ‡®è‡®π‡©Å / ‡®ì‡®á / ‡®ì‡®®‡®ø / ‡®ì‡®®‡©Ä / ‡®ì‡®π / ‡®ì‡®π‡®æ / ‡®ì‡®π‡®ø / ‡®ì‡®π‡©Ä / ‡®ì‡®π‡©Å / ‡®§‡®ø‡®® / ‡®§‡®ø‡®®‡®π‡®ø / ‡®§‡©á / ‡®§‡©á‡®ä / ‡®∏‡®æ / ‡®∏‡©á / ‡®∏‡©ã / ‡®∏‡©ã‡®á / ‡®∏‡©ã‡®à

                    **CoRelative / ‡®Ö‡®®‡©Å‡®∏‡©∞‡®¨‡©∞‡®ß Pronouns ‚Äì Case Examples**
                    - Ablative ‡®Ö‡®™‡®æ‡®¶‡®æ‡®®: ‡®§‡®ø‡®∏ (‡®§‡©á)
                    - Accusative ‡®ï‡®∞‡®Æ: ‡®§‡®æ‡®∏‡©Å / ‡®§‡®æ‡®∏‡©Å (‡®ï‡®â) / ‡®§‡®æ‡®π‡®ø / ‡®§‡®ø‡®® / ‡®§‡®ø‡®®‡©ç / ‡®§‡®ø‡®∏‡®π‡®ø / ‡®§‡®ø‡®∏‡©Å / ‡®§‡®ø‡®∏‡©à / ‡®§‡®ø‡®π / ‡®§‡©á / ‡®§‡©à
                    - Dative ‡®∏‡©∞‡®™‡©ç‡®¶‡®æ‡®®: ‡®§‡®æ‡®∏‡©Å / ‡®§‡®ø‡®® / ‡®§‡®ø‡®® (‡®ï‡®â) / ‡®§‡®ø‡®®‡®π‡©Å / ‡®§‡®ø‡®®‡®π‡©Ç (‡®ï‡®â) / ‡®§‡®ø‡®®‡®æ / ‡®§‡®ø‡®®‡®æ‡©ç / ‡®§‡®ø‡®∏ (‡®ï‡®â) / ‡®§‡®ø‡®∏ (‡®®‡©ã) / ‡®§‡®ø‡®∏ ‡®π‡©Ä / ‡®§‡®ø‡®∏‡®π‡®ø / ‡®§‡®ø‡®∏‡©Å / ‡®§‡®ø‡®∏‡©à / ‡®§‡®ø‡®π / ‡®§‡®ø‡©∞‡®®‡®æ / ‡®§‡©à
                    - Genitive ‡®∏‡©∞‡®¨‡©∞‡®ß: ‡®§‡®æ / ‡®§‡®æ‡®∏‡©Å / ‡®§‡®æ‡®π‡©Ç (‡®ï‡©ã) / ‡®§‡®ø‡®® / ‡®§‡®ø‡®® (‡®ï‡©Ä) / ‡®§‡®ø‡®®‡®æ / ‡®§‡®ø‡®®‡®æ‡©ç / ‡®§‡®ø‡®®‡®æ‡©ú‡®æ / ‡®§‡®ø‡®®‡©ç / ‡®§‡®ø‡®∏ (‡®ï‡®æ) / ‡®§‡®ø‡®∏ (‡®ï‡©Ä) / ‡®§‡®ø‡®∏ (‡®ï‡©á) / ‡®§‡®ø‡®∏ (‡®π‡®ø) / ‡®§‡®ø‡®∏ (‡®π‡©Ä) / ‡®§‡®ø‡®∏‡®π‡®ø / ‡®§‡®ø‡®∏‡©Å / ‡®§‡®ø‡®∏‡©à / ‡®§‡®ø‡®π / ‡®§‡©∞‡®®‡®ø (‡®ñ‡©á)
                    - Instrumental ‡®ï‡®∞‡®£: ‡®§‡®ø‡®§‡©Å
                    - Locative ‡®Ö‡®ß‡®ø‡®ï‡®∞‡®£: ‡®§‡®æ‡®∏ / ‡®§‡®æ‡®∏‡©Å / ‡®§‡®æ‡®π‡®ø (‡®Æ‡©à) / ‡®§‡®ø‡®§ (‡®π‡©Ä) / ‡®§‡®ø‡®§‡©Å / ‡®§‡®ø‡®®‡®ø / ‡®§‡®ø‡®∏‡©Å (‡®Æ‡®æ‡®π‡®ø) / ‡®§‡®ø‡®π‡®ø
                    - Nominative ‡®ï‡®∞‡®§‡®æ: ‡®ì‡®á / ‡®§‡®ø‡®® / ‡®§‡®ø‡®® ‡®π‡©Ä / ‡®§‡®ø‡®®‡®π‡®ø / ‡®§‡®ø‡®®‡®π‡©Ä / ‡®§‡®ø‡®®‡®π‡©Ç / ‡®§‡®ø‡®®‡®ø / ‡®§‡®ø‡®®‡©Ä / ‡®§‡®ø‡®®‡©ç / ‡®§‡®ø‡®π / ‡®§‡©á / ‡®∏‡®æ / ‡®∏‡®æ‡®à / ‡®∏‡®ø / ‡®∏‡©Å / ‡®∏‡©á / ‡®∏‡©á‡®á / ‡®∏‡©á‡®à / ‡®∏‡©ã / ‡®∏‡©ã‡®à / ‡®∏‡©ã‡®ä

                    **Indefinite / ‡®Ö‡®®‡®ø‡®∏‡®ö‡©á ‡®µ‡®æ‡®ö‡®ï Pronouns ‚Äì Case Examples**
                    - Ablative ‡®Ö‡®™‡®æ‡®¶‡®æ‡®®: ‡®∏‡®≠ (‡®¶‡©Ç) / ‡®π‡®≠‡®æ‡®π‡©Ç‡©∞ / ‡®π‡©ã‡®∞‡®®‡®ø / ‡®π‡©ã‡®∞‡®ø‡®Ç‡®ì
                    - Accusative ‡®ï‡®∞‡®Æ: ‡®Ö‡®â‡®∞‡®® / ‡®Ö‡®ó‡®≤‡®æ / ‡®Ö‡®µ‡®∞ / ‡®Ö‡®µ‡®∞‡®π‡®ø / ‡®Ö‡®µ‡®∞‡®æ / ‡®Ö‡®µ‡®∞‡©Ä (‡®®‡©ã) / ‡®Ö‡®µ‡®∞‡©Å / ‡®á‡®ï‡®®‡®æ / ‡®á‡®ï‡®®‡®æ‡©ç / ‡®á‡®ï‡®ø / ‡®á‡®ï‡©Å / ‡®á‡®§‡®®‡®æ (‡®ï‡©Å) / ‡®á‡®§‡®®‡©Ä / ‡®è‡®ï‡®∏‡©à / ‡®è‡®ï‡©Ä / ‡®è‡®§‡®æ / ‡®è‡®§‡©á / ‡®ï‡®õ‡©Å‡®Ü / ‡®ï‡®π‡®æ / ‡®ï‡®ø / ‡®ï‡®ø‡®Ü (‡®ï‡®ø‡®õ‡©Å) / ‡®ï‡®ø‡®õ‡©Å / ‡®ï‡®ø‡®ù‡©Å / ‡®ï‡®ø‡®§‡©Ä / ‡®ï‡®ø‡®∏ (‡®®‡©ã) / ‡®ï‡®ø‡®∏‡®π‡®ø / ‡®ï‡®ø‡®∏‡©Å / ‡®ï‡®ø‡®∏‡©à / ‡®ï‡®ø‡®π‡©Å / ‡®ï‡©ã‡®à / ‡®ò‡®£‡©á‡®∞‡©Ä / ‡®ú‡©á‡®§‡®æ / ‡®ú‡©á‡®§‡©Ä‡®Ü / ‡®§‡©á‡®§‡®æ / ‡®•‡©ã‡©ú‡®æ / ‡®•‡©ã‡©ú‡©Ä / ‡®¨‡®π‡©Å‡®§‡®æ / ‡®¨‡®π‡©Å‡®§‡©Å / ‡®¨‡®π‡©Å‡®§‡©ã / ‡®¨‡®æ‡®π‡®∞‡®æ / ‡®∏‡®ó‡®≤ / ‡®∏‡®≠ / ‡®∏‡®≠‡®®‡®æ / ‡®∏‡®≠‡®∏‡©Å / ‡®∏‡®≠‡®∏‡©à (‡®®‡©ã) / ‡®∏‡®≠‡®ø / ‡®∏‡®≠‡©Å (‡®ï‡®ø‡®õ‡©Å) / ‡®∏‡®≠‡©Å (‡®ï‡®ø‡®π‡©Å) / ‡®∏‡®≠‡©à / ‡®π‡®≠ / ‡®π‡®≠ (‡®ï‡®ø‡®õ‡©Å) / ‡®π‡®ø‡®ï‡©Å / ‡®π‡®ø‡®ï‡©ã / ‡®π‡©ã‡®∞‡®®‡®æ (‡®®‡©ã) / ‡®π‡©ã‡®∞‡®∏‡©Å / ‡®π‡©ã‡®∞‡©Å
                    - Dative ‡®∏‡©∞‡®™‡©ç‡®¶‡®æ‡®®: ‡®á‡®ï‡®®‡®æ / ‡®ï‡®π‡©Ä / ‡®ï‡®æ‡®π‡©Ç / ‡®ï‡®ø‡®®‡©à / ‡®ï‡®ø‡®∏ (‡®π‡©Ä) / ‡®ï‡®ø‡®∏‡©à / ‡®∏‡®≠‡®∏‡©Å / ‡®∏‡®≠‡®∏‡©à
                    - Genitive ‡®∏‡©∞‡®¨‡©∞‡®ß: ‡®Ö‡®µ‡®∞ / ‡®á‡®ï‡®®‡®æ / ‡®á‡®ï‡®®‡®æ‡©ç / ‡®ï‡®æ‡®π‡©Ç / ‡®ï‡®ø‡®∏‡©à / ‡®ï‡©à‡®π‡©Ä / ‡®∏‡®≠‡®®‡®æ / ‡®∏‡®≠‡®∏‡©à
                    - Instrumental ‡®ï‡®∞‡®£: ‡®ï‡®æ‡®π‡©Ç / ‡®ï‡®ø‡®®‡©à / ‡®π‡©ã‡®∞‡®§‡©Å
                    - Locative ‡®Ö‡®ß‡®ø‡®ï‡®∞‡®£: ‡®á‡®ï‡®®‡©Ä / ‡®ï‡®ø‡®∏‡©Å (‡®®‡®æ‡®≤‡®ø)
                    - Nominative ‡®ï‡®∞‡®§‡®æ: (‡®π‡©ã‡®∞) ‡®ï‡©á‡®§‡©Ä / ‡®Ö‡®â‡®∞ / ‡®Ö‡®â‡®∞‡©Å (‡®ï‡©ã) / ‡®Ö‡®®‡©á‡®ï / ‡®Ö‡®µ‡®∞‡®ø (‡®∏‡®≠‡®ø) / ‡®Ö‡®µ‡®∞‡©Å (‡®ï‡®õ‡©Å) / ‡®Ö‡®µ‡®∞‡©á / ‡®á‡®ï‡®®‡®æ / ‡®á‡®ï‡®®‡©Ä / ‡®á‡®ï‡®®‡©à / ‡®á‡®ï‡®ø / ‡®á‡®ï‡©Å / ‡®è‡®ï / ‡®è‡®ï‡®π‡®ø / ‡®è‡®ï‡©Å / ‡®è‡®ï‡©à / ‡®ï‡®â‡®£‡©Å / ‡®ï‡®â‡®®‡©Å / ‡®ï‡®õ‡©Å / ‡®ï‡®π / ‡®ï‡®π‡®æ / ‡®ï‡®æ / ‡®ï‡®æ‡®à / ‡®ï‡®æ‡®π‡©Ç / ‡®ï‡®ø‡®Ü / ‡®ï‡®ø‡®õ‡©Å / ‡®ï‡®ø‡®§‡©Ä / ‡®ï‡®ø‡®® (‡®π‡©Ä) / ‡®ï‡®ø‡®®‡®π‡®ø / ‡®ï‡®ø‡®®‡®π‡©Ä / ‡®ï‡®ø‡®®‡®π‡©Ç / ‡®ï‡®ø‡®®‡®ø / ‡®ï‡®ø‡®®‡©à / ‡®ï‡®ø‡®∏ ‡®π‡©Ä / ‡®ï‡®ø‡®π‡©Å / ‡®ï‡©á / ‡®ï‡©á‡®á / ‡®ï‡©á‡®à / ‡®ï‡©á‡®§‡®ï / ‡®ï‡©á‡®§‡®æ / ‡®ï‡©á‡®§‡©á / ‡®ï‡©ã / ‡®ï‡©ã‡®á / ‡®ï‡©ã‡®à / ‡®ï‡©ã‡®ä / ‡®ò‡®£‡©Ä / ‡®ò‡®£‡©á / ‡®ú‡©á‡®§‡©Ä / ‡®§‡©á‡®§‡©Ä / ‡®¨‡®π‡©Å / ‡®¨‡®π‡©Å‡®§‡®æ / ‡®¨‡®π‡©Å‡®§‡©á‡®∞‡©Ä / ‡®µ‡®ø‡®∞‡®≤‡©á / ‡®∏‡®ó‡®≤ / ‡®∏‡®ó‡®≤‡©Ä / ‡®∏‡®ó‡®≤‡©Ä‡®Ü / ‡®∏‡®ó‡®≤‡©á ‡®ï‡©á / ‡®∏‡®≠ / ‡®∏‡®≠‡®®‡®æ / ‡®∏‡®≠‡®®‡©Ä / ‡®∏‡®≠‡®π‡®ø / ‡®∏‡®≠‡®æ / ‡®∏‡®≠‡®ø / ‡®∏‡®≠‡©Å (‡®ï‡®ø‡®õ‡©Å) / ‡®∏‡®≠‡©Å (‡®ï‡©ã) / ‡®∏‡®≠‡©Å (‡®ï‡©ã‡®á) / ‡®∏‡®≠‡©Å (‡®ï‡©ã‡®à) / ‡®∏‡®≠‡©á / ‡®∏‡®æ‡®∞‡©Ä / ‡®π‡®≠‡®ø / ‡®π‡®≠‡©á / ‡®π‡®ø‡®ï‡®®‡©Ä / ‡®π‡®ø‡®ï‡®ø / ‡®π‡®ø‡®ï‡©Å / ‡®π‡©ã‡®∞‡®ø / ‡®π‡©ã‡®∞‡©Å

                    **Interrogative / ‡®™‡©ç‡®∞‡®∂‡®® ‡®µ‡®æ‡®ö‡®ï Pronouns ‚Äì Case Examples**
                    - Accusative ‡®ï‡®∞‡®Æ: ‡®ï‡®π‡®æ / ‡®ï‡®æ‡®π‡®ø / ‡®ï‡®ø‡®Ü / ‡®ï‡®ø‡®∏‡©Å
                    - Dative ‡®∏‡©∞‡®™‡©ç‡®¶‡®æ‡®®: ‡®ï‡®æ (‡®ï‡®â) / ‡®ï‡®ø‡®®‡®æ‡®π / ‡®ï‡®ø‡®∏ (‡®ï‡®â) / ‡®ï‡®ø‡®∏‡©Å / ‡®ï‡©à
                    - Genitive ‡®∏‡©∞‡®¨‡©∞‡®ß: ‡®ï‡®ø‡®∏‡©Å
                    - Locative ‡®Ö‡®ß‡®ø‡®ï‡®∞‡®£: ‡®ï‡®æ (‡®™‡®π‡®ø) / ‡®ï‡®æ (‡®∏‡®ø‡®â) / ‡®ï‡®ø‡®∏‡©Å (‡®™‡®π‡®ø) / ‡®ï‡©à (‡®™‡®π‡®ø)
                    - Nominative ‡®ï‡®∞‡®§‡®æ: ‡®ï‡®â‡®£‡©Å / ‡®ï‡®â‡®® / ‡®ï‡®µ‡®£ / ‡®ï‡®µ‡®® / ‡®ï‡®µ‡®®‡©Å / ‡®ï‡®µ‡®®‡©à / ‡®ï‡®ø‡®®‡®ø / ‡®ï‡©Å‡®®‡©Å / ‡®ï‡©ã

                    **Reflexive / ‡®®‡®ø‡®ú ‡®µ‡®æ‡®ö‡®ï Pronouns ‚Äì Case Examples**
                    - Ablative ‡®Ö‡®™‡®æ‡®¶‡®æ‡®®: ‡®Ü‡®™‡®∏ (‡®§‡©á) / ‡®Ü‡®™‡®π‡©Å / ‡®Ü‡®™‡©å
                    - Accusative ‡®ï‡®∞‡®Æ: ‡®Ö‡®™‡®§‡©Å / ‡®Ü‡®™‡®§‡©Å / ‡®Ü‡®™‡®æ / ‡®Ü‡®™‡©Å
                    - Dative ‡®∏‡©∞‡®™‡©ç‡®¶‡®æ‡®®: ‡®Ü‡®™‡®∏ (‡®ï‡®â) / ‡®Ü‡®™‡©à (‡®®‡©ã)
                    - Genitive ‡®∏‡©∞‡®¨‡©∞‡®ß: ‡®Ö‡®™ / ‡®Ö‡®™‡®£‡®æ / ‡®Ö‡®™‡®®‡®æ / ‡®Ö‡®™‡®®‡©Ä / ‡®Ö‡®™‡®®‡©à / ‡®Ö‡®™‡©Å‡®®‡®æ / ‡®Ö‡®™‡©Å‡®®‡©Ä / ‡®Ü‡®™ / ‡®Ü‡®™‡®£ / ‡®Ü‡®™‡®£‡®æ / ‡®Ü‡®™‡®£‡©à / ‡®Ü‡®™‡®® / ‡®Ü‡®™‡®®‡®æ / ‡®Ü‡®™‡®æ
                    - Instrumental ‡®ï‡®∞‡®£: ‡®Ü‡®™‡©à (‡®®‡®æ‡®≤‡®ø)
                    - Locative ‡®Ö‡®ß‡®ø‡®ï‡®∞‡®£: ‡®Ü‡®™‡®π‡®ø / ‡®Ü‡®™‡®ø / ‡®Ü‡®™‡©à
                    - Nominative ‡®ï‡®∞‡®§‡®æ: ‡®Ü‡®™ (‡®π‡©Ä) / ‡®Ü‡®™‡®π‡®ø / ‡®Ü‡®™‡®ø / ‡®Ü‡®™‡©Ä‡®®‡©à‡©ç / ‡®Ü‡®™‡©á (‡®π‡©Ä) / ‡®Ü‡®™‡©à

                    **Relative / ‡®∏‡©∞‡®¨‡©∞‡®ß Pronouns ‚Äì Case Examples**
                    - Ablative ‡®Ö‡®™‡®æ‡®¶‡®æ‡®®: ‡®ú‡®ø‡®¶‡©Ç / ‡®ú‡®ø‡®∏ (‡®§‡©á) / ‡®ú‡®ø‡®π (‡®§‡©á)
                    - Accusative ‡®ï‡®∞‡®Æ: ‡®ú‡®æ (‡®ï‡®â) / ‡®ú‡®æ‡®∏‡©Å / ‡®ú‡®æ‡®π‡®ø / ‡®ú‡®ø / ‡®ú‡®ø‡®® / ‡®ú‡®ø‡®® (‡®ï‡®â) / ‡®ú‡®ø‡®®‡®æ / ‡®ú‡®ø‡®®‡©ç / ‡®ú‡®ø‡®∏‡®π‡®ø / ‡®ú‡®ø‡®∏‡©Å / ‡®ú‡®ø‡®π / ‡®ú‡©á‡®π‡©ú‡®æ / ‡®ú‡©ã / ‡®ú‡©ã‡®à ‡®ú‡©ã‡®à / ‡®Ø‡®æ‡®∏‡©Å
                    - Dative ‡®∏‡©∞‡®™‡©ç‡®¶‡®æ‡®®: ‡®ú‡®ø‡®® / ‡®ú‡®ø‡®®‡®æ / ‡®ú‡®ø‡®∏‡®π‡®ø / ‡®ú‡®ø‡®∏‡©Å / ‡®ú‡®ø‡®π / ‡®ú‡©à
                    - Genitive ‡®∏‡©∞‡®¨‡©∞‡®ß: ‡®ú‡®æ / ‡®ú‡®æ (‡®ï‡©à) / ‡®ú‡®æ (‡®Æ‡®π‡®ø) / ‡®ú‡®æ‡®∏‡©Å / ‡®ú‡®ø‡®® / ‡®ú‡®ø‡®® (‡®ï‡©á) / ‡®ú‡®ø‡®®‡®æ / ‡®ú‡®ø‡®®‡®æ (‡®ï‡©Ä) / ‡®ú‡®ø‡®®‡©ç / ‡®ú‡®ø‡®∏ (‡®ï‡®æ) / ‡®ú‡®ø‡®∏ (‡®ï‡©Ä) / ‡®ú‡®ø‡®∏ (‡®ï‡©á) / ‡®ú‡®ø‡®∏‡©Å / ‡®ú‡®ø‡®π
                    - Instrumental ‡®ï‡®∞‡®£: ‡®ú‡®ø‡®§‡©Å / ‡®ú‡®ø‡®π
                    - Locative ‡®Ö‡®ß‡®ø‡®ï‡®∞‡®£: ‡®ú‡®ø‡®§‡©Å / ‡®ú‡®ø‡®π
                    - Nominative ‡®ï‡®∞‡®§‡®æ: ‡®ú‡®ø / ‡®ú‡®ø‡®® / ‡®ú‡®ø‡®®‡®π‡®ø / ‡®ú‡®ø‡®®‡®π‡©Å / ‡®ú‡®ø‡®®‡®æ / ‡®ú‡®ø‡®®‡®æ‡©ç / ‡®ú‡®ø‡®®‡®ø / ‡®ú‡®ø‡®®‡©Ä / ‡®ú‡®ø‡®®‡©Ä‡©ç / ‡®ú‡®ø‡®®‡©ç / ‡®ú‡®ø‡®π / ‡®ú‡©Å / ‡®ú‡©ã / ‡®ú‡©ã‡®à

                    _Ending note: **‚Äì‡®â** is often **omitted** before postpositions like ‡®§‡©ã‡®Ç, ‡®®‡©Ç‡©∞, ‡®µ‡®ø‡®ö, ‡®§‡©á.  
                    e.g., **‡®§‡®ø‡®∏ ‡®π‡®•‡®ø** instead of **‡®§‡®ø‡®∏‡©Å ‡®π‡®•‡®ø**_
                """).strip() + "\n\n"

            elif entry["Type"] == "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£":
                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                # 3-B  IMPLICIT-NOTE  ‚Äì how to ‚Äúread‚Äù the gloss
                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                implicit_note = textwrap.dedent("""
                    **ADJECTIVES IN GURBƒÄ·πÜƒ™ ‚Äì AGREEMENT & HINTS FROM THE DARPAN GLOSS**

                    ‚Ä¢ An adjective always **agrees in gender & number** with the noun /
                    pronoun it qualifies.  Case is *not* tagged independently for adjectives;
                    if a noun shifts to an oblique form (due to post-positions like
                    `‡®®‡©Ç‡©∞, ‡®§‡©á, ‡®§‡©ã‡®Ç‚Ä¶`) the adjective may simply copy that *ending*.

                    ‚Ä¢ **Look at the helper words the Darpan adds**:
                    - If the gloss inserts a post-position after the noun
                        (*e.g.* ‚Äúto the **good** one‚Äù, ‚Äúin the **other** realm‚Äù), the adjective
                        will mirror whatever oblique ending the noun shows ‚Äì **but you still
                        classify the adjective only by Gender / Number / Class**.
                    - If the gloss repeats the adjective without a helper,
                        treat the form you see in the verse as the **direct** (base) form.

                    _Quick reminder ‚Äì common agreement endings_  
                    | Ending-class | Masc.Sg | Fem.Sg | Plural | Notes |
                    |--------------|---------|--------|--------|-------|
                    | **Mukta**    | ‚Äì‡®Ö      | ‚Äì‡®Æ‡©Å‡®ï‡®§‡®æ◊Ä **‡®Ö** dropped for fem./pl. |
                    | **KannƒÅ**    | ‚Äì‡®Ü      | ‚Äì‡®à     | ‚Äì‡®è     | |
                    | **SihƒÅrƒ´**   | ‚Äì‡®ø      | ‚Äì‡®ø      | ‚Äì‡©á      | |
                    | **BihƒÅrƒ´**   | ‚Äì‡©Ä      | ‚Äì‡®à     | ‚Äì‡®è/‚Äì‡®à‡®Ü‡®Ç| |

                    _When in doubt: match what the noun is doing rather than forcing
                    a new inflection on the adjective._
                """).strip() + "\n\n"

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                # 3-C  COMMON-SENSE-NOTE  ‚Äì semantic & class sanity
                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                common_sense_note = textwrap.dedent("""
                    **SEMANTIC CHECK ‚Äì DOES THE LABEL FIT THIS ADJECTIVE?**

                    ‚ë† **Identify the class** (use the column ‚ÄúAdjective Class / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£ ‡®ï‡®ø‡®∏‡®Æ‚Äù):  
                    ‚Ä¢ **Qualitative / Descriptive (‡®ó‡©Å‡®£ ‡®µ‡®æ‡®ö‡®ï)** ‚Äì *‡®ö‡©∞‡®ó‡®æ, ‡®∏‡©ã‡®π‡®£‡®æ, ‡®ï‡®æ‡®≤‡®æ*  
                    ‚Ä¢ **Demonstrative (‡®®‡®ø‡®∏‡®º‡®ö‡©á ‡®µ‡®æ‡®ö‡®ï)** ‚Äì *‡®á‡®π, ‡®â‡®π, ‡®â‡®π‡©Ä, ‡®¶‡©á‡®â, ‡®¶‡®ø‡®®‡©Å*  
                    ‚Ä¢ **Indefinite (‡®Ö‡®®‡∞ø‡∞∂‡®ö‡©á ‡®µ‡®æ‡®ö‡®ï)** ‚Äì *‡®ï‡©ã‡®à, ‡®ï‡©à, ‡®ï‡®â‡®®, ‡®∏‡®≠*  
                    ‚Ä¢ **Pronominal**  
                        ‚Äì *‡®Æ‡©á‡®∞‡®æ, ‡®§‡©á‡®∞‡®æ (possessive) / ‡®ú‡©à, ‡®ú‡®ø‡®â (relative)*  
                    ‚Ä¢ **Interrogative (‡®™‡©ç‡®∞‡®∏‡®º‡®® ‡®µ‡®æ‡®ö‡®ï)** ‚Äì *‡®ï‡®â‡®£, ‡®ï‡®ø‡®π, ‡®ï‡®ø‡®â‡©≥, ‡®ï‡®ø‡®µ‡©á‡®Ç*  
                    ‚Ä¢ **Numeral (‡®∏‡©∞‡®ñ‡®ø‡®Ü ‡®µ‡®æ‡®ö‡®ï)**  
                        ‚Äì **Cardinal** *‡®á‡®ï, ‡®¶‡©ã, ‡®¨‡©Ä‡®π* | **Ordinal** *‡®™‡®π‡®ø‡®≤‡®æ, ‡®¶‡©Ç‡®ú‡®æ, ‡®§‡©Ä‡®ú‡®æ‚Ä¶*

                    ‚ë° **Verify agreement** ‚Äì does the ending you see match the gender &
                    number of the noun in the gloss?  Typical pitfalls:  
                    ‚Ä¢ plural nouns paired with singular adjective forms,  
                    ‚Ä¢ masculine endings left on a feminine noun after emendation.

                    ‚ë¢ **Ambiguity guardrails**  
                    ‚Ä¢ Many demonstratives (*‡®á‡®π, ‡®â‡®π, ‡®∏‡©ã‚Ä¶*) double as pronouns ‚Äì keep them
                        in **Adjective** only when they *modify* a following noun.  
                    ‚Ä¢ Some numerals can work adverbially (*‡®¨‡®π‡©Å‡®§ ‡®≠‡®ú‡©á*, ‚Äúran a lot‚Äù) ‚Äì do not
                        tag those as adjectives.

                    _If two classes seem possible, pick the one that best serves the
                    **function in that specific gloss line** and give one-line reasoning._
                """).strip() + "\n\n"

                ending_cheat_sheet = textwrap.dedent("""\
                **ADJECTIVE ENDINGS ‚Äì QUICK REFERENCE (GurbƒÅ·πáƒ´ corpus)**

                üîπ **Agreement grid (what can legally combine)**  
                ‚Ä¢ **Number / ‡®µ‡®ö‡®®** ‚Üí Singular / ‡®á‡®ï, Plural / ‡®¨‡®π‡©Å, NA  
                ‚Ä¢ **Gender / ‡®≤‡®ø‡©∞‡®ó** ‚Üí Masc / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó, Fem / ‡®á‡®∏‡®§‡®∞‡©Ä, Neut / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï, NA  
                ‚Ä¢ **Surface ending-classes** ‚Üí ‡®Æ‡©Å‡®ï‡®§‡®æ, ‡®ï‡©∞‡®®‡®æ, ‡®∏‡®ø‡®π‡®æ‡®∞‡©Ä, ‡®¨‡®ø‡®π‡®æ‡®∞‡©Ä, ‡®π‡©ã‡®∞‡®æ, ‡©Å, ‡©ã, ‡©å, NA  
                ‚Ä¢ **Sub-classes** ‚Üí Qualitative, Demonstrative, Indefinite, Possessive-pronom., Pronominal, Interrogative, Numeral (Card & Ord), Diminutive, Negation, Tat-sam, Compound, NA  

                <sub>Adjectives never carry an independent ‚Äúcase‚Äù; if the noun is oblique, the adjective just copies that ending.</sub>

                ---

                ### A ¬∑ Canonical ending patterns  

                | Ending-class | Masc Sg | Fem Sg | Plural | Tiny sample from text |
                |--------------|---------|--------|--------|-----------------------|
                | **‡®Æ‡©Å‡®ï‡®§‡®æ**    | ‡®∏‡®æ‡®ö**‡®æ** | ‚Äî | ‡®∏‡®æ‡®ö**‡©á** | **‡®•‡®ø‡®∞‡©Å**, ‡®™‡®µ‡®ø‡®§‡©Å, ‡®¨‡©á‡®Ö‡©∞‡®§ |
                | **‡®ï‡©∞‡®®‡®æ**     | ‡®ö‡©∞‡®ó**‡®æ** | ‡®ö‡©∞‡®ó**‡©Ä** | ‡®ö‡©∞‡®ó**‡©á** | ‡®ï‡®æ‡®≤‡®æ, ‡®®‡®æ‡®Æ‡®æ, ‡®∏‡®æ‡®ö‡®æ |
                | **‡®∏‡®ø‡®π‡®æ‡®∞‡©Ä**   | ‚Äî | ‚Äî | ‡®®‡®ø‡®∞‡®Æ‡®≤**‡©á** | ‡®®‡®ø‡®∏‡®º‡®ö‡®ø, ‡®Ö‡®∏‡®≤‡®ø |
                | **‡®¨‡®ø‡®π‡®æ‡®∞‡©Ä**   | ‡®¨‡®æ‡®µ‡®∞**‡©Ä** | ‡®¨‡®æ‡®µ‡®∞**‡©Ä** | ‡®¨‡®æ‡®µ‡®∞**‡©Ä‡®Ü‡®Ç** | ‡®≤‡©ã‡®≠‡©Ä, ‡®®‡®ø‡®ó‡©Å‡®£‡©Ä |
                | **‡®π‡©ã‡®∞‡®æ**     | ‡®∏‡©Å‡®≠**‡®â** | ‚Äî | ‚Äî | ‡®â‡®§‡©Å (rare) |
                | **‡©Å / ‡©ã / ‡©å** | ‡®Ö‡®Æ‡©Å‡®≤**‡©Å** | ‚Äî | ‚Äî | ‡®ï‡®æ‡®≤‡©ã, ‡®Æ‡®ø‡©±‡®†‡©å |

                ---

                ### B ¬∑ Sub-class snapshots  

                | Class / ‡®ï‡®ø‡®∏‡®Æ | 2-4 high-frequency examples (agreement marked) |
                |--------------|-----------------------------------------------|
                | **Qualitative (‡®ó‡©Å‡®£)** | ‡®ö‡©∞‡®ó‡®æ (M), ‡®ö‡©∞‡®ó‡©Ä (F), ‡®ö‡©∞‡®ó‡©á (Pl) ‚Ä¢ ‡®•‡®ø‡®∞‡©Å (M) ‚Ä¢ ‡®Ö‡®Æ‡©Å‡®≤‡©Å (M) |
                | **Demonstrative (‡®®‡®ø‡®∏‡®º‡®ö‡©á)** | ‡®á‡®π‡©Å (M Sg), ‡®á‡®π (F Sg), ‡®â‡®π, ‡®è‡®π, ‡®ì‡®π‡©Å |
                | **Indefinite (‡®Ö‡®®‡®ø‡®∏‡®º‡®ö‡©á)** | ‡®ï‡©ã‡®à, ‡®ï‡®à, ‡®∏‡®≠, ‡®π‡©ã‡®∞, ‡®ò‡®£‡©Ä |
                | **Possessive-pronominal** | ‡®Æ‡©á‡®∞‡®æ (M), ‡®Æ‡©á‡®∞‡©Ä (F), ‡®Æ‡©á‡®∞‡©á (Pl) ‚Ä¢ ‡®Ö‡®™‡®£‡®æ |
                | **Pronominal (relative etc.)** | ‡®ú‡©ã (F/M), ‡®ú‡®ø‡®∏‡©Å, ‡®ú‡®ø‡®®, ‡®§‡®ø‡®∏‡©Å |
                | **Interrogative (‡®™‡©ç‡®∞‡®∂‡®®)** | ‡®ï‡®â‡®£‡©Å (M Sg), ‡®ï‡®µ‡®£, ‡®ï‡®ø‡®Ü, ‡®ï‡®ø‡®§‡©Å |
                | **Numeral ‚Äì Cardinal** | ‡®á‡®ï, ‡®¶‡©Å‡®á, ‡®™‡©∞‡®ú, ‡®¶‡®∏, ‡®∏‡®â |
                | **Numeral ‚Äì Ordinal** | ‡®™‡®π‡®ø‡®≤‡®æ, ‡®¶‡©Ç‡®ú‡®æ, ‡®§‡©Ä‡®ú‡©Ä, ‡®ö‡®â‡®•‡©à |
                | **Negation** | ‡®®, ‡®®‡®æ‡®π‡©Ä |
                | **Tat-sam (‡®∏‡©∞‡®∏‡®ï‡©ç‡®∞‡®ø‡®§ loan)** | ‡®Ö‡®∏‡®≤‡®ø, ‡®¨‡®∞‡®æ‡®¨‡®∞‡®ø, ‡®∏‡®§‡®∞‡®ø |
                | **Diminutive** | ‡®¨‡©∞‡®ï‡©Å‡©ú‡®æ, ‡®Æ‡©ã‡®π‡®ø‡®Ö‡©ú‡©Ä, ‡®®‡®µ‡©á‡®≤‡©ú‡©Ä‡®è |
                | **Compound** | ‡®Ö‡®®‡®π‡®¶ ‡®ß‡©Å‡®®‡®ø, ‡®ú‡©Ä‡®µ‡®® ‡®Æ‡©Å‡®ï‡®§‡®ø, ‡®¨‡®π‡©Å ‡®ó‡©Å‡®£‡®ø |

                """).strip() + "\n\n"

            elif entry["Type"] == "Verb / ‡®ï‡®ø‡®∞‡®ø‡®Ü":
                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                # 4-B  IMPLICIT-NOTE  ‚Äì how to ‚Äúread‚Äù the gloss
                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                implicit_note = textwrap.dedent("""\
                **VERBS IN GURBƒÄ·πÜƒ™ ‚Äì IMPLIED CLUES FROM THE GLOSS**

                Verbs in GurbƒÅ·πáƒ´ span a wide linguistic spectrum‚ÄîLahindƒ´, Braj, HindustƒÅnƒ´, and archaic PanjƒÅbƒ´. The verse alone often omits explicit markers for **tense, voice, mood, or even subject**. Prof. SƒÅhib Si·πÖgh‚Äôs **Darpan gloss** therefore becomes our decoder ring: it regularly inserts the **hidden agent, auxiliary, or intent** that lets us recover the full verbal meaning.

                ---

                ### ‚úî Step 1 ¬∑ Read the gloss literally
                Ask yourself:
                * Is the action **ongoing**, **completed**, or **yet to come**?
                * Is the subject **doing** the action or **receiving** it?
                * Is the clause a **command**, a **wish**, or a **hypothetical**?
                * Do helper words appear‚Äî*has, was, should, may, being, let*‚Äîthat hint at aspect or mood?

                ---

                ### ‚úî Step 2 ¬∑ Map the gloss cue to a grammatical category

                | Category            | Common cues in the gloss (Eng. gloss)            |
                |---------------------|--------------------------------------------------|
                | **Present**         | do, does, is, are, becomes, gives                |
                | **Past**            | did, was, were, had, gave, came                  |
                | **Future**          | will, shall, would                               |
                | **Imperative**      | (you) give, fall, listen ‚Äî direct command forms  |
                | **Subjunctive**     | if ‚Ä¶ may / might / should / let us               |
                | **Passive**         | is called, was given ‚Äî object promoted to subject |
                | **Participles**     | having done, while doing, upon going, imbued     |
                | **Compound/Aux**    | do come, has gone, may go ‚Äî multi-verb chains    |

                ---

                ### üß† Key heuristics from the Darpan gloss
                * **‚Äúwas made / is given‚Äù** ‚Üí strong passive signal.  
                * **‚Äúhas shown / had come‚Äù** ‚Üí perfect aspect; expect past-participle + auxiliary.  
                * If the gloss shows the subject **causing** another to act (*was made to go*) ‚Üí tag the verb **causative**.

                ---

                ### üìå Postposition surrogates
                Gloss words like *to, by, with, for, from* often reveal an implied **shift in voice** or a **participial/causative chain** hidden in the surface form.

                ---

                ### üîÑ When in doubt
                * Subject absent, object prominent ‚Üí suspect **passive**.  
                * Two verbs side-by-side (*will come go*, *has been given*) ‚Üí parse for **compound** or **auxiliary** roles.  
                * Conditional tone (*if ‚Ä¶ may ‚Ä¶*, *let it be ‚Ä¶*) ‚Üí test for **subjunctive**.

                ---

                ### üß© Suffix hints  
                Endings like **‚Äì‡®π‡®â, ‚Äì‡®π‡©Ä, ‚Äì‡®Æ, ‚Äì‡®∏‡©Ä‡®Ö** (and Lahindƒ´ ‚Äì‡®â, ‚Äì‡®π‡©Å) can encode person or emphasis. Cross-check with the gloss‚Äôs subject reference.

                ---

                > **Rule of thumb**  
                > *If the gloss shows something **happening to** someone and the agent is missing ‚Üí think passive.*  
                > *If multiple verbs are chained, the **right-most** verb usually carries tense/voice; earlier ones express the semantic action.*

                _Use the gloss‚Äîits hidden auxiliaries, agents, and helpers‚Äîto uncover the verb‚Äôs true grammatical load._\
                """).strip() + "\n\n"


                common_sense_note = textwrap.dedent("""\
                ### üîπ `common_sense_note` ‚Äì VERBS / ‡®ï‡®ø‡®∞‡®ø‡®Ü (semantic sanity layer)

                **Essence**‚ÄÉA sieve that questions every verb label: *Does this person √ó number √ó tense truly fit what the verb is doing in the pa·πÖktƒ´?*

                **Vision**‚ÄÉFuse surface-form clues with syntactic/semantic roles so edge-cases (poetic plurals, ergative flips, auxiliary drop, Lahindƒ´ quirks) are flagged, not rubber-stamped.

                ---

                ## 1 ¬∑ Finite vs Non-finite: cheat grid  

                | Tag you plan | Sanity checks (abort / relabel if violated) |
                |--------------|---------------------------------------------|
                | **Present / Future** | Ending shows **person+number; no gender**. If ending = ‚Äì‡®¶‡®æ/‡®¶‡©Ä/‡®¶‡©á **without** auxiliary **‡®π‡©à/‡®π‡®®**, treat as participle (habitual/progressive) not finite. |
                | **Imperative** | Only 2nd-person. Command/request mood. If clause is conditional (*‡®ú‡©á ‡®∏‡©Å‡®£‡®π‡©Å‚Ä¶*) ‚Üí **Subjunctive** not Imperative. |
                | **Subjunctive** | Expresses wish/suggestion; often with *‡®ú‡©á, ‡®ú‡©á‡®ï‡®∞, ‡®§‡®æ‡®Ç*. Never shows gender agreement. |
                | **Past / Perfective** | Built on past-participle endings **‚Äì‡®Ü / ‚Äì‡®à / ‚Äì‡®è**. Transitive verbs agree with **object** (ergative); intransitives with **subject**. |
                | **Passive finite** | Look for **‡®ï‡®∞‡©Ä‡®ê, ‡®ï‡©Ä‡®Ü ‡®ú‡®æ‡®è, ‡®ï‡®π‡©Ä‡®è** etc. Object promoted to subject; auxiliary **‡®ï‡®∞‡©Ä‡®®‡®ø, ‡®ï‡®∞‡©Ä‡®ê** etc. present/past table (¬ß passive pages). |
                | **Causative** | Endings ‚Äì‡®Ü‡®µ‡®æ, ‚Äì‡®®‡®æ‡©≥, ‚Äì‡®µ‡®â, ‚Äì‡®è‡®á, ‚Äì‡®µ‡®π‡®ø‚Ä¶; semantics must show *caused* action. |
                | **Auxiliary-only token** | If root **‡®π‡©ã** form (‡®π‡®æ, ‡®π‡©à, ‡®π‡®æ‡®Ç, ‡®π‡©Å‡©∞, ‡®∏‡©Ä, ‡®∏‡©á, ‡®∏‡©Ä‡®ê, ‡®∏‡®æ‚Ä¶) appears **alone**, tag = **Auxiliary Verb** not main finite. |
                *If the Canonical row label is ‚ÄúPronominal Suffixes ‚Ä¶‚Äù you **must tag Grammar Case = ‚ÄúPronominal Suffixes ‚Ä¶‚Äù**, not plain Past/Present.*
                *For finite verbs, **Word-Root must record the person (1st / 2nd / 3rd)**; tense or aspect belongs in ‚ÄúGrammar Case / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£,‚Äù not in Word-Root.*

                ---

                ## 2 ¬∑ Past-participle agreement sanity  

                1. **Intransitive:** participle ‚Üî subject.  
                2. **Transitive (ergative):** participle ‚Üî object; subject in instrumental/obl.  
                3. **Pron.-suffix ‚Äì‡®â/-‡®π‡©Å:** when object = **‡®§‡©à/‡®§‡©Ç‡©∞**, endings like **‡®ï‡©Ä‡®â, ‡®ï‡®ø‡®â‡®π‡©Å** act as clitics ‚Üí tag ‚ÄúPronominal-suffix‚Äù sub-type.  
                4. Gender/number mismatch with controller ‚Üí flag for review.

                ---

                ## 2A ¬∑ When gender actually matters  

                * **Finite verbs** (Present, Future, Imperative, Subjunctive, Causative, Auxiliary)  
                  ‚Üí **never carry masc/fem marks** in SGGS.  *Finite verbs must therefore be tagged **Gender = Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï** (not NA).*

                * **Participles** ‚Äì the only verb forms that **do** mark gender:  
                  ‚Ä¢ Perfect / perfective: **Masc SG -‡®Ü / Fem SG -‡®à / Masc PL -‡®è / Fem PL -‡®à‡®Ü‡®Ç**  
                  ‚Ä¢ Habitual / imperfective: **Masc SG -‡®¶‡®æ / Fem SG -‡®¶‡©Ä / Masc PL -‡®¶‡©á / Fem PL -‡®¶‡©Ä‡®Ü‡®Ç**  
                  ‚Ä¢ Dialectal allomorphs (‡®≤‡®π‡®ø‡©∞‡®¶‡©Ä **-‡®á‡®ì**, ‡®¨‡©ç‡®∞‡®ú **-‡®Ø‡©ã**, etc.) are **still Masc SG**.

                * **Controller rule**  
                  ‚Äì **Intransitive** ‚Üí participle agrees with **subject**.  
                  ‚Äì **Transitive perfective** (ergative) ‚Üí participle agrees with **object**.

                * **Auxiliaries stay neuter.**  `‡®π‡©à/‡®π‡®®/‡®∏‡©Ä‚Ä¶` never add gender; only the participle does.

                ---

                ## 3 ¬∑ Auxiliary verbs & silent dropping  

                * Present auxiliaries: **‡®π‡®æ (1 sg), ‡®π‡©à (2 sg), ‡®π‡©à (3 sg), ‡®π‡®æ‡®Ç (1 pl), ‡®π‡®â/‡®π‡©Å (2 pl respect), ‡®π‡®®/hin (3 pl)**.  
                * Past auxiliaries (rare): **‡®∏‡®æ/‡®∏‡©á/‡®∏‡©Ä/‡®∏‡®ø‡®§, ‡®∏‡®ø‡®Ü, ‡®∏‡®æ; 3 pl = ‡®∏‡©á, ‡®∏‡©à‡®®, ‡®∏‡©Ä‡®Æ‡®æ**.  
                * In GurbƒÅ·πáƒ´ the auxiliary is **often absorbed** into a longer verb with pronominal suffix: *‡®ö‡®≤‡®¶‡®ø‡®µ‡©à, ‡®≠‡®∞‡®µ‡®æ‡®à‡®ê*. If you can‚Äôt locate a free auxiliary, confirm tense via surface ending first.

                ---

                ## 4 ¬∑ Imperative & Subjunctive overlap  

                | Ending cluster | True Imperative if‚Ä¶ | Else ‚Üí likely Subjunctive |
                |----------------|---------------------|---------------------------|
                | **‚Äì‡®π‡©Å / ‚Äì‡®π‡©Å‡®ó‡©á / ‚Äì‡®π‡©ã** | Stand-alone command/request | Used inside conditional/wish |
                | **‚Äì‡®π‡©á / ‚Äì‡®π‡©Ä / ‚Äì‡®π‡©á‡®á** | Vocative context | Hypothetical clause |

                ---

                ## 5 ¬∑ Passive voice heuristics  

                * **Surface template:** participle (‡®ò‡®≤‡®ø‡®Ü) + auxiliary **‡®ï‡®∞‡©Ä‡®ê / ‡®ï‡®π‡©Ä‡®ê / ‡®ï‡®µ‡®æ‡®á‡®ì** etc.  
                * Only 3rd-person shows full paradigm in tables; 1st/2nd are scarce ‚Üí flag if you tag 1st-person finite passive without strong textual evidence.  
                * Present passive often masquerades as adjective; ensure a *patient-as-subject* reading is plausible.

                ---

                ## 6 ¬∑ Causative sanity  

                * First-person causatives: **‚Äì‡®Ü‡®µ‡®æ / ‚Äì‡®Ü‡®µ‡®æ, ‚Äì‡®ï‡®∞‡®æ‡®µ‡®æ**. No object ‚Üí verb likely **inchoative**, not causative.  
                * 3rd-person causatives: **‚Äì‡®µ‡®æ‡®á‡®Ü, ‚Äì‡®µ‡®ß‡®æ‡®á‡®Ü, ‚Äì‡®§‡®ø‡®µ‡®æ‡®á‡®Ü, ‚Äì‡®à‡®Ø‡©à**: must show agent-causes-other scenario.  
                * If semantic agent = performer, drop ‚Äúcausative‚Äù tag.

                ---

                ## 7 ¬∑ Compound verbs  

                * Earlier element -> conjunct ending **-‡®ï‡©á / -‡®á / -‡®Ü / -‡®ï‡©á‡®Ç**.  
                * Last element holds tense/person.  
                * Tag first as ‚ÄúConjunct Verb / Gerund‚Äù, second as finite.

                ---

                ## 8 ¬∑ Auto-highlight (red flags)  

                | Pattern | Likely mis-label |
                |---------|------------------|
                | Ending **-‡®ó‡®æ/‡®ó‡©Ä/‡®ó‡©á** but tag ‚â† Future | Wrong tense |
                | Ending **-‡®π‡©Å/-‡®π‡©Å‡®ó‡©á** tagged 1st/3rd person | Imperative bleed |
                | Ending **-‡®¶‡®æ/‡®¶‡©Ä/‡®¶‡©á** with no **‡®π‡©à/‡®π‡®®** & tag = Present/Future | Participle, not finite |
                | Two consecutive finite-verb tags inside one clause | Probably compound verb ‚Äì split roles |
                | Passive participle **‡®ï‡®∞‡©Ä‡®ê/‡®ï‡®∞‡®æ‡®§‡©Å** but subject‚Äêagent reading given | Reverse voice |
                | Finite verb tagged Masc/Fem | Finite forms should be Trans ‚Äì likely mis-tag |
                | Participial ending gender ‚â† controller noun/pronoun | Agreement error (ergative or intransitive mix-up) |
                | Ending-tense combo not found in Canonical table | Illegal combination ‚Äì override gloss |
                | Finite verb with Gender = NA | Should be Trans ‚Äì fix label |

                ---

                <sub>Heuristics sourced from pages 5.1 ‚Äì 5.12: Present, Past, Future, Imperative, Subjunctive, Participles, Compound, Passive, Causative, Auxiliary, Pron-suffix sections.</sub>\
                """).strip() + "\n\n"

                ending_cheat_sheet = textwrap.dedent("""\
                üîî **Authoritative workflow**

                1Ô∏è‚É£ **Check legality** ‚Äì If a surface ending √ó person/number √ó tense combo is **absent** from the
                Canonical table below, reject or relabel.

                2Ô∏è‚É£ **Decide meaning** ‚Äì Among the *legal* options, pick the tag that is **best supported by
                the Darpan Translation and Darpan Meanings** (Prof. SƒÅhib Si·πÖgh).  
                *Those glosses remain the primary key to tense, mood, voice, and agent/object choice.*

                3Ô∏è‚É£ Apply common-sense sanity rules (¬ß 1‚Äì8) for edge-case flags.

                ---

                **VERB / ‡®ï‡®ø‡®∞‡®ø‡®Ü ENDINGS ‚Äì QUICK REFERENCE (GurbƒÅ·πáƒ´ corpus, Sheet 1)**  

                üîπ **Agreement grid (what can legally combine)**  
                ‚Ä¢ **Person / ‡®™‡©Å‡®∞‡®ñ** ‚Üí 1st (‡®â‡©±‡®§‡®Æ) | 2nd (‡®Æ‡®ß‡®Æ) | 3rd (‡®Ö‡®®‡®Ø)  
                ‚Ä¢ **Number / ‡®µ‡®ö‡®®** ‚Üí Singular / ‡®á‡®ï | Plural / ‡®¨‡®π‡©Å  
                ‚Ä¢ **Tense / Mood** ‚Üí Present / ‡®µ‡®∞‡®§‡®Æ‡®æ‡®® | Past / ‡®≠‡©Å‡®§ | Future / ‡®≠‡®µ‡®ø‡©±‡®ñ‡®§ | Causative / ‡®™‡©á‡©ç‡®∞‡®£‡®æ‡®∞‡®•‡®ï | Pronominal suffix  
                <sub>*Finite verbs ignore noun-gender; ‚Äì‡®¶‡®æ/‚Äì‡®¶‡©Ä/‚Äì‡®¶‡©á are participial*</sub>

                ---

                ### A ¬∑ Canonical ending patterns (+ three toy forms on **‡®ó‡®æ‡®µ-**)

                | Person ¬∑ Number | Tense / Mood | Surface endings | Micro-examples |
                |-----------------|--------------|-----------------|---------------|
                | **1st Sg** | Present | ‡®à/‡®â/‡®ä/‡®æ/‡©Ä/‡®§/‡®£‡®æ/‡®§‡®æ/‡®¶‡®æ/‡®®‡®æ/‡©á‡®â/‡©∞‡®¶‡®æ/‡©á‡®Ç‡®¶‡©Ä | ‡®ó‡®æ‡®µ‡®à, ‡®ó‡®æ‡®µ‡®â, ‡®ó‡®æ‡®µ‡©á‡®â |
                |  | Past | ‡®æ/‡©Ä | ‡®ó‡®æ‡®µ‡®æ, ‡®ó‡®æ‡®µ‡©Ä |
                |  | Future | ‡®â/‡®ä/‡®æ/‡®∏‡®æ/‡®â‡®ó‡®æ/‡®â‡®ó‡©Ä/‡®â‡®ó‡©ã/‡©à ‡®π‡®â | ‡®ó‡®æ‡®µ‡®â, ‡®ó‡®æ‡®µ‡®ä, ‡®ó‡®æ‡®µ‡®â‡®ó‡®æ |
                |  | Causative | ‡®µ‡®â/‡®æ‡®à/‡®æ‡®µ‡®æ/‡®æ‡®π‡®æ | ‡®ó‡®æ‡®µ‡®µ‡®â, ‡®ó‡®æ‡®µ‡®æ‡®à, ‡®ó‡®æ‡®µ‡®æ‡®µ‡®æ |
                |  | Pronominal | ‡®Æ/‡®Æ‡©Å | ‡®ó‡®æ‡®µ‡®Æ, ‡®ó‡®æ‡®µ‡®Æ‡©Å |
                | **1st Pl** | Present | ‡®π/‡®π‡®æ/‡®§/‡®§‡©á/‡®¶‡©á | ‡®ó‡®æ‡®µ‡®π, ‡®ó‡®æ‡®µ‡®§, ‡®ó‡®æ‡®µ‡®§‡©á |
                |  | Past | ‡©á | ‡®ó‡®æ‡®µ‡©á |
                |  | Future | ‡®∏‡®π/‡®π‡®ó‡©á/‡®π‡®ø‡®ó‡©á | ‡®ó‡®æ‡®µ‡®∏‡®π, ‡®ó‡®æ‡®µ‡®π‡®ó‡©á |

                | Person ¬∑ Number | Tense / Mood | Surface endings | Micro-examples |
                |-----------------|--------------|-----------------|---------------|
                | **2nd Sg** | Present | ‡®§/‡©à/‡®∏‡®ø/‡®π‡®ø/‡®π‡©Ä/‡®π‡©á/‡©á‡®π‡©Ä/‡®¶‡®æ | ‡®ó‡®æ‡®µ‡®§, ‡®ó‡®æ‡®µ‡©à, ‡®ó‡®æ‡®µ‡®π‡®ø |
                |  | Past | ‡®æ/‡©Ä/‡®π‡©Å | ‡®ó‡®æ‡®µ‡®æ, ‡®ó‡®æ‡®µ‡©Ä, ‡®ó‡®æ‡®µ‡®π‡©Å |
                |  | Future | ‡®∏‡®ø/‡®∏‡©Ä/‡®π‡®ø/‡®π‡©Ä/‡®π‡©ã/‡®∏‡®π‡®ø/‡®π‡®ø‡®ó‡®æ | ‡®ó‡®æ‡®µ‡®∏‡®ø, ‡®ó‡®æ‡®µ‡®∏‡©Ä |
                |  | Causative | ‡®π‡®ø/‡®á‡®¶‡®æ/‡®á‡®π‡®ø | ‡®ó‡®æ‡®µ‡®π‡®ø, ‡®ó‡®æ‡®µ‡®á‡®¶‡®æ |
                |  | Pronominal | ‡®á/‡®à/‡®π‡®ø/‡®π‡©Å | ‡®ó‡®æ‡®µ‡®á, ‡®ó‡®æ‡®µ‡®à |
                | **2nd Pl** | Present | ‡®π‡©Å/‡®§ ‡®π‡®â/‡®§ ‡®π‡©å/‡®§ ‡®π‡®π‡©Å/‡®à‡®Ö‡®§ ‡®π‡©å | ‡®ó‡®æ‡®µ‡®π‡©Å, ‡®ó‡®æ‡®µ‡®§ ‡®π‡®â |
                |  | Past | ‡©á/‡®π‡©ã | ‡®ó‡®æ‡®µ‡©á, ‡®ó‡®æ‡®µ‡®π‡©ã |
                |  | Future | ‡®π‡©Å/‡©á‡®π‡©Å/‡®π‡©Å‡®ó‡©á | ‡®ó‡®æ‡®µ‡®π‡©Å, ‡®ó‡®æ‡®µ‡©á‡®π‡©Å |

                | Person ¬∑ Number | Tense / Mood | Surface endings | Micro-examples |
                |-----------------|--------------|-----------------|---------------|
                | **3rd Sg** | Present | ‡®á/‡®à/‡®è/‡©à/‡®§/‡®§‡®æ/‡®§‡©Ä/‡®§‡®ø/‡©á/‡®Ç‡®§/‡®¶‡®æ/‡®¶‡©Ä/‡©∞‡®§‡®æ/‡®∏‡®ø/‡®π‡©à | ‡®ó‡®æ‡®µ‡®á, ‡®ó‡®æ‡®µ‡®à, ‡®ó‡®æ‡®µ‡®§‡©Ä |
                |  | Past | ‡®æ/‡©Ä | ‡®ó‡®æ‡®µ‡®æ, ‡®ó‡®æ‡®µ‡©Ä |
                |  | Future | ‡®à/‡©à/‡®ó‡®æ/‡®ó‡©Ä/‡®ó‡©ã/‡®∏‡®ø/‡®∏‡©Ä | ‡®ó‡®æ‡®µ‡®ó‡®æ, ‡®ó‡®æ‡®µ‡®ó‡©Ä |
                |  | Causative | ‡®è/‡®à‡®ê/‡®ø‡®µ‡©à/‡®ø‡®¶‡®æ/‡®æ‡®µ‡©à | ‡®ó‡®æ‡®µ‡®è, ‡®ó‡®æ‡®µ‡®á‡®¶‡®æ |
                |  | Pronominal | ‡®®‡©Å/‡®∏‡©Å | ‡®ó‡®æ‡®µ‡®®‡©Å, ‡®ó‡®æ‡®µ‡®∏‡©Å |
                | **3rd Pl** | Present | ‡®§/‡®§‡©á/‡©∞‡®§‡©á/‡®¶‡©á/‡©∞‡®¶‡©á/‡®®‡®ø/‡®®‡©Ä/‡®∏‡®ø/‡®π‡®ø/‡®π‡©Ä/‡®á‡®®‡®ø/‡®á‡©∞‡®®‡®ø/‡®¶‡©Ä‡®Ü/‡®¶‡©Ä‡®Ü‡®Ç | ‡®ó‡®æ‡®µ‡®§‡©á, ‡®ó‡®æ‡®µ‡®¶‡©á |
                |  | Past | ‡©á | ‡®ó‡®æ‡®µ‡©á |
                |  | Future | ‡®π‡®ø/‡®π‡©Ä/‡®∏‡®®‡®ø/‡®π‡®ø‡®ó‡©á | ‡®ó‡®æ‡®µ‡®π‡®ø, ‡®ó‡®æ‡®µ‡®π‡®ø‡®ó‡©á |
                |  | Causative | ‡®á‡®¶‡©á/‡®á‡®®‡®ø/‡®µ‡®π‡®ø | ‡®ó‡®æ‡®µ‡®á‡®¶‡©á, ‡®ó‡®æ‡®µ‡®µ‡®π‡®ø |

                ---

                ### B ¬∑ How to use the dashboard  

                1. **Validate annotations** ‚Äì If you tag a form ‚Äú2nd Pl Future‚Äù but it ends in **‚Äì‡®¶‡®æ**, the table shows that combo never occurs ‚Üí revisit the tag.  
                2. **Debug machine predictions** ‚Äì Surface ending not found under predicted role ‚Üí flag for review.  
                3. **Handle sandhi** ‚Äì Remember silent ‚Äì‡®â can drop before postpositions (e.g. **‡®§‡©ã‡®Ç, ‡®®‡©Ç‡©∞**).  

                _Export or further slicing on request._\
                """).strip() + "\n\n"

            elif entry["Type"] == "Adverb / ‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡©á‡®∂‡®£":
                implicit_note = textwrap.dedent("""\
                ### üîπ `implicit_note` ‚Äì ADVERB / ‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡®º‡©á‡®∏‡®º‡®£  
                *(SGGS-centric discovery guide)*  

                **Essence**‚ÄÇTeach the evaluator to recognise words that **modify the *action itself***‚Äînever the doer (noun) nor the quality‚Äêword (adjective).  

                **Vision**‚ÄÇLean on *Prof. SƒÅhib Si·πÖgh‚Äôs* Darpan gloss to infer *how, when, where* the verb happens‚Äîeven when SGGS omits explicit post-positions or auxiliaries.  

                ---

                ## 1 ¬∑ Adverb ‚â† Adjective ‚â† Noun ‚Äî the litmus test ü©∫  

                | Ask this first | Pass ‚úîÔ∏è ‚Üí Adverb | Fail ‚úñÔ∏è ‚Üí something else |
                |----------------|------------------|--------------------------|
                | **Does the word alter the meaning of the verb?** <br>(time, place, manner, measure‚Ä¶) | ‚úîÔ∏è modifies *action* ‚Üí keep testing | ‚úñÔ∏è modifies noun ‚Üí likely *Adjective* or *Noun* |
                | **Will the clause stay grammatical if the word is removed?** | ‚úîÔ∏è sentence remains; nuance lost | ‚úñÔ∏è structure breaks ‚Üí maybe pronoun/helper |
                | **Can the word move freely in the clause?** | ‚úîÔ∏è adverbs float (‡©¥ ‡®¶‡®á‡®Ü‡®≤‡©Å **‡®π‡©Å‡®£‡®ø** ‡®Æ‡®ø‡®≤‡®ø‡®Ü) | ‚úñÔ∏è fixed next to noun ‚Üí adjective/compound |
                | **Any number/gender inflection visible?** | ‚úîÔ∏è none (adverbs are **indeclinable**) | ‚úñÔ∏è ‚Äì ‡®Ü/‚Äì‡®à/‚Äì‡®è etc. ‚Üí participle/adjective |
                | **Darpan gloss clue** says: ‚Äúnow, then, quickly, here, twice‚Ä¶‚Äù | ‚úîÔ∏è adopt adverb label | ‚úñÔ∏è gloss uses ‚Äúof, to, with‚Äù ‚Üí case marker |

                > **Rule:** In this framework an adverb may *expand* a phrase (‡®ú‡®ó‡®ø **‡®∏‡®≠‡®§‡©à**), but it still targets the action, **not** the noun.  

                ---

                ## 2 ¬∑ Functional buckets üóÇÔ∏è  

                | Category (Punjabi) | Core semantic cue | Minimal examples* |
                |--------------------|-------------------|-------------------|
                | **‡®∏‡®Æ‡®æ / Time**        | ‚Äò‡®ï‡®¶‡©ã‡®Ç? ‡®ï‡®ø‡©∞‡®®‡®æ ‡®∏‡®Æ‡®æ‡®Ç?‚Äô | ‡®π‡©Å‡®£‡®ø, ‡®ï‡®¶‡©á, ‡®Ö‡®ú‡©Å, ‡®®‡®ø‡®§, ‡®Ö‡®π‡®ø‡®®‡®ø‡®∏‡®ø |
                | **‡®•‡®æ‡®Ç / Place**       | ‚Äò‡®ï‡®ø‡©±‡®•‡©á?‚Äô            | ‡®Ö‡®ó‡©à, ‡®Ö‡©∞‡®¶‡®∞‡®ø, ‡®¶‡©Ç‡®∞‡®ø, ‡®®‡©á‡®∞‡©à, ‡®ä‡®™‡®∞‡®ø |
                | **‡®µ‡®ø‡®ß‡©Ä / Manner**     | ‚Äò‡®ï‡®ø‡®µ‡©á‡®Ç? ‡®ï‡®ø‡®∏ ‡®¢‡©∞‡®ó ‡®®‡®æ‡®≤?‚Äô | ‡®ú‡®ø‡®â, ‡®á‡®â, ‡®®‡®ø‡®∏‡©∞‡®ó‡©Å, ‡®∞‡®∏‡®ï‡®ø ‡®∞‡®∏‡®ï‡®ø |
                | **‡®™‡®∞‡®Æ‡®æ‡®£ / Measure**   | ‚Äò‡®ï‡®ø‡©∞‡®®‡®æ?‚Äô            | ‡®Ö‡®§‡®ø, ‡®¨‡®π‡©Å‡®§‡©Å, ‡®ò‡®£‡®æ, ‡®≠‡®∞‡®™‡©Ç‡®∞‡®ø |
                | **‡®∏‡©∞‡®ñ‡®ø‡®Ü / Number**    | ‚Äò‡®ï‡®ø‡©∞‡®®‡©Ä ‡®µ‡®æ‡®∞?‚Äô        | ‡®¨‡®æ‡®∞‡©∞ ‡®¨‡®æ‡®∞, ‡®´‡®ø‡®∞‡®ø ‡®´‡®ø‡®∞‡®ø |
                | **‡®®‡®ø‡®®‡©à / Decision**   | certainty / denial  | ‡®®‡®æ‡®π‡®ø, ‡®®‡®ø‡®π‡®ö‡®â |
                | **‡®ï‡®æ‡®∞‡®£ / Reason**     | causation           | ‡®Ø‡®æ‡®§‡©á, ‡®ï‡®ø‡®§‡©Å ‡®Ö‡®∞‡®•‡®ø |
                | **‡®§‡®æ‡®ï‡©Ä‡®¶ / Stress**    | emphasis            | ‡®π‡©Ä, ‡®≠‡©Ä, ‡®Æ‡©Ç‡®≤‡©á |

                * A full ‚Äúhigh-freq‚Äù table‚Äîincluding **phrase, compound & iterative** idioms‚Äîfollows in *common_sense_note*.

                ---

                ## 3 ¬∑ Zero-inflection principle üö´üß¨  

                * Adverbs **never** show number (-‡®è/-‡®â), gender, person or case.  
                * If a token **does** decline, re-classify: participial verb (*-‡®¶‡®æ/-‡®¶‡©Ä/-‡®¶‡©á*), adjective, or oblique noun.  

                ---

                ## 4 ¬∑ Typical gloss helpers üîç  

                | Gloss clue | Likely adverb class | Illustration |
                |------------|--------------------|--------------|
                | ‚Äú**now / today / always**‚Äù | Time | ‚Äú‡®π‡©Å‡®£‡®ø ‡®Æ‡®ø‡®≤‡®ø‡®Ü‚Äù |
                | ‚Äú**here / everywhere / within**‚Äù | Place | ‚Äú‡®Ö‡©∞‡®¶‡®∞‡®ø ‡®∞‡®π‡©à‚Äù |
                | ‚Äú**thus / quickly / secretly**‚Äù | Manner | ‚Äú‡®ú‡®ø‡®â ‡®ï‡®∞‡©á‚Äù |
                | ‚Äú**fully / a little**‚Äù | Measure | ‚Äú‡®≠‡®∞‡®™‡©Ç‡®∞‡®ø ‡®∞‡©∞‡®ó‡®ø ‡®∞‡®§‡®æ‚Äù |
                | ‚Äú**again / twice**‚Äù | Number | ‚Äú‡®´‡®ø‡®∞‡®ø ‡®´‡®ø‡®∞‡®ø ‡®Ü‡®á‡®Ü‚Äù |

                ---

                ## 5 ¬∑ Quick detection workflow ‚ö°  

                1. **Mark all gloss adverbials** ‚Äì scan Darpan for English adverbs.  
                2. **Map to Punjabi surface form** ‚Äì locate the SGGS token(s) that carry that nuance.  
                3. **Apply indeclinability test** ‚Äì no visible suffix change? keep as adverb.  
                4. **Check floating mobility** ‚Äì move token; if syntax survives, adverb confirmed.  
                5. **Edge alert** ‚Äì if token sits after a post-position (‡®¶‡©á, ‡®®‡®æ‡®≤‚Ä¶), probably **oblique noun** not adverb.

                ---

                ## 6 ¬∑ Red-flag heuristics üö©  

                * Word tagged *Adverb* but ends in **-‡®¶‡®æ/-‡®¶‡©Ä/-‡®¶‡©á** ‚Üí likely participial.  
                * Tagged *Adverb* but gloss shows possession (*of*) ‚Üí test for Genitive noun.  
                * Compound form **‡®∏‡®æ‡®∏‡®ø ‡®ó‡®ø‡®∞‡®æ‡®∏‡®ø** mis-tagged as Time/Manner interchangeably ‚Üí ensure Darpan intent.  
                * Form appears **twice with different endings** in same ·π≠uk ‚Üí must be *declinable* ‚Üí not adverb.  

                ---

                ### üìù Footnote on spreadsheet codes  
                The Excel ‚ÄúAdverbs‚Äù sheet groups every token into **eight functional sets** above, plus **Compound / Phrase** and **Iterative** markers. These codes are referenced only for *high-freq tables* and require **no inflection logic**.

                _Use this guide, then apply the sanity layer in `common_sense_note` for mis-tag traps._
                """).strip() + "\n\n"
            
                common_sense_note = textwrap.dedent("""\
                ### üîπ `common_sense_note` ‚Äì ADVERBS / ‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡®º‡©á‡®∏‡®º‡®£ (semantic sanity layer)

                **Essence**‚ÄÉA quick triage: *Does this token truly act as an **adverb**‚Äîi.e., modifies a verb (or a whole clause) and NEVER a noun/pronoun?*

                **Vision**‚ÄÉPrevent false-positives caused by:
                * Post-positions or emphatic particles masquerading as adverbs  
                * Adjectival or nominal words that look ‚Äúadverb-ish‚Äù but show agreement or case

                ---

                ## 1 ¬∑ Three-step sanity check üß™  

                | Step | Ask yourself | Abort / Relabel if‚Ä¶ |
                |------|--------------|--------------------|
                | ‚ë† | **Function** ‚Äì Does the word modify a **verb or clause** (manner, time, place, degree)? | It directly qualifies a noun/pronoun ‚Üí likely Adjective or Noun |
                | ‚ë° | **Morphology** ‚Äì No number / gender / person agreement & no case endings | You see ‚Äì‡®è/‚Äì‡®â etc. agreeing with noun ‚Üí it‚Äôs NOT an adverb |
                | ‚ë¢ | **Position / Helpers** ‚Äì Is it followed by a postposition (*‡®¶‡©á, ‡®®‡©Ç‡©∞, ‡®®‡®æ‡®≤*)? | Token + post-position ‚áí treat token as **Noun in oblique**, PP = post-position |

                ---

                ## 2 ¬∑ Category reference with high-frequency SGGS tokens üîç  

                | Category | Typical surface cues | SGGS high-freq examples |
                |----------|----------------------|-------------------------|
                | **Time / ‡®∏‡®Æ‡®æ‡®Ç** | ‚Äúwhen?‚Äù, duration, sequence | ‡®π‡©Å‡®£‡®ø, ‡®∏‡®¶‡®æ, ‡®ï‡®¶‡©á, ‡®§‡®¶‡®ø, ‡®∏‡®µ‡©á‡®∞‡©à |
                | **Place / ‡®•‡®æ‡®Ç** | ‚Äúwhere?‚Äù, location, direction | ‡®Ö‡®ó‡©à, ‡®Ö‡©∞‡®¶‡®∞‡®ø, ‡®¶‡©Ç‡®∞‡®ø, ‡®®‡©á‡®∞‡©à, ‡®ä‡®™‡®∞‡®ø |
                | **Manner / ‡®µ‡®ø‡®ß‡©Ä** | ‚Äúhow?‚Äù, style, attitude | ‡®ú‡®ø‡®â, ‡®∏‡®π‡®ú‡®ø, ‡®á‡®â, ‡®ï‡®ø‡®µ, ‡®®‡®ø‡®∏‡©∞‡®ó‡©Å |
                | **Measurement / ‡®™‡®∞‡®Æ‡®æ‡®£** | quantity / degree | ‡®Ö‡®§‡®ø, ‡®¨‡®π‡©Å‡®§‡®æ, ‡®ò‡®£‡®æ, ‡®≠‡®∞‡®™‡©Ç‡®∞‡®ø, ‡®§‡®ø‡®≤‡©Å |
                | **Number / ‡®∏‡©∞‡®ñ‡®ø‡®Ü** | frequency / repetition | ‡®´‡®ø‡®∞‡®ø ‡®´‡®ø‡®∞‡®ø, ‡®¨‡®æ‡®∞‡©∞ ‡®¨‡®æ‡®∞, ‡®µ‡®§‡®ø, ‡®≤‡®ñ ‡®≤‡®ñ, ‡®Ö‡®®‡®ø‡®ï ‡®¨‡®æ‡®∞ |
                | **Decision / ‡®®‡®ø‡®®‡©à** | negation / affirmation | ‡®®‡®æ, ‡®®‡®π, ‡®®‡®æ‡®π‡©Ä, ‡®®‡®ø‡®π‡®ö‡®â, ‡®Æ‡®§ |
                | **Reason / ‡®ï‡®æ‡®∞‡®£** | cause / purpose | ‡®Ø‡®æ‡®§‡©á |
                | **Stress / ‡®§‡®æ‡®ï‡©Ä‡®¶** | emphasis / focus | ‡®π‡©Ä, ‡®≠‡©Ä, ‡®π‡©à, ‡®∏‡®∞‡®™‡®∞, ‡®Æ‡©Ç‡®≤‡©á |
                
                ---

                ### ‚ñ∏ Phrase / Compound & Iterative idioms (extended reference)

                | Sub-group | Token set ‚Üí **all indeclinable adverbs** | Main category |
                |-----------|------------------------------------------|---------------|
                | **Time ‚Äî Phrase** | ‡®Ö‡®π‡®ø‡®®‡®ø‡®∏‡®ø, ‡®®‡®ø‡®∏‡®ø ‡®¨‡®æ‡®∏‡©Å‡®∞, ‡®™‡®π‡®ø‡®≤‡©ã ‡®¶‡©á, ‡®™‡®ø‡®õ‡©ã ‡®¶‡©á, ‡®∞‡®æ‡®§‡®ø ‡®¶‡®ø‡®®‡©∞‡®§‡®ø, ‡®Ö‡©∞‡®§ ‡®ï‡©Ä ‡®¨‡©á‡®≤‡®æ, ‡®Ö‡®¨ ‡®ï‡©à ‡®ï‡®π‡®ø‡®ê, ‡®Ü‡®† ‡®™‡®π‡®∞, ‡®Ü‡®¶‡®ø ‡®ú‡©Å‡®ó‡®æ‡®¶‡®ø, ‡®á‡®¨ ‡®ï‡©á ‡®∞‡®æ‡®π‡©á, ‡®®‡®ø‡®§ ‡®™‡©ç‡®∞‡®§‡®ø | Time / ‡®∏‡®Æ‡®æ |
                | **Place ‚Äî Phrase** | ‡®Ö‡©∞‡®§‡®∞‡®ø ‡®¨‡®æ‡®π‡®∞‡®ø, ‡®™‡®æ‡®∏‡®ø ‡®¶‡©Å‡®Ü‡®∏‡®ø, ‡®µ‡®ø‡®ö‡©Å‡®¶‡©á, ‡®Ü‡®∏ ‡®™‡®æ‡®∏, ‡®ä‡®™‡®∞‡®ø ‡®≠‡©Å‡®ú‡®æ ‡®ï‡®∞‡®ø, ‡®Ö‡®ó‡®π‡©Å ‡®™‡®ø‡®õ‡®π‡©Å, ‡®à‡®π‡®æ ‡®ä‡®π‡®æ, ‡®ï‡®ø‡®§‡©Å ‡®†‡®æ‡®á, ‡®§‡®ø‡®π‡®æ ‡®ß‡®ø‡®∞‡®ø, ‡®§‡®ø‡©∞‡®π‡©Å ‡®≤‡©ã‡®á, ‡®¶‡©á‡®∏ ‡®¶‡®ø‡®∏‡©∞‡®§‡®∞ | Place / ‡®•‡®æ‡®Ç |
                | **Manner ‚Äî Phrase** | ‡®§‡®æ ‡®≠‡©Ä, ‡®§‡®ø‡®≤‡©Å ‡®∏‡®æ‡®∞, ‡®á‡®ï ‡®Æ‡®®‡®ø, ‡®è‡®µ‡©à, ‡®∏‡®π‡®ú ‡®≠‡®æ‡®á, ‡®ï‡®µ‡®® ‡®Æ‡©Å‡®ñ‡®ø, ‡®ï‡®æ‡®π‡©á ‡®ï‡®â, ‡®ï‡®ø‡®â ‡®®, ‡®ï‡®ø‡®§‡©Å ‡®Ö‡®∞‡®•‡®ø, ‡®®‡®æ‡®®‡®æ ‡®¨‡®ø‡®ß‡®ø, ‡®ï‡®ø‡®µ‡©à ‡®®, ‡®∞‡®∏‡®ï‡®ø ‡®∞‡®∏‡®ï‡®ø | Manner / ‡®µ‡®ø‡®ß‡©Ä |
                | **Iterative (Time)** | ‡®´‡®ø‡®∞‡®ø ‡®´‡®ø‡®∞‡®ø, ‡®¶‡®ø‡®®‡©Å ‡®¶‡®ø‡®®‡©Å, ‡®∏‡®¶‡®æ ‡®∏‡®¶‡®æ, ‡®∏‡®æ‡®∏‡®ø ‡®∏‡®æ‡®∏‡®ø, ‡®®‡®ø‡®§ ‡®®‡®ø‡®§, ‡®®‡®ø‡®Æ‡®ñ ‡®®‡®ø‡®Æ‡®ñ, ‡®™‡®≤‡©Å ‡®™‡®≤‡©Å, ‡®¨‡®æ‡®∞‡©∞ ‡®¨‡®æ‡®∞, ‡®™‡©Å‡®®‡®π ‡®™‡©Å‡®®‡®π | Time / ‡®∏‡®Æ‡®æ |
                | **Iterative (Place)** | ‡®ú‡®§ ‡®ï‡®§, ‡®ò‡®∞‡®ø ‡®ò‡®∞‡®ø, ‡®ú‡®π ‡®ú‡®π, ‡®ú‡®ø‡®§‡©Å ‡®ú‡®ø‡®§‡©Å, ‡®¶‡©á‡®∏ ‡®¶‡®ø‡®∏‡©∞‡®§‡®∞‡®ø | Place / ‡®•‡®æ‡®Ç |
                | **Iterative (Manner)** | ‡®ù‡®ø‡®Æ‡®ø ‡®ù‡®ø‡®Æ‡®ø, ‡®§‡®ø‡®≤ ‡®§‡®ø‡®≤, ‡®ñ‡®ø‡®∞ ‡®ñ‡®ø‡®∞, ‡®∞‡®∏‡®ø‡®ï ‡®∞‡®∏‡®ø‡®ï, ‡®≤‡©Å‡®°‡®ø ‡®≤‡©Å‡®°‡®ø | Manner / ‡®µ‡®ø‡®ß‡©Ä |

                *(Duplicates collapsed; diacritics kept as in SGGS.)*

                ---

                ## 3 ¬∑ Red-flag heuristics üö®  

                | Pattern | Likely mis-tag |
                |---------|---------------|
                | Token shows **plural/oblique ‚Äì‡®Ü‡®Ç / ‚Äì‡®è / ‚Äì‡®â** agreement | Probably a noun or adjective |
                | Token immediately followed by post-position (**‡®®‡®æ‡®≤, ‡®§‡©á, ‡®µ‡®ø‡®ö**) | Treat as noun + PP |
                | Token doubles as **auxiliary verb** (*‡®π‡©Ä, ‡®π‡©à*) in context | Re-evaluate as Stress adverb OR auxiliary |
                | Same stem appears with changing endings inside verse | Likely **declinable adjective**, not adverb |
                | Gloss marks token as **object / subject** | Not an adverb |

                ---

                ## 4 ¬∑ Usage tips üí°  

                1. **No gender/number tags** ‚Äì Always set **Gender = NA** & **Number = NA** for adverbs.  
                2. **POS override wins** ‚Äì If sanity check fails, switch POS before finishing the task.  
                3. Quote at least one verb the adverb is modifying when you justify your choice.

                ---

                <sub>Source pages: Grammar book ch. 6 (pp. 6.1‚Äì6.2.6) & ‚ÄúAdverbs‚Äù sheet from 0.2 For Data to GPT.xlsx.</sub>\
                """).strip() + "\n\n"

                ending_cheat_sheet = (
                    "**ADVERBS:** Indeclinable in SGGS ‚Üí no ending table required."
                )

            elif entry["Type"] == "Postposition / ‡®∏‡©∞‡®¨‡©∞‡®ß‡®ï":
                implicit_note = textwrap.dedent("""\
                    **POSTPOSITIONS IN GURBƒÄ·πÜƒ™ ‚Äì SEEING THE HIDDEN LINKS**  

                    A postposition (_‡®∏‡©∞‡®¨‡©∞‡®ß‡®ï_) expresses the *relationship* of a noun or pronoun to the
                    rest of the clause.  Think of it as a Punjabi sibling of the English preposition,
                    except it normally **follows** the word it governs.

                    ### 1 ¬∑ Why they matter in annotation  
                    ‚Ä¢ **Old case-endings ‚Üí new helpers** ‚Äì Classical Punjabi often fused case endings
                    straight onto the noun (e.g. ‡®ï‡©à, ‡®ï‡®â).  Over centuries these endings began to act
                    like separate postpositions‚Äîand GurbƒÅ·πáƒ´ preserves *both* layers.  
                    ‚Ä¢ **One helper ‚â† one case** ‚Äì Don‚Äôt map ‚Äúeach postposition to one case‚Äù by reflex.
                    Many helpers (esp. ‚Äòof‚Äô, ‚Äòfrom‚Äô, ‚Äòwith‚Äô) sit across **multiple traditional cases**.  
                    ‚Ä¢ **Pre-noun surprise** ‚Äì Forms such as **‡®ï‡©à** can surface *before* the noun when
                    they co-occur with another postposition; still tag them as postpositions.

                    ### 2 ¬∑ How to read the Darpan gloss  
                    1. **Scan the English helper** inserted by Prof. SƒÅhib Si·πÖgh ‚Äì _to, of, from,
                    with, without, in, on, before, after, near, far‚Ä¶_  
                    2. **Locate the Punjabi token(s)** that deliver that meaning in the pƒÅ·πÖktƒ´.
                    They may be:  
                    ‚Ä¢ an **attached ending** (*‚Ä¶‡®ï‡©à ‡®∏‡©∞‡®§*),  
                    ‚Ä¢ a **stand-alone word** (*‡®®‡®æ‡®≤, ‡®µ‡®ø‡®ö, ‡®â‡®™‡®∞‡®ø*), or  
                    ‚Ä¢ an **archaic variant** (e.g. _‡®ï‡®π, ‡®µ‡®∏‡©á, ‡®¨‡®æ‡®∏‡©á_).  
                    3. **Check the noun form** ‚Äì the governed noun should be in the **oblique** (‡®∏‡©∞‡®¨‡©∞‡®ß‡®ï)
                    if the language still marks one; otherwise, rely on meaning.

                    > **Rule of thumb** ‚Äì If the gloss supplies a relational word the verse omits,
                    > treat that English word as a flag that ‚Äúa postposition is hiding here.‚Äù\
                    """).strip() + "\\n\\n"

                common_sense_note = textwrap.dedent("""\
                    **SEMANTIC SANITY CHECK ‚Äì IS THIS *REALLY* A POSTPOSITION?**  

                    ### ‚ë†  Function test  
                    ‚Ä¢ Does the candidate **link** its noun/pronoun to the verb or another noun?  
                    _Yes_ ‚Üí proceed.  _No_ ‚Üí it may be an **adverb**, **case-suffix**, or even
                    part of a **compound noun**.

                    ### ‚ë°  Morphology test  
                    ‚Ä¢ Postpositions are **indeclinable** ‚Äì no gender/number/person endings of their
                    own.  If the token shows ‚Äì‡®Ü/‡®à/‡®è etc., suspect an *oblique noun* instead.  
                    ‚Ä¢ Possessive markers **‡®¶‡®æ, ‡®¶‡©á, ‡®¶‡©Ä** *look* like adjectives but behave as
                    postpositions.  Tag them here only when they attach to another noun
                    (‚Äú‡®∞‡®æ‡®Æ **‡®¶‡®æ** ‡®¶‡®æ‡®∏‚Äù).  

                    ### ‚ë¢  Dependency test  
                    ‚Ä¢ A true postposition normally keeps a **dependent noun** close by.  If none
                    appears, ask whether the word is actually an **adverbial particle** (‚Äú‡®§‡®¶‡®ø,
                    ‡®Ö‡®ó‡©à‚Äù) or part of a **verb phrase**.

                    ### ‚ë£  Red-flag heuristics üö©  
                    | Pattern | Likely mis-tag | Example cue |
                    |---------|---------------|-------------|
                    | Token plus **another postposition** with no noun in between | Missing oblique noun | ‚Äú‡®ï‡©à **‡®®‡®æ‡®≤**‚Äù |
                    | Token followed by *‡®π‡©à/‡®π‡®®* | Probably predicate adjective | ‚Äú‡®®‡®æ‡®®‡®ï‡©Å ‡®¶‡©ã‡®ñ‡©Ä **‡®®‡®æ‡®π‡®ø**‚Äù |
                    | Token appears twice with changing endings | Declining noun, not postposition | ‚Äú‡®ò‡®∞‡®ø ‡®ò‡®∞‡®ø‚Äù |

                    ### ‚ë§  Quick role alignment  
                    | Semantic role | Common helpers (non-exhaustive) |
                    |---------------|----------------------------------|
                    | **Genitive / OF** | ‡®ï‡®æ, ‡®ï‡©á, ‡®ï‡©Ä, ‡®¶‡®æ, ‡®¶‡©á, ‡®¶‡©Ä, ‡®ï‡©ã‡®∞‡®æ |
                    | **Dative / TO, FOR** | ‡®ï‡®â, ‡®ï‡©ã, ‡®ï‡©à, ‡®®‡©Ç, ‡®≤‡®à |
                    | **Ablative / FROM** | ‡®§‡©ã‡®Ç, ‡®§‡©á, ‡®µ‡©à‡®π‡©Å, ‡®¨‡®ø‡®®, ‡®¨‡®æ‡®π‡®∞ |
                    | **Instrumental / WITH** | ‡®®‡®æ‡®≤, ‡®∏‡©∞‡®ó, ‡®∏‡®æ‡®•, ‡®∏‡®ø‡®â, ‡®∏‡©á‡®§‡©Ä |
                    | **Locative / IN, ON, AT** | ‡®µ‡®ø‡®ö, ‡®Ö‡©∞‡®¶‡®∞‡®ø, ‡®Æ‡®æ‡®π‡®ø, ‡®â‡®™‡®∞‡®ø, ‡®ä‡®§‡©á |
                    | **Orientational / BEFORE, AFTER, NEAR, FAR** | ‡®Ö‡®ó‡©à, ‡®™‡®ø‡®õ‡©à, ‡®ï‡©ã‡®≤, ‡®®‡®ø‡®ï‡®ü, ‡®¶‡©Ç‡®∞‡®ø |

                    _If a helper can sit in more than one row, choose the case that best matches the
                    **meaning of the clause**, and note the alternative in comments._\
                    """).strip() + "\\n\\n"
                
                ending_cheat_sheet = textwrap.dedent("""\
                    **POSTPOSITION QUICK-REFERENCE ‚Äì SURFACE FORMS BY SEMANTIC GROUP**  

                    | Role (Eng.) | Core Punjabi forms* | Notes |
                    |-------------|---------------------|-------|
                    | **OF / Possessive** | ‡®¶‡®æ, ‡®¶‡©á, ‡®¶‡©Ä ¬∑ ‡®ï‡®æ, ‡®ï‡©á, ‡®ï‡©Ä ¬∑ ‡®ï‡®æ, ‡®ï‡©à, ‡®ï‡©à‡®π‡®ø‡®â ¬∑ ‡®ï‡©ã‡®∞‡®æ / ‡®ï‡©ã‡®∞‡©à | Masculine/Feminine variants; decline with possessed noun, not with owner |
                    | **TO / FOR** | ‡®ï‡®â, ‡®ï‡©Ç, ‡®ï‡©à, ‡®ï‡©ã ¬∑ ‡®®‡©Ç, ‡®®‡©Ç‡©∞ ¬∑ ‡®≤‡®à | Older endings (‡®ï‡®â‚Ä¶) often fuse; **‡®®‡©Ç‡©∞** modern |
                    | **FROM / OUT OF** | ‡®§‡©ã‡®Ç, ‡®§‡©á, ‡®â‡®§‡©ã‡®Ç, ‡®µ‡©à‡®π‡©Å, ‡®¨‡®æ‡®π‡®∞, ‡®¨‡®ø‡®®‡®æ | Ablative / separative sense; *‡®¨‡®ø‡®®‡®æ* also ‚Äúwithout‚Äù |
                    | **WITH / BY / ALONG** | ‡®®‡®æ‡®≤, ‡®®‡®æ‡®≤‡©á, ‡®∏‡©∞‡®ó, ‡®∏‡®æ‡®•, ‡®∏‡®ø‡®â, ‡®∏‡©á‡®§‡©Ä | Instrumental & associative; choice shaped by metre |
                    | **WITHOUT / THAN** | ‡®¨‡®æ‡®ú‡®π‡©Å, ‡®¨‡®æ‡®ó‡©à, ‡®¨‡®ø‡®®, ‡®¨‡®ø‡®®‡©Å, ‡®µ‡®ø‡®£, ‡®µ‡®ø‡®£‡®π‡©Å, ‡®•‡©ã‡©ú‡®æ | Negative / comparative nuance |
                    | **IN / INSIDE / WITHIN** | ‡®µ‡®ø‡®ö, ‡®µ‡®ø‚∏±‡®ö, ‡®Ö‡©∞‡®¶‡®∞‡®ø, ‡®Æ‡®æ‡®π‡®ø, ‡®Æ‡®π‡®ø, ‡®Æ‡®æ‡®π‡®∞‡©à | Locative & internal |
                    | **ON / OVER / ABOVE** | ‡®â‡®™‡®∞‡®ø, ‡®â‡®™‡®∞, ‡®â‡®§‡©á, ‡®ä‡®§‡©á, ‡®ä‡®™‡®∞‡®ø | Spatial elevation; *‡®§‡©á* doubles as generic PP |
                    | **UNDER / BELOW** | ‡®§‡®≤‡®ø, ‡®•‡®≤‡©à, ‡®π‡©á‡®†, ‡®π‡©á‡®†‡®æ‡®Ç | Lower level |
                    | **BEFORE / FRONT** | ‡®Ö‡®ó‡©à, ‡®Ö‡®ó‡©á | Temporal or spatial precedence |
                    | **AFTER / BEHIND** | ‡®™‡®ø‡®õ‡©à, ‡®™‡®æ‡®õ‡©à, ‡®™‡®ø‡®õ‡©ã | Temporal or spatial following |
                    | **TOWARDS / NEAR / FAR** | ‡®µ‡®≤, ‡®ï‡®®, ‡®ï‡©ã‡®≤, ‡®ï‡©ã‡®≤‡©Ä, ‡®®‡®ø‡®ï‡®ü, ‡®™‡®æ‡®∏‡®ø, ‡®™‡®æ‡®∏‡©á, ‡®¶‡©Ç‡®∞‡®ø | Directional & proximity |

                    <sub>*Forms collated from pp. 1-7 of your textbook; diacritics left as printed.
                    The list is not exhaustive‚Äîadd dialectal or Braj variants as you meet them.</sub>

                    **Oblique rule** ‚Äì The governed noun normally appears in the **oblique**; the
                    postposition itself **never inflects**.

                    **Pre-noun exception** ‚Äì When **‡®ï‡©à** precedes another PP, it may surface *before*
                    its noun (e.g. ‚Äú‡®Æ‡©∞‡®®‡©á ‡®ú‡®Æ **‡®ï‡©à** ‡®∏‡®æ‡®• ‡®® ‡®ú‡®æ‡®á‚Äù) ‚Äì still tag as postposition.

                    **Cross-case cautions**  
                    ‚Ä¢ Some helpers (esp. ‚Äúwith‚Äù, ‚Äúin‚Äù, ‚Äúfrom‚Äù) can realise **Instrumental, Locative,
                    or Ablative** ‚Äì decide by semantics.  
                    ‚Ä¢ Genitive set **‡®¶‡®æ/‡®¶‡©á/‡®¶‡©Ä** functions like an adjective in modern speech but
                    grammatically remains a postposition in SGGS.

                    _Use this sheet to *reject impossible guesses* and to **confirm legal surface
                    forms** before finalising your annotation._\
                    """).strip() + "\\n\\n"

            elif entry["Type"] == "Conjunction / ‡®Ø‡©ã‡®ú‡®ï":
                implicit_note = textwrap.dedent("""\
                    **CONJUNCTIONS IN GURBƒÄ·πÜƒ™ ‚Äì HOW TO HEAR THE HINGES**

                    A conjunction (_‡®Ø‡©ã‡®ú‡®ï_) links words, phrases, or entire clauses‚Äî*and, but, or,
                    if ‚Ä¶ then, even though‚Ä¶. *  GurbƒÅ·πáƒ´ uses a small core set, but the
                    multilingual texture of the text supplies many **variants** (‡©≤‡©à, ‡®Ö‡®§‡©á, ‡®Ö‡®â,
                    ‡®´‡©Å‡®®‡®ø; ‡®ú‡©á, ‡®ú‡©á‡®ï‡®∞; ‡®§‡®æ, ‡®§‡®æ‡®Ç, ‡®§‡®≠).

                    #### 1 ¬∑ Spotting them in the verse
                    1. **Look for clause boundaries** ‚Äì commas or the metrical ‚Äú||‚Äù often signal the
                    join.  
                    2. **Map the gloss cue** ‚Äì Prof. SƒÅhib Si·πÖgh frequently inserts
                    *and / but / or / if / then / even*, etc.  Trace that helper back to a Punjabi
                    token (sometimes a tiny vowel like **‡®§, ‡®ú‡©á, ‡®§‡©á**).  
                    3. **Check the flow** ‚Äì removing a true conjunction should split the sentence
                    into two meaningful parts; if the sense collapses, the token may be an
                    **adverb** (*‡®§‡©å‡®Ç = then* vs. *‡®§‡©ã‡®Ç = from*), **post-position**, or **particle**.

                    > **Rule of thumb** ‚Äì If the gloss supplies an English linker and the Punjabi
                    > token neither declines nor carries case, you‚Äôve found a conjunction.
                    """).strip() + "\\n\\n"
                
                common_sense_note = textwrap.dedent("""\
                    **SEMANTIC SANITY CHECK ‚Äì DOES THIS REALLY JOIN THINGS?**

                    | Quick test | Keep as conjunction ‚úîÔ∏é | Rethink ‚úò |
                    |------------|------------------------|-----------|
                    | **Function** | Links two clauses / words of equal status | Adds a helper to a noun (*post-position*) |
                    | **Morphology** | Indeclinable; no gender/number | Ends -‡®Ü/-‡®à/-‡®è ‚Üí likely adjective/noun |
                    | **Mobility** | Can often move to clause edge without breaking grammar | Locked to noun it follows ‚Üí PP/adjective |
                    | **Gloss cue** | gloss shows *and, but, or, if ‚Ä¶ then* | gloss shows *to, of, from* ‚Üí case helper |

                    #### Red-flag patterns üö©
                    * Token plus **post-position** (e.g. *‡®ú‡©á ‡®ï‡©ã*): maybe *‡®ú‡©á* = ‚Äúif‚Äù (OK) but *‡®ï‡©ã* =
                    Dative ‚Üí label both separately.  
                    * **‡®®‡©Ä‚Ä¶‡®®‡®æ** or **‡®®‡©ã‚Ä¶‡®®‡©ã** ‚Äì might be emphatic repetition, not conjunction.  
                    * **‡®§‡®æ/‡®§‡©á/‡®§‡©ã‡®Ç**: confirm r√¥le‚Äî*‡®§‡®æ* = ‚Äúthen‚Äù, *‡®§‡©á* often Locative PP, *‡®§‡©ã‡®Ç* Ablative.
                    """).strip() + "\\n\\n"
                
                ending_cheat_sheet = textwrap.dedent("""\
                    **CONJUNCTION QUICK-REFERENCE ‚Äì HIGH-FREQ FORMS IN SGGS**

                    | Logical role | Punjabi forms* | Example gloss cue |
                    |--------------|---------------|-------------------|
                    | **AND / THEN** | ‡®§‡©á, ‡®Ö‡®§‡©á, ‡®Ö‡®§‡®ø, ‡®Ö‡®â, ‡®Ö‡®µ‡®∞, ‡®Ö‡®â‡®∞‡©Å, ‡®´‡©Å‡®®‡®ø | ‚Äúand‚Äù, ‚Äúthen‚Äù, ‚Äúalso‚Äù |
                    | **OR** | ‡®ï‡©à, ‡®ï‡®ø, ‡®Ö‡®ï‡©á | ‚Äúor / whether‚Äù |
                    | **BUT / HOWEVER** | ‡®ò‡®ü, ‡®™‡®∞, ‡®™‡®∞‡©∞‡®§‡©Ç, ‡®´‡©Å‡®®‡®ø | ‚Äúbut‚Äù, ‚Äúyet‚Äù |
                    | **IF** | ‡®ú‡©á, ‡®ú‡©á‡®ï‡®∞, ‡®ú‡©á‡®µ‡©Ä | ‚Äúif / provided that‚Äù |
                    | **IF ‚Ä¶ THEN** | ‡®ú‡©á ‚Ä¶ ‡®§‡®æ/‡®§‡®æ‡®Ç/‡®§‡©ã‡®Ç | paired correlative |
                    | **EVEN IF / EVEN THEN** | ‡®§, ‡®ú‡©á, ‡®≠‡®æ‡®µ‡©á, ‡®§‡®â ‡®≠‡©Ä, ‡®§‡®â, ‡®§‡®â‡®Ç | concessive |
                    | **NEITHER ‚Ä¶ NOR** | ‡®® ‚Ä¶ ‡®®‡®æ | correlative negative |
                    | **OTHERWISE** | ‡®®‡®§ ‡®∞‡®ø, ‡®®‡®§‡©Ç, ‡®®‡®π‡©Ä‡®Ç, ‡®®‡®π‡©Ä‡®Ç ‡®§‡®æ‡®Ç | ‚Äúotherwise‚Äù |
                    | **THEREFORE / HENCE** | ‡®§‡®æ, ‡®§‡®æ ‡®§‡©á, ‡®§‡®∏‡©Ç, ‡®ï‡®æ ‡®§‡©á | result / inference |
                    | **AS / LIKE** | ‡®ú‡®ø‡®â, ‡®ú‡®ø‡®µ‡©á‡®Ç | comparative |
                    | **LEST** | ‡®Æ‡®§‡©Å | preventative |

                    <sub>*Forms taken from textbook pp. 8.1 ‚Äì 8.4; diacritics preserved.</sub>

                    **Key reminders**

                    * **Indeclinable** ‚Äì conjunctions never carry case or agreement.
                    * **Dual tokens** ‚Äì Some forms (*‡®§‡®æ, ‡®§‡©á, ‡®§‡©ã‡®Ç*) double as post-positions.
                    Decide by context: if it *links* clauses ‚Üí conjunction; if it *marks* a noun
                    ‚Üí post-position.
                    * **Correlative pairs** ‚Äì Tag both halves (e.g. **‡®ú‡©á** ‚Ä¶ **‡®§‡®æ‡®Ç**) as one
                    logical conjunction with a note ‚Äúcorrelative‚Äù.
                    """).strip() + "\\n\\n"
                
            elif entry["Type"] == "Interjection / ‡®µ‡®ø‡®∏‡®Æ‡®ø‡®ï":
                implicit_note = textwrap.dedent("""\
                    **INTERJECTIONS IN GURBƒÄ·πÜƒ™ ‚Äì PURE, UNINFLECTED EMOTION**

                    An interjection (_‡®µ‡®ø‡®∏‡®Æ‡®ø‡®ï_) erupts outside normal grammar to voice **feeling**:
                    surprise, pain, devotion, blessing, awe‚Ä¶  Because they sit *outside* the clause
                    structure, they **never govern case, never inflect, never agree**.

                    #### 1 ¬∑ What to notice in a verse
                    1. **Standalone or comma-bound** tokens ‚Äì often at the start, end, or mid-clause,
                    separated by a breve pause.  E.g. **‡®µ‡®æ‡®π‡©Å ‡®µ‡®æ‡®π‡©Å**, **‡®π‡©à ‡®π‡©à**, **‡®π‡®∞‡®ø ‡®π‡®∞‡®ø**.
                    2. **Gloss cue** ‚Äì Prof. SƒÅhib Si·πÖgh usually inserts an English exclamation
                    (*O!, Alas!, Wow!, Blessed!*) or italicises the Punjabi for emphasis.
                    3. **No syntactic load** ‚Äì if you remove the interjection, the grammar of the
                    sentence remains intact (though colour is lost).

                    #### 2 ¬∑ Ten broad emotional classes in SGGS
                    1. **Vocative** ‚Äì calling or invoking (*‡®è, ‡®ê, ‡®ì, ‡®π‡©à, ‡®π‡®â, ‡®π‡©á ‡®ú‡©Ä‚Ä¶*).  
                    2. **Repulsive** ‚Äì aversion or disgust (*‡®µ‡®ø‡®ö‡©Å, ‡®´‡®ø‡®ü‡©Å*).  
                    3. **Painful** ‚Äì sorrow, lament (*‡®π‡®æ ‡®π‡®æ, ‡®π‡®æ‡®è ‡®π‡®æ‡®è, ‡®π‡©à ‡®π‡©à*).  
                    4. **Submission** ‚Äì ‚ÄòDivine willing‚Äô (*‡®Ö‡®≤‡®π*).  
                    5. **Wondrous** ‚Äì ecstatic awe (*‡®µ‡®æ‡®π‡©Å ‡®µ‡®æ‡®π‡©Å, ‡®µ‡®æ‡®π ‡®≠‡©à‡®∞‡©Ä*).  
                    6. **Caution / Warning** ‚Äì prudent cry (*‡®π‡®∞‡®ø ‡®π‡®∞‡®ø ‡®π‡®∞‡©á* used admonishingly).  
                    7. **Blessing** ‚Äì goodwill (*‡®ú‡©Å‡®ó‡©Å ‡®ú‡©Å‡®ó‡©Å ‡®ú‡©Ä‡®µ‡®π‡©Å*).  
                    8. **Curse** ‚Äì condemnation (*‡®ú‡®≤‡®â, ‡®ú‡®≤‡®ø ‡®ú‡®æ‡®â*).  
                    9. **Sacrificial** ‚Äì self-offering (*‡®¨‡®≤‡®ø‡®π‡®æ‡®∞‡©á, ‡®¨‡®≤‡®ø ‡®¨‡®≤‡®ø*).  
                    10. **Reverence** ‚Äì respectful welcome (*‡®Ü‡®á ‡®ú‡©Ä, ‡®™‡®ø‡®õ‡©ã ‡®ú‡©Ä*).

                    > **Rule of thumb** ‚Äì if the word communicates *only* emotion and detaches
                    > cleanly from clause syntax, tag it as Interjection; otherwise test Adverb,
                    > Vocative Noun, or Particle.
                    """).strip() + "\\n\\n"

                common_sense_note = textwrap.dedent("""\
                    **SEMANTIC SANITY CHECK ‚Äì IS THIS TOKEN *JUST* AN EMOTION?**

                    | Quick probe | Keep as Interjection ‚úî | Rethink ‚úñ |
                    |-------------|-----------------------|-----------|
                    | **Function** | Adds emotional colour, no syntactic role | Performs grammatical work (case, link, inflection) |
                    | **Inflection** | Completely indeclinable | Shows ‚Äì‡®Ü / ‚Äì‡®à / ‚Äì‡®è endings ‚Üí maybe adjective/noun |
                    | **Dependence** | Can float; removal leaves clause intact | Sentence breaks ‚Üí probably verb/particle |
                    | **Gloss cue** | Gloss marks ‚ÄúO!‚Äù, ‚ÄúAlas!‚Äù, ‚ÄúBlessed!‚Äù etc. | Gloss gives ‚Äúto, from, with‚Äù ‚Üí post-position |

                    #### Red-flag patterns üö©
                    * **‡®µ‡®æ‡®π‡©Å ‡®µ‡®æ‡®π‡©Å** appears as noun/adjective elsewhere ‚Äì decide per context.  
                    * **‡®π‡©à ‡®Æ‡©à, ‡®π‡©á ‡®≠‡®æ‡®à** ‚Äì first token vocative interjection, second token noun;
                    split tags, don‚Äôt bundle.  
                    * Repeated **‡®π‡®∞‡®ø ‡®π‡®∞‡®ø** could be mantra (noun) *or* caution interjection ‚Äì
                    weigh meaning.

                    _For every interjection, fill **Number = NA** and **Gender = NA**; they never
                    agree with anything._
                    """).strip() + "\\n\\n"
                
                ending_cheat_sheet = textwrap.dedent("""\
                    **INTERJECTION QUICK-REFERENCE ‚Äì FREQUENT FORMS BY EMOTIONAL CLASS**

                    | Class               | High-frequency tokens* (SGGS spelling)        |
                    |---------------------|----------------------------------------------|
                    | **Vocative**        | ‡®è, ‡®ê, ‡®ì, ‡®ì‡®π, ‡®π‡©á, ‡®π‡©à, ‡®π‡®â, ‡®π‡®≤‡©à, ‡®Æ‡©Å‡®∏‡©à, ‡®ú‡©Ä, ‡®∞‡©á, ‡®¨‡©á |
                    | **Repulsive**       | ‡®µ‡®ø‡®ö‡©Å, ‡®´‡®ø‡®ü‡©Å                                   |
                    | **Painful**         | ‡®π‡®æ ‡®π‡®æ, ‡®π‡®æ‡®è ‡®π‡®æ‡®è, ‡®π‡©à ‡®π‡©à, ‡®ù‡©Ç‡®Ö‡®π ‡®¨‡©Ç‡®¢‡®π           |
                    | **Submission**      | ‡®Ö‡®≤‡®π                                          |
                    | **Wondrous**        | ‡®µ‡®æ‡®π‡©Å ‡®µ‡®æ‡®π‡©Å, ‡®µ‡®æ‡®π ‡®µ‡®æ‡®π, ‡®µ‡®æ‡®Ö ‡®µ‡®æ‡®Ö, ‡®µ‡®π‡©Å ‡®µ‡®π‡©Å, ‡®µ‡®æ‡®π ‡®≠‡©à, ‡®µ‡®π‡©Å ‡®µ‡®π‡©Å |
                    | **Caution / Warning** | ‡®π‡®∞‡®ø ‡®π‡®∞‡®ø ‡®π‡®∞‡©á, ‡®π‡®∞‡©á ‡®π‡®∞‡©á                       |
                    | **Blessing**        | ‡®ú‡©Å‡®ó‡©Å ‡®ú‡©Å‡®ó‡©Å ‡®ú‡©Ä‡®µ‡®π‡©Å, ‡®ú‡©Å‡®ó‡©Å ‡®ú‡©Å‡®ó‡©Å ‡®ú‡©Ä‡®µ‡©à              |
                    | **Curse**           | ‡®ú‡®≤‡®â, ‡®ú‡®≤‡®ø ‡®ú‡®æ‡®â, ‡®ú‡®≤‡®ø ‡®ú‡®≤‡®ø ‡®ú‡®∞‡®π‡©Å                  |
                    | **Sacrificial**     | ‡®¨‡®≤‡®ø‡®π‡®æ‡®∞‡©á, ‡®¨‡®≤‡®ø ‡®¨‡®≤‡®ø, ‡®µ‡®æ‡®∞‡©Ä ‡®µ‡©∞‡®û‡®æ, ‡®ï‡®£‡©Ä‡®è ‡®µ‡©∞‡®û‡®æ    |
                    | **Reverence**       | ‡®Ü‡®â ‡®ú‡©Ä, ‡®Ü‡®á ‡®ú‡©Ä, ‡®™‡®ø‡®õ‡©ã ‡®ú‡©Ä                       |

                    <sub>*Tokens taken from textbook pp. 9.1‚Äì9.4; diacritics preserved.  
                    Feel free to trim or expand as corpus stats evolve.</sub>

                    **Remember** ‚Äì Interjections are **indeclinable** and **carry no grammatical
                    features**.  Therefore the spreadsheet needs **no ending table** beyond this
                    categorical list.
                    """).strip() + "\\n\\n"
                
            notes_block = ending_cheat_sheet + implicit_note + common_sense_note

            prompt = textwrap.dedent(f"""
                **You are a Punjabi grammar expert.**

                Below are the *allowed choices* for each feature of the highlighted word:

                {opts_block}
                {matches_block}

                {notes_block}

                **IMPORTANT:**  
                Base **all** confirmations or corrections **solely on the Darpan translation** below.  
                Do **not** consult any other translation or external context.

                **My Current Selections:**  
                - Word Under Analysis: **{ve}**  
                - Number / ‡®µ‡®ö‡®®: **{num}**  
                - Grammar Case / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£: **{gram}**  
                - Gender / ‡®≤‡®ø‡©∞‡®ó: **{gen}**  
                - Word Root: **{root}**

                **Context (use *only* the Darpan gloss):**  
                ‚Ä¢ **Verse:** {verse}  
                ‚Ä¢ **Darpan Translation:** {trans}  
                ‚Ä¢ **Darpan-Meanings:** {dm}

                **Task:**  
                1. **Confirm or correct** each feature‚Äîif blank, **choose** the best option  
                (one-sentence rationale citing the inflection or usage).
                ‚Ä¢ For finite forms, choose **1st / 2nd / 3rd Person** in Word-Root (do not use Past/Perfect there). 
                2. **Corrections**, if any:  
                - Number ‚Üí ‚Ä¶  
                - Grammar Case ‚Üí ‚Ä¶  
                - Word Root ‚Üí ‚Ä¶  
                3. **Example Usage:**  
                Provide **one** new GurbƒÅ·πáƒ´-style sentence using **‚Äú{ve}‚Äù** with the
                confirmed ending, number, case, gender, and root.
                4. **Table citation:**  
                Quote the person √ó number √ó tense row header you matched in the Canonical table  
                (e.g., ‚Äú1 Sg | Past‚Äù). **Use that row‚Äôs category name for ‚ÄúGrammar Case / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£,‚Äù unless a sanity rule forbids it.**
                5. **Ending ‚áÑ Case cross-check:**
                ‚Ä¢ If the cheat-sheet already lists a suffix for your chosen case, use it.  
                ‚Ä¢ If the case is **missing**, you may propose a likely form
                    (or say ‚Äúuninflected‚Äù) **but give one-line reasoning**.
                6. **Commentary:**  
                Please write 2‚Äì3 sentences as ‚ÄúChatGPT Commentary:‚Äù explaining how you arrived at each feature choice.
            """).strip()

            self.root.clipboard_clear()
            self.root.clipboard_append(prompt)
            messagebox.showinfo(
                "Prompt Ready",
                "The detailed-grammar prompt has been copied to your clipboard.\n"
                "Paste it into ChatGPT, then paste its response back into the text box."
            )

        tk.Button(
            frm, text="üìã Build Detailed Grammar Prompt",
            font=("Arial", 12, "italic"),
            bg="white", fg="dark cyan",
            command=build_detailed_prompt
        ).grid(row=6, column=0, columnspan=2, pady=(10, 0))

        # 6) --------------  Bottom buttons (unchanged)  --------------
        sep = tk.Frame(win, bg="#cccccc", height=2)
        sep.pack(side=tk.BOTTOM, fill=tk.X, padx=20, pady=(5, 0))

        btns = tk.Frame(win, bg="light gray")
        btns.pack(side=tk.BOTTOM, fill=tk.X, padx=20, pady=20)

        tk.Button(
            btns, text="‚Äπ Back",
            font=("Arial", 12), bg="gray", fg="white",
            command=lambda: [win.destroy(),
                            self.show_matches_grammar(self._last_matches, word, index)]
        ).pack(side=tk.LEFT)

        tk.Button(
            btns, text="Save & Finish ‚Üí",
            font=("Arial", 12, "bold"), bg="dark cyan", fg="white",
            command=lambda: self.on_accept_detailed_grammar(win)
        ).pack(side=tk.RIGHT)

        win.transient(self.root)
        win.grab_set()
        self.root.wait_window(win)








































    def launch_verse_analysis_dashboard(self):
        """Clears the main dashboard and launches the Verse Analysis Dashboard."""
        for widget in self.root.winfo_children():
            widget.destroy()
        self.root.title("Verse Analysis Dashboard")
        self.setup_verse_analysis_dashboard()

    def setup_verse_analysis_dashboard(self):
        """Builds the Verse Analysis Dashboard interface."""
        for widget in self.root.winfo_children():
            widget.destroy()
        self.main_frame = tk.Frame(self.root, bg='light gray', padx=10, pady=10)
        self.main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=20)

        header_label = tk.Label(
            self.main_frame,
            text="Verse Analysis Dashboard",
            font=('Arial', 18, 'bold'),
            bg='dark slate gray',
            fg='white',
            pady=10
        )
        header_label.pack(fill=tk.X, pady=10)

        # Create a frame for the option buttons
        button_frame = tk.Frame(self.main_frame, bg='light gray')
        button_frame.pack(expand=True)

        # Literal Translation Button
        literal_btn = tk.Button(
            button_frame,
            text="Literal Translation",
            font=('Arial', 14, 'bold'),
            bg='dark cyan',
            fg='white',
            padx=20,
            pady=10,
            command=self.launch_literal_analysis
        )
        literal_btn.pack(pady=10)

        # New button for editing saved literal translation
        edit_saved_btn = tk.Button(
            button_frame,
            text="Edit Saved Literal Translation",
            font=('Arial', 14, 'bold'),
            bg='dark cyan',
            fg='white',
            padx=20,
            pady=10,
            command=self.launch_select_verse  # Updated command
        )
        edit_saved_btn.pack(pady=10)

        # Placeholder for Spiritual Translation (future implementation)
        spiritual_btn = tk.Button(
            button_frame,
            text="Spiritual Translation (Coming Soon)",
            font=('Arial', 14, 'bold'),
            bg='gray',
            fg='white',
            padx=20,
            pady=10,
            state=tk.DISABLED
        )
        spiritual_btn.pack(pady=10)

        # Placeholder for Translation Management (future implementation)
        management_btn = tk.Button(
            button_frame,
            text="Translation Management (Coming Soon)",
            font=('Arial', 14, 'bold'),
            bg='gray',
            fg='white',
            padx=20,
            pady=10,
            state=tk.DISABLED
        )
        management_btn.pack(pady=10)

        # Back button to return to the main dashboard
        back_btn = tk.Button(
            self.main_frame,
            text="Back to Main Dashboard",
            font=('Arial', 14, 'bold'),
            bg='red',
            fg='white',
            padx=20,
            pady=10,
            command=self.show_dashboard
        )
        back_btn.pack(pady=10)

    def launch_select_verse(self):
        """
        Creates a centered modal window to let the user select a verse by:
        1. Searching by verse content
        2. Filtering by metadata (Raag, Writer, Bani, Page)
        Displays only those verses that already exist in the assessment Excel.
        """

        # === Setup modal ===
        select_win = tk.Toplevel(self.root)
        select_win.title("Select Verse")
        select_win.geometry("800x600")
        select_win.state("zoomed")
        select_win.configure(bg="light gray")

        # === Load Excel data ===
        file_path = "1.2.1 assessment_data.xlsx"
        df_existing = self.load_existing_assessment_data(file_path)

        # === Center content ===
        center_frame = tk.Frame(select_win, bg="light gray")
        center_frame.pack(expand=True)

        content_frame = tk.Frame(center_frame, bg="light gray", width=960)
        content_frame.pack()

        # === Header ===
        header_label = tk.Label(
            content_frame,
            text="Select Verse",
            bg="dark slate gray",
            fg="white",
            font=("Helvetica", 18, "bold"),
            padx=20, pady=10
        )
        header_label.pack(fill=tk.X, pady=(0, 10))

        # === Mode selection (Search or Filter) ===
        mode_var = tk.StringVar(value="search")
        mode_frame = tk.Frame(content_frame, bg="light gray")
        mode_frame.pack(pady=10)

        tk.Radiobutton(mode_frame, text="Search by Verse", variable=mode_var, value="search",
                    bg="light gray", font=("Arial", 12)).pack(side=tk.LEFT, padx=10)
        tk.Radiobutton(mode_frame, text="Filter by Metadata", variable=mode_var, value="filter",
                    bg="light gray", font=("Arial", 12)).pack(side=tk.LEFT, padx=10)

        # === Create container for dynamic frames ===
        main_content_area = tk.Frame(content_frame, bg="light gray")
        main_content_area.pack(fill=tk.BOTH, expand=True)

        # === Search Frame ===
        search_frame = tk.Frame(main_content_area, bg="light gray")
        tk.Label(search_frame, text="Enter or paste a verse to search:", bg="light gray",
                font=("Arial", 12, "bold")).pack(pady=5)

        search_entry = tk.Entry(search_frame, font=("Arial", 12), width=80)
        search_entry.pack(pady=5)

        search_button = tk.Button(
            search_frame, text="Search", font=("Arial", 12, "bold"),
            bg="navy", fg="white", width=12,
            command=lambda: perform_search(search_entry.get())
        )
        search_button.pack(pady=5)

        search_results_list = tk.Listbox(search_frame, font=("Arial", 12), width=80, height=10)
        search_results_list.pack(pady=10)

        # === Filter Frame ===
        filter_frame = tk.Frame(main_content_area, bg="light gray")
        tk.Label(filter_frame, text="Filter verses by metadata:", bg="light gray",
                font=("Arial", 12, "bold")).pack(pady=5)

        filter_controls = tk.Frame(filter_frame, bg="light gray")
        filter_controls.pack(pady=5)

        # === Dropdowns for filter ===
        df = df_existing.copy()
        raag_var, writer_var, bani_var, page_var = tk.StringVar(), tk.StringVar(), tk.StringVar(), tk.StringVar()
        # Sort Raag, Writer, Bani based on first page appearance
        initial_raag = df.dropna(subset=["Raag (Fixed)"]).drop_duplicates("Raag (Fixed)", keep="first").sort_values("Page Number")["Raag (Fixed)"].tolist()
        initial_writer = df.dropna(subset=["Writer (Fixed)"]).drop_duplicates("Writer (Fixed)", keep="first").sort_values("Page Number")["Writer (Fixed)"].tolist()
        initial_bani = df.dropna(subset=["Bani Name"]).drop_duplicates("Bani Name", keep="first").sort_values("Page Number")["Bani Name"].tolist()
        # Sort Page Numbers numerically
        initial_page = sorted(df["Page Number"].dropna().unique())
        initial_page = [str(p) for p in initial_page]  # Convert back to string for dropdown


        tk.Label(filter_controls, text="Raag:", bg="light gray", font=("Arial", 12)).grid(row=0, column=0, padx=5, pady=5)
        raag_dropdown = ttk.Combobox(filter_controls, textvariable=raag_var, values=initial_raag, width=15)
        raag_dropdown.grid(row=0, column=1, padx=5, pady=5)

        tk.Label(filter_controls, text="Writer:", bg="light gray", font=("Arial", 12)).grid(row=0, column=2, padx=5, pady=5)
        writer_dropdown = ttk.Combobox(filter_controls, textvariable=writer_var, values=initial_writer, width=15)
        writer_dropdown.grid(row=0, column=3, padx=5, pady=5)

        tk.Label(filter_controls, text="Bani:", bg="light gray", font=("Arial", 12)).grid(row=0, column=4, padx=5, pady=5)
        bani_dropdown = ttk.Combobox(filter_controls, textvariable=bani_var, values=initial_bani, width=15)
        bani_dropdown.grid(row=0, column=5, padx=5, pady=5)

        tk.Label(filter_controls, text="Page:", bg="light gray", font=("Arial", 12)).grid(row=0, column=6, padx=5, pady=5)
        page_dropdown = ttk.Combobox(filter_controls, textvariable=page_var, values=initial_page, width=10)
        page_dropdown.grid(row=0, column=7, padx=5, pady=5)

        # === Update filter dropdowns dynamically ===
        def update_dropdowns(*args):
            filtered_df = df.copy()
            if raag_var.get():
                filtered_df = filtered_df[filtered_df["Raag (Fixed)"] == raag_var.get()]
            if writer_var.get():
                filtered_df = filtered_df[filtered_df["Writer (Fixed)"] == writer_var.get()]
            if bani_var.get():
                filtered_df = filtered_df[filtered_df["Bani Name"] == bani_var.get()]
            if page_var.get():
                filtered_df = filtered_df[filtered_df["Page Number"].astype(str) == page_var.get()]
            raag_dropdown['values'] = (
                filtered_df.dropna(subset=["Raag (Fixed)"])
                .drop_duplicates("Raag (Fixed)", keep="first")
                .sort_values("Page Number")["Raag (Fixed)"].tolist()
            )

            writer_dropdown['values'] = (
                filtered_df.dropna(subset=["Writer (Fixed)"])
                .drop_duplicates("Writer (Fixed)", keep="first")
                .sort_values("Page Number")["Writer (Fixed)"].tolist()
            )

            bani_dropdown['values'] = (
                filtered_df.dropna(subset=["Bani Name"])
                .drop_duplicates("Bani Name", keep="first")
                .sort_values("Page Number")["Bani Name"].tolist()
            )

            sorted_pages = sorted(filtered_df["Page Number"].dropna().unique())
            page_dropdown['values'] = [str(p) for p in sorted_pages]

        for var in (raag_var, writer_var, bani_var, page_var):
            var.trace_add("write", update_dropdowns)

        # === Filter Button & Listbox ===
        filter_button = tk.Button(filter_frame, text="Apply Filter", font=("Arial", 12, "bold"),
                                bg="navy", fg="white", command=lambda: update_verse_list())
        filter_button.pack(pady=5)

        filter_results_list = tk.Listbox(filter_frame, font=("Arial", 12), width=80, height=10)
        filter_results_list.pack(pady=10)

        # === Toggle mode display ===
        def update_mode():
            filter_frame.pack_forget()
            search_frame.pack_forget()
            if mode_var.get() == "search":
                search_frame.pack(pady=10)
            else:
                filter_frame.pack(pady=10)

        mode_var.trace_add("write", lambda *args: update_mode())
        update_mode()

        # === Search Logic ===
        def perform_search(query):
            search_results_list.delete(0, tk.END)
            if not query:
                search_results_list.insert(tk.END, "No query entered.")
                return
            headers, candidate_matches = self.match_sggs_verse(query)
            candidate_verses = list(dict.fromkeys(candidate["Verse"] for candidate in candidate_matches))
            excel_verses = set(df_existing["Verse"].unique())
            unique_verses = [v for v in candidate_verses if v in excel_verses]
            if unique_verses:
                for verse in unique_verses:
                    search_results_list.insert(tk.END, verse)
            else:
                search_results_list.insert(tk.END, "No analyzed verse matches found.")

        # === Filter Logic ===
        def update_verse_list():
            filter_results_list.delete(0, tk.END)
            filtered_df = df_existing.copy()
            if raag_var.get():
                filtered_df = filtered_df[filtered_df["Raag (Fixed)"].str.contains(raag_var.get(), case=False, na=False)]
            if writer_var.get():
                filtered_df = filtered_df[filtered_df["Writer (Fixed)"].str.contains(writer_var.get(), case=False, na=False)]
            if bani_var.get():
                filtered_df = filtered_df[filtered_df["Bani Name"].str.contains(bani_var.get(), case=False, na=False)]
            if page_var.get():
                filtered_df = filtered_df[filtered_df["Page Number"].astype(str).str.contains(page_var.get(), case=False, na=False)]
            for verse in filtered_df["Verse"].unique():
                filter_results_list.insert(tk.END, verse)

        # === Finalize & Back buttons ===
        def finalize_selection():
            if mode_var.get() == "search":
                sel = search_results_list.curselection()
                final_verse = search_results_list.get(sel[0]) if sel else ""
            else:
                sel = filter_results_list.curselection()
                final_verse = filter_results_list.get(sel[0]) if sel else ""
            if final_verse:
                self.finalized_verse = final_verse
                select_win.destroy()
                self.launch_edit_saved_literal_translation()
            else:
                tk.messagebox.showerror("Error", "Please select a verse before proceeding.")

        bottom_frame = tk.Frame(main_content_area, bg="light gray")
        bottom_frame.pack(side=tk.BOTTOM, pady=10)

        tk.Button(bottom_frame, text="Finalize Selection", font=("Arial", 12, "bold"),
                bg="green", fg="white", padx=15, pady=5,
                command=finalize_selection).pack(side=tk.LEFT, padx=(10, 5))

        tk.Button(bottom_frame, text="Back to Dashboard", font=("Arial", 12, "bold"),
                bg="gray", fg="white", padx=15, pady=5,
                command=lambda: (select_win.destroy(), self.setup_verse_analysis_dashboard())).pack(side=tk.LEFT, padx=(5, 10))

        # === Modal behavior ===
        select_win.transient(self.root)
        select_win.grab_set()
        self.root.wait_window(select_win)

    def launch_edit_saved_literal_translation(self):
        """Launch a window to review and select words for re-analysis from a saved verse."""

        verse = self.finalized_verse
        df = self.load_existing_assessment_data("1.2.1 assessment_data.xlsx")

        # === Clear root ===
        for widget in self.root.winfo_children():
            widget.destroy()

        # === Setup main frame ===
        main_frame = tk.Frame(self.root, bg="light gray", padx=20, pady=20)
        main_frame.pack(fill=tk.BOTH, expand=True)

        # === Header ===
        header = tk.Label(
            main_frame,
            text="Edit Saved Literal Translation",
            font=("Helvetica", 18, "bold"),
            bg="dark slate gray",
            fg="white",
            pady=10
        )
        header.pack(fill=tk.X, pady=(0, 20))

        # === Display Verse ===
        verse_label = tk.Label(
            main_frame,
            text=f"Verse:\n  {verse}",
            font=("Arial", 14),
            bg="light gray",
            anchor="w",
            justify="left"
        )
        verse_label.pack(fill=tk.X, padx=10)

        # === Display Translation ===
        row_data = df[df['Verse'] == verse].iloc[0]
        translation = row_data.get('Translation', '')
        translation_label = tk.Label(
            main_frame,
            text=f"Translation:\n  {translation}",
            font=("Arial", 13, "italic"),
            bg="light gray",
            anchor="w",
            justify="left"
        )
        translation_label.pack(fill=tk.X, padx=10, pady=(10, 5))

        # Returns the real value if it isn‚Äôt NaN; otherwise it returns a ‚Äú‚Äî‚Äù placeholder
        def safe(val):
            return val if pd.notna(val) else "‚Äî"

        # === Metadata (Raag, Writer, Bani, Page) ===
        metadata_frame = tk.Frame(main_frame, bg="light gray")
        metadata_frame.pack(fill=tk.X, padx=10, pady=(0, 15))

        for label, col in [
            ("Raag:", "Raag (Fixed)"),
            ("Writer:", "Writer (Fixed)"),
            ("Bani:", "Bani Name"),
            ("Page:", "Page Number"),
        ]:
            val = row_data.get(col)
            # only show non-missing values
            if val is None or pd.isna(val):
                continue

            meta = tk.Label(
                metadata_frame,
                text=f"{label} {val}",
                font=("Arial", 11, "bold"),
                bg="light gray",
                anchor="w",
                justify="left"
            )
            meta.pack(anchor="w")

        # === Editable Metadata for Entire Verse ===
        # Create a frame for editing verse-wide settings like "Framework?" and "Explicit?"
        edit_metadata_frame = tk.Frame(main_frame, bg="light gray")
        edit_metadata_frame.pack(fill=tk.X, padx=10, pady=(0, 15))

        # Helper: safely extract a boolean value (assuming Excel stores these as numbers 0/1 or booleans)
        def safe_bool(val):
            try:
                # If the value is a numpy integer, convert it to a normal int first
                if isinstance(val, np.integer):
                    val = int(val)
                # Now, if it's numeric, nonzero means True; otherwise, use its boolean conversion
                return bool(val) if isinstance(val, (int, float)) else val
            except Exception:
                return False

        # Get initial values from the Excel row data
        initial_framework = safe_bool(row_data.get("Framework?"))
        initial_explicit = safe_bool(row_data.get("Explicit?"))

        # Create checkboxes for Framework and Explicit metadata
        framework_var_edit = tk.BooleanVar(value=initial_framework)
        explicit_var_edit = tk.BooleanVar(value=initial_explicit)

        framework_cb_edit = tk.Checkbutton(
            edit_metadata_frame,
            text="Framework?",
            variable=framework_var_edit,
            font=("Arial", 11, "bold"),
            bg="light gray"
        )
        framework_cb_edit.pack(side=tk.LEFT, padx=10)

        explicit_cb_edit = tk.Checkbutton(
            edit_metadata_frame,
            text="Explicit?",
            variable=explicit_var_edit,
            font=("Arial", 11, "bold"),
            bg="light gray"
        )
        explicit_cb_edit.pack(side=tk.LEFT, padx=10)

        def update_verse_metadata(new_framework, new_explicit):
            file_path = "1.2.1 assessment_data.xlsx"
            try:
                # Load existing data
                df_existing = self.load_existing_assessment_data(file_path)
                # Create a mask for rows corresponding to the current verse
                verse_mask = df_existing["Verse"] == verse
                # Update the columns for the entire verse with the new values
                df_existing.loc[verse_mask, "Framework?"] = int(new_framework)
                df_existing.loc[verse_mask, "Explicit?"] = int(new_explicit)
                df_existing.to_excel(file_path, index=False)
                messagebox.showinfo("Updated", "Verse metadata updated successfully!")
            except Exception as e:
                messagebox.showerror("Error", f"Failed to update verse metadata: {e}")

        update_btn = tk.Button(
            edit_metadata_frame,
            text="Update Verse Metadata",
            font=("Arial", 11, "bold"),
            bg="dark cyan", fg="white",
            padx=10, pady=5,
            command=lambda: update_verse_metadata(framework_var_edit.get(), explicit_var_edit.get())
        )
        update_btn.pack(side=tk.LEFT, padx=10)

        # === Word Table Frame ===
        word_frame = tk.LabelFrame(
            main_frame,
            text="Select words to re-analyze:",
            bg="light gray",
            font=("Arial", 12, "bold")
        )
        word_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        # === Checkbox Simulation ===
        selected_items = set()

        columns = [
            'Select', 'Word', 'Vowel Ending', 'Number / ‡®µ‡®ö‡®®',
            'Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', 'Gender / ‡®≤‡®ø‡©∞‡®ó', 'Word Type',
            'Word Root', 'Word Index'
        ]

        tree = ttk.Treeview(word_frame, columns=columns, show='headings', selectmode='none')
        for col in columns:
            tree.heading(col, text=col)
            tree.column(col, anchor=tk.CENTER, width=110 if col != 'Select' else 60)

        # Add scrollbar
        vsb = ttk.Scrollbar(word_frame, orient="vertical", command=tree.yview)
        tree.configure(yscrollcommand=vsb.set)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)
        tree.pack(fill=tk.BOTH, expand=True)

        # Insert rows
        # 1) Configure tag styles for odd/even rows
        tree.tag_configure('oddrow', background='white')
        tree.tag_configure('evenrow', background='#E8E8E8')  # light gray

        # 2) Insert rows with alternating row tags
        rows = df[df['Verse'] == verse]
        for i, (_, row) in enumerate(rows.iterrows()):
            row_id = f"row{i}"
            # Your existing 'values' building
            values = [
                "",  # checkbox
                safe(self._norm_get(row, "Word")),
                safe(self._norm_get(row, "\ufeffVowel Ending")),
                safe(self._norm_get(row, "Number / ‡®µ‡®ö‡®®")),
                safe(self._norm_get(row, "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£")),
                safe(self._norm_get(row, "Gender / ‡®≤‡®ø‡©∞‡®ó")),
                safe(self._norm_get(row, "Word Root")),
                safe(self._norm_get(row, "Type")),
                int(self._norm_get(row, "Word Index") or -1)
            ]

            # Determine odd/even row coloring
            if i % 2 == 0:
                tree.insert('', tk.END, iid=row_id, values=values, tags=('evenrow',))
            else:
                tree.insert('', tk.END, iid=row_id, values=values, tags=('oddrow',))

        # === Toggle ‚úì in first column ===
        def on_tree_click(event):
            region = tree.identify_region(event.x, event.y)
            if region == 'cell':
                row_id = tree.identify_row(event.y)
                col = tree.identify_column(event.x)
                if row_id and col == '#1':
                    if row_id in selected_items:
                        selected_items.remove(row_id)
                        tree.set(row_id, 'Select', "")
                    else:
                        selected_items.add(row_id)
                        tree.set(row_id, 'Select', "‚úì")

        tree.bind('<Button-1>', on_tree_click)

        # === Action Buttons ===
        btn_frame = tk.Frame(main_frame, bg="light gray")
        btn_frame.pack(pady=20)

        def analyze_selected_words():
            if not hasattr(self, "results_text"):
                self.results_text = scrolledtext.ScrolledText(
                    self.root,
                    width=90,
                    height=20,
                    font=("Consolas", 11),
                    bd=3,
                    relief=tk.SUNKEN,
                    wrap=tk.WORD
                )
                self.results_text.pack_forget()  # Don‚Äôt show it to the user during re-analysis

            # Get column names to dynamically determine index of "Word" and "Word Index"
            column_names = tree["columns"]
            word_col_index = column_names.index("Word")
            index_col_index = column_names.index("Word Index")

            # Build the list of selected words with indices
            selected_words_with_indices = [
                (
                    tree.item(rid)['values'][word_col_index],
                    int(tree.item(rid)['values'][index_col_index])
                )
                for rid in selected_items
            ]

            all_words_in_verse = [tree.item(rid)['values'][1] for rid in tree.get_children()]

            if not selected_words_with_indices:
                messagebox.showwarning(
                    "No Words Selected",
                    "You haven‚Äôt selected any words for re-analysis.\n\n"
                    "Click the ‚úì box beside the word(s) you wish to re-analyze, then press the button again."
                )
                return

            # Step 1: Set context before any processing
            self.current_pankti = verse
            self.accumulated_pankti = verse
            self.pankti_words = all_words_in_verse  # Keep '‡••' if part of original flow
            self.selected_verses = [verse]
            self.accumulated_meanings = [{} for _ in self.pankti_words]
            self.accumulated_finalized_matches = [[] for _ in self.pankti_words]
            self.all_new_entries = []
            self.current_reanalysis_index = []

            # Step 2: Load Excel and pre-fill all words from verse
            df = self.load_existing_assessment_data("1.2.1 assessment_data.xlsx")
            verse_rows = df[df["Verse"] == verse]

            for i, word in enumerate(self.pankti_words):
                word_rows = verse_rows[
                    (verse_rows["Word"] == word) &
                    (verse_rows["Verse"] == verse) &
                    (verse_rows["Word Index"] == i)
                ]
                if not word_rows.empty:
                    # Meanings
                    meanings = [
                        row.get("Selected Darpan Meaning", "")
                        for _, row in word_rows.iterrows()
                        if pd.notna(row.get("Selected Darpan Meaning"))
                    ]
                    self.accumulated_meanings[i] = {"word": word, "meanings": meanings}

                    # Grammar matches
                    finalized = word_rows.to_dict("records")
                    self.accumulated_finalized_matches[i] = finalized
                    self.all_new_entries.extend(finalized)

            # Step 3: Determine which indices to re-analyze
            selected_words_with_indices.sort(key=lambda x: x[1])  # Sort by Word Index (ascending)
            word_indices = [idx for (_, idx) in selected_words_with_indices]
            if not word_indices:
                messagebox.showinfo("Not Found", "Selected words not found in verse structure.")
                return

            # Populate past details (grammar and selected meanings) for each word in the verse.
            self.past_word_details = {}

            # Iterate over each tuple (word, idx) in selected_words_with_indices.
            for word, idx in selected_words_with_indices:
                # Filter the DataFrame for rows matching the word, verse, and unique word index.
                word_rows = verse_rows[
                    (verse_rows["Word"] == word) &
                    (verse_rows["Verse"] == verse) &
                    (verse_rows["Word Index"] == idx)
                ]
                
                if not word_rows.empty:
                    # Choose the representative row that has the highest Grammar Revision.
                    latest_idx = word_rows["Grammar Revision"].idxmax()
                    latest_row = word_rows.loc[latest_idx]
                    
                    # Gather all available past Darpan meanings from the filtered rows.
                    darpan_meanings = []
                    for _, row in word_rows.iterrows():
                        val = row.get("Selected Darpan Meaning")
                        if pd.notna(val):
                            # Split on comma and strip whitespace
                            split_meanings = [m.strip() for m in val.split("| ")]
                            darpan_meanings.extend(split_meanings)

                    # Store the past details in the dictionary using the word index as the key.
                    self.past_word_details[idx] = {
                        "Word": word,
                        "\ufeffVowel Ending": self._norm_get(latest_row, "\ufeffVowel Ending") or "",
                        "Number / ‡®µ‡®ö‡®®": self._norm_get(latest_row, "Number / ‡®µ‡®ö‡®®") or "",
                        "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£": self._norm_get(latest_row, "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£") or "",
                        "Gender / ‡®≤‡®ø‡©∞‡®ó": self._norm_get(latest_row, "Gender / ‡®≤‡®ø‡©∞‡®ó") or "",
                        "Type": self._norm_get(latest_row, "Type") or "",
                        "Word Root": self._norm_get(latest_row, "Word Root") or "",
                        "Word Index": idx,
                        "darpan_meanings": darpan_meanings
                    }

            # Step 4: Store queue and start
            self.reanalysis_queue = word_indices
            self.process_next_selected_word()

        def back_to_search():
            self.launch_select_verse()

        def back_to_dashboard():
            self.setup_verse_analysis_dashboard()

        tk.Button(
            btn_frame,
            text="Analyze Selected Words",
            bg="navy", fg="white",
            font=("Arial", 12, "bold"),
            padx=15, pady=5,
            command=analyze_selected_words
        ).pack(side=tk.LEFT, padx=10)

        tk.Button(
            btn_frame,
            text="Back to Search",
            bg="gray", fg="white",
            font=("Arial", 12, "bold"),
            padx=15, pady=5,
            command=back_to_search
        ).pack(side=tk.LEFT, padx=10)

        tk.Button(
            btn_frame,
            text="Back to Dashboard",
            bg="red", fg="white",
            font=("Arial", 12, "bold"),
            padx=15, pady=5,
            command=back_to_dashboard
        ).pack(side=tk.LEFT, padx=10)

        # === Optional: Customize style ===
        style = ttk.Style()
        style.configure("Treeview.Heading", font=('Arial', 11, 'bold'))
        style.configure("Treeview", rowheight=28, font=('Arial', 11))

    def process_next_selected_word(self):
        """Process the next word from re-analysis queue, or prompt to save if finished."""
        if not hasattr(self, 'reanalysis_queue') or not self.reanalysis_queue:
            messagebox.showinfo("Done", "Re-analysis completed for all selected words.")
            
            # Proceed to save reanalyzed results
            if hasattr(self, 'save_results_btn') and self.save_results_btn.winfo_exists():
                self.save_results_btn.config(state=tk.NORMAL)
            
            self.prompt_save_results_reanalysis(self.all_new_entries, skip_copy=False)  # Skip clipboard step for reanalysis
            return

        # Process the next word from queue
        idx = self.reanalysis_queue.pop(0)
        self.current_word_index = idx
        word = self.pankti_words[idx]
        self.ensure_meanings_slot_initialized(idx, word)

        self.fetch_data_for_reanalysis(word, self.accumulated_pankti, idx)

    def fetch_data_for_reanalysis(self, word, pankti, index):
        self.reset_input_variables()
        self.current_word_index = index
        self.user_input_reanalysis(word, pankti, index)

        if hasattr(self, 'input_window') and self.input_window.winfo_exists():
            self.root.wait_window(self.input_window)
        else:
            return

        if self.input_submitted:
            self.handle_submitted_input(word)
        else:
            print(f"Skipped: {word}")

        # Now move to next in reanalysis queue
        self.process_next_selected_word()

    def user_input_reanalysis(self, word, pankti, index):
        print(f"[Reanalysis] Opening input window for {word} (index {index})")
        self.input_submitted = False
        self.current_word_index = index  # Ensure correct word gets highlighted

        self.input_window = tk.Toplevel(self.root)
        self.input_window.title(f"[Edit Mode] Input for {word}")
        self.input_window.configure(bg='light gray')
        self.input_window.state('zoomed')
        self.input_window.resizable(True, True)

        # Display the Pankti with word highlight
        pankti_frame = tk.Frame(self.input_window, bg='light gray')
        pankti_frame.pack(fill=tk.X, padx=20, pady=10)

        pankti_display = tk.Text(
            pankti_frame, wrap=tk.WORD, bg='light gray', font=('Arial', 32),
            height=2, padx=5, pady=5
        )
        pankti_display.pack(fill=tk.X, expand=False)
        pankti_display.insert(tk.END, pankti)
        pankti_display.tag_add("center", "1.0", "end")
        pankti_display.tag_configure("center", justify='center')

        # Highlight the word at the re-analysis index
        words = pankti.split()
        start_idx = 0
        for i, w in enumerate(words):
            if i == index:
                end_idx = start_idx + len(w)
                pankti_display.tag_add("highlight", f"1.{start_idx}", f"1.{end_idx}")
                pankti_display.tag_config("highlight", foreground="blue", font=('Arial', 32, 'bold'))
                break
            start_idx += len(w) + 1
        pankti_display.config(state=tk.DISABLED)

        # Create layout pane
        split_pane = tk.PanedWindow(self.input_window, orient=tk.HORIZONTAL, bg='light gray')
        split_pane.pack(fill=tk.BOTH, expand=True)

        # Left: Meanings
        self.left_pane = tk.Frame(split_pane, bg='light gray')
        split_pane.add(self.left_pane, stretch="always")
        tk.Label(self.left_pane, text=f"Re-analyze Meanings for {word}:", bg='light gray',
                font=('Arial', 14, 'bold')).pack(anchor='center', pady=(0, 10))
        self.meanings_scrollbar = tk.Scrollbar(self.left_pane, orient=tk.VERTICAL)
        self.meanings_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.meanings_canvas = tk.Canvas(self.left_pane, bg='light gray', borderwidth=0,
                                        yscrollcommand=self.meanings_scrollbar.set)
        self.meanings_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        self.meanings_scrollbar.config(command=self.meanings_canvas.yview)
        self.meanings_inner_frame = tk.Frame(self.meanings_canvas, bg='light gray')
        self.meanings_canvas.create_window((0, 0), window=self.meanings_inner_frame, anchor='nw')

        # Right: Grammar Options
        right_pane = tk.Frame(split_pane, bg='light gray')
        split_pane.add(right_pane, stretch="always")
        tk.Label(right_pane, text="Adjust Grammar Options:", bg='light gray',
                font=('Arial', 14, 'bold')).pack(pady=10)
        self.setup_options(
            right_pane,
            "Do you know the Number of the word?",
            [("Singular", "Singular / ‡®á‡®ï"), ("Plural", "Plural / ‡®¨‡®π‡©Å"), ("Not Applicable", "NA")],
            self.number_var
        )
        self.setup_options(
            right_pane,
            "Do you know the Gender of the word?",
            [("Masculine", "Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó"), ("Feminine", "Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä"), ("Neutral", "Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï")],
            self.gender_var
        )
        self.setup_options(
            right_pane,
            "Do you know the Part of Speech for the word?",
            [("Noun", "Noun / ‡®®‡®æ‡®Ç‡®µ"), ("Adjective", "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£"),
            ("Adverb", "Adverb / ‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡©á‡®∂‡®£"), ("Verb", "Verb / ‡®ï‡®ø‡®∞‡®ø‡®Ü"),
            ("Pronoun", "Pronoun / ‡®™‡©ú‡®®‡®æ‡®Ç‡®µ"), ("Postposition", "Postposition / ‡®∏‡©∞‡®¨‡©∞‡®ß‡®ï"),
            ("Conjunction", "Conjunction / ‡®Ø‡©ã‡®ú‡®ï"), ("Interjection", "Interjection / ‡®µ‡®ø‡®∏‡®Æ‡®ø‡®ï")],
            self.pos_var
        )

        # Submit / Skip
        button_frame = tk.Frame(self.input_window, bg='light gray')
        button_frame.pack(pady=20)
        tk.Button(button_frame, text="Submit", command=self.submit_input_reanalysis,
                font=('Arial', 12, 'bold'), bg='navy', fg='white',
                padx=30, pady=15).pack(side=tk.LEFT, padx=15)
        tk.Button(button_frame, text="Skip", command=self.skip_input,
                font=('Arial', 12, 'bold'), bg='navy', fg='white',
                padx=30, pady=15).pack(side=tk.LEFT, padx=15)

        # Progress + Dictionary Lookup
        self.start_progress()
        threading.Thread(target=self.lookup_meanings_thread, args=(word,), daemon=True).start()

        self.input_window.transient(self.root)
        self.input_window.grab_set()
        self.root.wait_window(self.input_window)
        print(f"[Reanalysis] Closed input window for {word}")

    def submit_input_reanalysis(self):
        self.input_submitted = True
        if hasattr(self, 'input_window') and self.input_window.winfo_exists():
            self.input_window.destroy()

        # Start the progress bar
        self.start_progress()

        # Run the reanalysis search in a separate thread
        search_thread = threading.Thread(target=self.perform_search_and_finish_reanalysis)
        search_thread.start()

    def perform_search_and_finish_reanalysis(self):
        current_word = self.pankti_words[self.current_word_index]
        number = self.number_var.get()
        gender = self.gender_var.get()
        pos = self.pos_var.get()

        print(f"[Reanalysis] Processing word: {current_word}, Number: {number}, Gender: {gender}, POS: {pos}")
        
        if number == "NA" and gender == "NA" and pos == "NA":
            matches = self.search_by_inflections(current_word)
        else:
            matches = self.search_by_criteria(current_word, number, gender, pos)
            if not matches:
                messagebox.showinfo(
                    "No Matches Found",
                    "No matches were found as per the criteria. Now conducting a general search."
                )
                matches = self.search_by_inflections(current_word)

        # Meanings are guaranteed to be preloaded from the Excel into self.accumulated_meanings
        entry = self.accumulated_meanings[self.current_word_index]
        meanings = entry.get("meanings", []) if isinstance(entry, dict) else entry

        # Stop progress bar first
        self.root.after(0, self.stop_progress)

        if matches:
            print(f"[Reanalysis] Found matches for {current_word}: {matches}")
            self.root.after(0, lambda: self.show_matches_reanalysis(matches, self.current_pankti, meanings, self.current_word_index))
        else:
            self.root.after(0, lambda: messagebox.showinfo("No Matches", f"No matches found for the word: {current_word}"))
            self.current_word_index += 1
            self.root.after(0, self.process_next_selected_word)

    def show_matches_reanalysis(self, matches, pankti, meanings, index, max_display=30):
        # Destroy any existing match window
        if hasattr(self, 'match_window') and self.match_window.winfo_exists():
            self.match_window.destroy()

        self.match_window = tk.Toplevel(self.root)
        self.match_window.title("Re-analysis: Select Matches and Meanings")
        self.match_window.configure(bg='light gray')
        self.match_window.state('zoomed')

        self.match_vars = []
        self.meaning_vars = []
        unique_matches = self.filter_unique_matches(matches)
        self.all_matches.append(unique_matches)

        self.current_reanalysis_index.append(index)
        
        # Display Pankti
        self.display_pankti_with_highlight(self.match_window, pankti, index)

        # --- Explanation Section ---
        explanation_frame = tk.Frame(self.match_window, bg='AntiqueWhite', 
                                    relief='groove', bd=2)  # A tinted frame with a grooved border
        explanation_frame.pack(fill=tk.X, padx=20, pady=(5, 10))

        heading_label = tk.Label(
            explanation_frame, 
            text="Important Note", 
            font=("Arial", 14, 'bold'),
            bg='AntiqueWhite'
        )
        heading_label.pack(pady=(5, 0))

        explanation_text = (
            "‚Ä¢ Highlighted selections (displayed in MistyRose) indicate the meanings or grammar rules that "
            "were previously confirmed in your assessment.\n"
            "‚Ä¢ This helps you quickly recognize which items reflect your earlier choices."
        )

        body_label = tk.Label(
            explanation_frame, 
            text=explanation_text,
            bg='AntiqueWhite', 
            fg='black', 
            font=('Arial', 12),
            wraplength=900,    # Adjust wrap length to your window‚Äôs width
            justify=tk.LEFT
        )
        body_label.pack(pady=(0, 10), padx=10)

        # Main layout
        main_frame = tk.Frame(self.match_window, bg='light gray')
        main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)

        # --- Left Pane: Meanings ---
        word = self.pankti_words[index]
        self.display_meanings_section_reanalysis(main_frame, word, index, meanings)

        # --- Right Pane: Matches ---
        self.display_matches_section_reanalysis(main_frame, unique_matches, index, max_display)

        # --- Bottom Buttons ---
        button_frame = tk.Frame(self.match_window, bg='light gray')
        button_frame.pack(pady=10)

        tk.Button(
            button_frame,
            text="Submit",
            command=self.submit_matches_reanalysis,
            font=('Arial', 12, 'bold'), bg='navy', fg='white',
            padx=20, pady=10
        ).pack(side=tk.LEFT, padx=5)

        tk.Button(
            button_frame,
            text="Back",
            command=lambda: self.back_to_user_input_reanalysis(pankti, index),
            font=('Arial', 12, 'bold'), bg='navy', fg='white',
            padx=20, pady=10
        ).pack(side=tk.LEFT, padx=5)

    def display_pankti_with_highlight(self, parent, pankti, index):
        """
        Displays the full pankti and highlights the word at the given index in a Text widget.
        """
        pankti_frame = tk.Frame(parent, bg='light gray')
        pankti_frame.pack(fill=tk.BOTH, padx=30, pady=20)

        display = tk.Text(
            pankti_frame,
            wrap=tk.WORD,
            bg='light gray',
            font=('Arial', 32),
            height=1,
            padx=5,
            pady=5
        )
        display.pack(fill=tk.BOTH, expand=False)
        display.insert(tk.END, pankti)
        display.tag_add("center", "1.0", "end")
        display.tag_configure("center", justify='center')

        words = pankti.split()
        start_idx = 0
        for i, w in enumerate(words):
            if i == index:
                break
            start_idx += len(w) + 1
        end_idx = start_idx + len(words[index])

        display.tag_add("highlight", f"1.{start_idx}", f"1.{end_idx}")
        display.tag_config("highlight", foreground="blue", font=('Arial', 32, 'bold'))
        display.config(state=tk.DISABLED)

    def display_meanings_section_reanalysis(self, parent_frame, word, index, meanings):
        """Display meanings as checkboxes for reanalysis with prior selection support."""
        meanings_frame = tk.Frame(parent_frame, bg='light gray')
        meanings_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=10)

        tk.Label(meanings_frame, text=f"Select Meanings for {word}:", bg='light gray',
                font=('Arial', 14, 'bold')).pack(pady=10)

        self.select_all_meanings_var = tk.BooleanVar(value=True)
        tk.Checkbutton(meanings_frame, text="Select/Deselect All Meanings",
                    variable=self.select_all_meanings_var, bg='light gray',
                    font=('Arial', 12), command=self.toggle_all_meanings).pack(pady=5)

        meanings_canvas = tk.Canvas(meanings_frame, bg='light gray', borderwidth=0)
        meanings_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar = tk.Scrollbar(meanings_frame, orient=tk.VERTICAL, command=meanings_canvas.yview)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        meanings_canvas.config(yscrollcommand=scrollbar.set)

        inner_frame = tk.Frame(meanings_canvas, bg='light gray')
        meanings_canvas.create_window((0, 0), window=inner_frame, anchor='nw')

        # Merge past meanings
        first_index = next((i for i, w in enumerate(self.pankti_words) if w == word), index)
        merged_meanings = []
        for idx in range(first_index, index):
            if idx < len(self.accumulated_meanings):
                entry = self.accumulated_meanings[idx]
                if isinstance(entry, dict):
                    merged_meanings.extend(entry.get("meanings", []))
        prior_meanings = list(dict.fromkeys(merged_meanings))

        # Extract assessment-specific selected meanings from past_word_details.
        # These are the meanings the user had previously selected (from assessment).
        assessment_meanings = self.past_word_details.get(index, {}).get("darpan_meanings", [])

        # Reorder current meanings
        if isinstance(meanings, dict):
            current_meanings = meanings.get("meanings", [])
        else:
            current_meanings = meanings
            
        # Tier 1: assessment_meanings at very top
        reordered_assessment = [m for m in current_meanings if m in assessment_meanings]

        # Tier 2: prior_meanings next
        reordered_prior = [m for m in current_meanings 
                        if (m in prior_meanings and m not in assessment_meanings)]

        # Tier 3: everything else
        reordered_others = [m for m in current_meanings
                            if m not in prior_meanings and m not in assessment_meanings]

        reordered = reordered_assessment + reordered_prior + reordered_others

        split = self.split_meanings_for_display(reordered)
        self.meaning_vars = []

        for i, column in enumerate(split.values()):
            col_frame = tk.Frame(inner_frame, bg='light gray')
            col_frame.grid(row=0, column=i, padx=10, pady=10, sticky='nw')
            for meaning in column:
                # Determine whether this meaning was previously chosen during
                # the earlier assessment.  Those meanings should stand out in
                # MistyRose so that the user can easily recognise them when
                # re‚Äëanalysing a word (mirroring the behaviour of grammar
                # rule highlighting).
                highlight = (meaning in assessment_meanings)

                # Default selection ‚Äì for re‚Äëanalysis we only pre‚Äëselect a
                # meaning if it was explicitly chosen earlier.  Previously the
                # first occurrence of a word had every meaning pre‚Äëselected
                # which made it difficult to spot the assessed choice.  By
                # limiting the pre‚Äëselection to the highlighted meanings we
                # keep the focus on what was actually picked before.
                if index != first_index:
                    preselect = (meaning in prior_meanings) or highlight
                else:
                    preselect = highlight

                # Apply the MistyRose background when the meaning was part of
                # the previous assessment; otherwise fall back to light gray.
                bg_color = "MistyRose" if highlight else "light gray"

                var = tk.BooleanVar(value=preselect)
                chk = tk.Checkbutton(
                    col_frame,
                    text=f"- {meaning}",
                    variable=var,
                    bg=bg_color,
                    font=('Arial', 12),
                    wraplength=325,
                    anchor='w',
                    justify=tk.LEFT,
                    selectcolor='light blue',
                )
                chk.pack(anchor='w', padx=15, pady=5)
                self.meaning_vars.append((var, meaning))

        inner_frame.update_idletasks()
        meanings_canvas.config(scrollregion=meanings_canvas.bbox("all"))

    def display_matches_section_reanalysis(self, parent_frame, unique_matches, index, max_display=30):
        """Display matching rule checkboxes in the reanalysis pane."""
        matches_frame = tk.Frame(parent_frame, bg='light gray')
        matches_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=10)

        tk.Label(matches_frame, text="Select the matching rules:",
                bg='light gray', font=('Arial', 14, 'bold')).pack(pady=10)

        canvas = tk.Canvas(matches_frame, bg='light gray', borderwidth=0)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar = tk.Scrollbar(matches_frame, orient=tk.VERTICAL, command=canvas.yview)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.config(yscrollcommand=scrollbar.set)

        inner_frame = tk.Frame(canvas, bg='light gray')
        canvas.create_window((0, 0), window=inner_frame, anchor='nw')

        self.match_vars = []  # Reset match_vars in reanalysis flow

        # Retrieve the prior assessment grammar details for this word occurrence.
        grammar_assessment = self.past_word_details.get(index, {})
        assessment_fields = self.extract_grammar_fields(grammar_assessment)
        
        # For each match, we assume a tuple (label, value). In some cases,
        # label might be a composite string (e.g. with fields joined by " | ").
        # We reorder or highlight based on whether the extracted values match.
        reordered_matches = unique_matches[:max_display]  # (Assume prior reordering if needed)

        for match in reordered_matches:
            field_label, match_value = match[0], match[1]

            # If the field label is composite, parse it.
            if " | " in field_label:
                parsed = self.parse_composite(field_label)
                # Check for each target field whether the parsed value matches the assessment.
                highlight = True
                for key, expected in assessment_fields.items():
                    if key in parsed:
                        if not self.safe_equal_matches_reanalysis(parsed[key], expected):
                            highlight = False
                            break
                bg_color = "MistyRose" if highlight else "light gray"
            else:
                # Otherwise, if the label is a single field name, use a simple check.
                if field_label in assessment_fields and assessment_fields[field_label] == match_value:
                    bg_color = "MistyRose"
                else:
                    bg_color = "light gray"

            var = tk.BooleanVar()
            chk = tk.Checkbutton(inner_frame,
                                text=f"{field_label}: {match_value}",
                                variable=var,
                                bg=bg_color,
                                font=('Arial', 12),
                                selectcolor='light blue',
                                anchor='w')
            chk.pack(fill=tk.X, padx=10, pady=5)
            self.match_vars.append((var, match))

        inner_frame.update_idletasks()
        canvas.config(scrollregion=canvas.bbox("all"))

    def safe_equal_matches_reanalysis(self, val1, val2):
        def normalize(v):
            # Convert real NaN to empty
            if pd.isna(v):
                return ""
            # Turn into string, strip whitespace, and treat "NA" (any case) as empty
            s = str(v).strip()
            return "" if s.upper() == "NA" else s
        return normalize(val1) == normalize(val2)

    def extract_grammar_fields(self, grammar_assessment):
        """
        Extract only the fields we wish to highlight from grammar_assessment.
        Returns a dict with keys:
        - "Vowel Ending"
        - "Number / ‡®µ‡®ö‡®®"
        - "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£"
        - "Gender / ‡®≤‡®ø‡©∞‡®ó"
        - "Word Root"
        - "Word Type"
        """
        target_keys = ["\ufeffVowel Ending", "Number / ‡®µ‡®ö‡®®", "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",
                    "Gender / ‡®≤‡®ø‡©∞‡®ó", "Word Root", "Type"]
        return {key: grammar_assessment.get(key) for key in target_keys}

    def parse_composite(self, label):
        """
        Assume a composite label is built by joining fields with " | ".
        This function splits the composite string into its individual parts
        and returns a dictionary mapping (in order) the following keys:
        "Word", "Vowel Ending", "Number / ‡®µ‡®ö‡®®", "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",
        "Gender / ‡®≤‡®ø‡©∞‡®ó", "Word Root", "Type"
        """
        parts = label.split(" | ")
        keys = ["Word", "\ufeffVowel Ending", "Number / ‡®µ‡®ö‡®®", "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",
                "Gender / ‡®≤‡®ø‡©∞‡®ó", "Word Root", "Type"]
        return dict(zip(keys, parts))

    def back_to_user_input_reanalysis(self, pankti, index):
        """
        Returns to the grammar input screen for the current word during re-analysis.
        """
        try:
            if hasattr(self, 'match_window') and self.match_window:
                self.match_window.destroy()

            if 0 <= index < len(self.pankti_words):
                word = self.pankti_words[index]
                self.current_word_index = index
                self.reset_input_variables()
                self.user_input_reanalysis(word, pankti, index)
            else:
                messagebox.showerror("Invalid Index", "Cannot return to word ‚Äî index out of range.")

        except Exception as e:
            messagebox.showerror("Error", f"An error occurred while going back: {e}")

    def submit_matches_reanalysis(self):
        any_selection = False
        current_entries = []

        for var, match in self.match_vars:
            if var.get():
                match_string = match[0]
                data = match_string.split(" | ")
                new_entry = {
                    "Word": data[0],
                    "\ufeffVowel Ending": data[1],
                    "Number / ‡®µ‡®ö‡®®": data[2],
                    "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£": data[3],
                    "Gender / ‡®≤‡®ø‡©∞‡®ó": data[4],
                    "Word Root": data[5],
                    "Type": data[6]
                }
                current_entries.append(new_entry)
                any_selection = True

        selected_meanings = [meaning for var, meaning in self.meaning_vars if var.get()]
        self.accumulate_meanings_data(selected_meanings)

        current_word = self.pankti_words[self.current_word_index]
        first_index = next((i for i, w in enumerate(self.pankti_words) if w == current_word), self.current_word_index)

        current_selected = selected_meanings
        if self.current_word_index != first_index:
            merged_meanings = []
            for idx in range(first_index, self.current_word_index):
                if idx < len(self.accumulated_meanings) and self.pankti_words[idx] == current_word:
                    entry = self.accumulated_meanings[idx]
                    if isinstance(entry, dict):
                        merged_meanings.extend(entry.get("meanings", []))
                    else:
                        merged_meanings.extend(entry)
            prior_meanings = list(dict.fromkeys(merged_meanings))

            if set(current_selected) != set(prior_meanings):
                update_prev = messagebox.askyesno(
                    "Update Previous Meanings",
                    f"You have selected different meanings for the word '{current_word}'.\n"
                    "Do you want to update the meanings for all previous occurrences of this word?"
                )
                if update_prev:
                    for idx in range(first_index, self.current_word_index):
                        if idx < len(self.accumulated_meanings) and self.pankti_words[idx] == current_word:
                            self.accumulated_meanings[idx] = {"word": current_word, "meanings": current_selected}

        if self.current_word_index < len(self.accumulated_meanings):
            self.accumulated_meanings[self.current_word_index] = {"word": current_word, "meanings": current_selected}
        else:
            self.accumulated_meanings.append({"word": current_word, "meanings": current_selected})

        # Assign the current verse using index-to-verse mapping
        verse_boundaries = []
        pointer = 0
        for verse in self.selected_verses:
            verse_words = verse.split()
            start = pointer
            end = pointer + len(verse_words)
            verse_boundaries.append((start, end))
            pointer = end

        current_verse = None
        for i, (start, end) in enumerate(verse_boundaries):
            if start <= self.current_word_index < end:
                current_verse = self.selected_verses[i]
                break

        for entry in current_entries:
            entry["Verse"] = current_verse
            entry["Word Index"] = self.current_word_index

        finalized_matches = []
        for var, match in self.match_vars:
            if var.get():
                match_word = match[0].split(" | ")[0]
                for entry in current_entries:
                    if entry["Word"] == match_word and entry not in finalized_matches:
                        finalized_matches.append(entry)
        self.accumulate_finalized_matches(finalized_matches)

        if not any_selection:
            messagebox.showwarning("No Selection", "No matches were selected. Please select at least one match.")
        else:
            self.match_window.destroy()
            self.all_new_entries.extend(current_entries)
            self.process_next_selected_word()

    def prompt_save_results_reanalysis(self, new_entries, skip_copy=False):
        file_path = "1.2.1 assessment_data.xlsx"
        existing_data = self.load_existing_assessment_data(file_path)
        original_accumulated_pankti = self.accumulated_pankti

        for verse in self.selected_verses:
            self.accumulated_pankti = verse
            current_verse_words = verse.replace('‡••', '').split()
            selected_words = set(current_verse_words)

            # Filter grammar entries specific to this verse
            # now you can pick only the entries for that exact word‚Äêindex
            filtered_new_entries = [
                entry for entry in new_entries
                if entry.get("Verse", "").strip() == verse.strip()
                and entry.get("Word Index") in self.current_reanalysis_index
            ]

            # Silently remove exact duplicates based on your key fields
            seen = set()
            unique_entries = []

            keys = [
                "Word", "\ufeffVowel Ending", "Number / ‡®µ‡®ö‡®®", "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",
                "Gender / ‡®≤‡®ø‡©∞‡®ó", "Word Root", "Type", "Verse", 'Word Index'
            ]

            for entry in filtered_new_entries:
                # build a normalized tuple of comparison values
                key = tuple(self.normalize_save_results_reanalysis(entry.get(k)) for k in keys)
                if key not in seen:
                    seen.add(key)
                    unique_entries.append(entry)

            if not skip_copy:
                self.prompt_copy_to_clipboard_reanalysis()

            if unique_entries:
                save = messagebox.askyesno("Save Results", f"Would you like to save the new entries for the following verse?\n\n{verse}")
                if save:
                    assessment_data = self.prompt_for_assessment_once_reanalysis()

                    # Extract verse metadata from chosen_match if available
                    verse_to_match = self.accumulated_pankti.strip()
                    candidate = None
                    if hasattr(self, 'candidate_matches') and self.chosen_match:
                        for cand in self.chosen_match:
                            if cand.get("Verse", "").strip() == verse_to_match:
                                candidate = cand
                                break
                        if candidate is None:
                            candidate = self.chosen_match[0]

                        verse_metadata = {
                            "Verse": verse_to_match,
                            "S. No.": candidate.get("S. No.", ""),
                            "Verse No.": candidate.get("Verse No.", ""),
                            "Stanza No.": candidate.get("Stanza No.", ""),
                            "Text Set No.": candidate.get("Text Set No.", ""),
                            "Raag (Fixed)": candidate.get("Raag (Fixed)", ""),
                            "Sub-Raag": candidate.get("Sub-Raag", ""),
                            "Writer (Fixed)": candidate.get("Writer (Fixed)", ""),
                            "Verse Configuration (Optional)": candidate.get("Verse Configuration (Optional)", ""),
                            "Stanza Configuration (Optional)": candidate.get("Stanza Configuration (Optional)", ""),
                            "Bani Name": candidate.get("Bani Name", ""),
                            "Musical Note Configuration": candidate.get("Musical Note Configuration", ""),
                            "Special Type Demonstrator": candidate.get("Special Type Demonstrator", ""),
                            "Verse Type": candidate.get("Verse Type", ""),
                            "Page Number": candidate.get("Page Number", "")
                        }
                    else:
                        verse_metadata = {}

                    # Group grammar entries by word and partition by occurrence index
                    word_groups = {}
                    for entry in unique_entries:
                        word = entry["Word"]
                        word_groups.setdefault(word, []).append(entry)

                    final_entries = []
                    occurrence_mapping = {}
                    for word in set(current_verse_words):
                        count = current_verse_words.count(word)
                        if word not in word_groups:
                            continue
                        entries_list = word_groups[word]
                        n = len(entries_list)
                        k = count
                        group_size = n // k
                        remainder = n % k
                        start = 0
                        groups = []
                        for i in range(k):
                            size = group_size + (1 if i < remainder else 0)
                            group = entries_list[start:start+size]
                            groups.append(group)
                            start += size

                        occurrence_positions = [i for i, w in enumerate(current_verse_words) if w == word]
                        for occ, pos in zip(range(k), occurrence_positions):
                            occurrence_mapping[(word, pos)] = groups[occ]

                    for idx, word in enumerate(current_verse_words):
                        key = (word, idx)
                        entries = occurrence_mapping.get(key, [])
                        if not entries:
                            continue

                        seen = set()
                        dedup_entries = []
                        for entry in entries:
                            entry_tuple = tuple(sorted(entry.items()))
                            if entry_tuple not in seen:
                                seen.add(entry_tuple)
                                dedup_entries.append(entry)
                        entries = dedup_entries

                        if len(entries) > 1:
                            chosen_entry = self.prompt_for_final_grammar_reanalysis(entries)
                        else:
                            chosen_entry = entries[0]

                        chosen_entry['Word Index'] = idx

                        if len(self.accumulated_finalized_matches) <= idx:
                            self.accumulated_finalized_matches.extend([[]] * (idx - len(self.accumulated_finalized_matches) + 1))
                        self.accumulated_finalized_matches[idx] = [chosen_entry]
                        final_entries.append(chosen_entry)

                    for entry in final_entries:
                        entry.update(assessment_data)
                        entry.update(verse_metadata)
                        self.save_assessment_data_reanalysis(entry)

                    messagebox.showinfo("Saved", "Assessment data saved successfully for verse:\n" + verse)

        self.accumulated_pankti = original_accumulated_pankti

        if hasattr(self, 'copy_button') and self.copy_button.winfo_exists():
            self.copy_button.config(state=tk.NORMAL)

    def normalize_save_results_reanalysis(self, v):
        # Convert real NaN ‚Üí ""
        if pd.isna(v):
            return ""
        # Convert None ‚Üí ""
        if v is None:
            return ""
        # Convert the literal string "NA" (any case, with whitespace) ‚Üí ""
        s = str(v).strip()
        return "" if s.upper() == "NA" else s

    def safe_equal_save_results_reanalysis(self, val1, val2):
        return self.normalize_save_results_reanalysis(val1) == self.normalize_save_results_reanalysis(val2)

    def prompt_copy_to_clipboard_reanalysis(self):
        print("Prompting to copy re-analysis to clipboard...")

        copy_prompt = messagebox.askyesno(
            "Copy to Clipboard",
            f"Would you like to copy the re-analysis for the verse '{self.accumulated_pankti}' to your clipboard?"
        )

        if not copy_prompt:
            return

        # Validation checks
        if not self.accumulated_pankti:
            messagebox.showerror("Error", "No verse (pankti) available to copy.")
            return

        if not self.accumulated_meanings or not self.accumulated_finalized_matches or not self.all_new_entries:
            messagebox.showerror("Error", "Missing re-analysis data to copy.")
            print("Error: Data is incomplete for clipboard copy.")
            return

        try:
            # Use the existing composing utility
            clipboard_text = self.compose_clipboard_text_for_chatgpt_reanalysis()
            pyperclip.copy(clipboard_text)
            messagebox.showinfo("Copied", "The re-analysis has been copied to the clipboard!")
            print("Clipboard content copied successfully.")

        except Exception as e:
            print(f"Unexpected error while copying reanalysis: {e}")
            messagebox.showerror("Error", f"Unexpected error occurred: {e}")

    def compose_clipboard_text_for_chatgpt_reanalysis(self):
        clipboard_text = "### Detailed Reanalysis & Literal Translation\n\n"
        clipboard_text += (
            f"The verse **'{self.accumulated_pankti}'** has undergone re-analysis. "
            "Below is a breakdown of each word with revised user-selected meanings and grammar details.\n\n"
        )

        # --- Preceding Verses & Translations ---
        existing_data = self.load_existing_assessment_data("1.2.1 assessment_data.xlsx")
        selected_columns = [
            'S. No.', 'Verse', 'Verse No.', 'Stanza No.', 'Text Set No.',
            'Raag (Fixed)', 'Sub-Raag', 'Writer (Fixed)', 'Verse Configuration (Optional)',
            'Stanza Configuration (Optional)', 'Bani Name', 'Musical Note Configuration',
            'Special Type Demonstrator', 'Verse Type', 'Page Number'
        ]
        df_filtered = existing_data[selected_columns]
        chosen_list = df_filtered[
            df_filtered["Verse"].str.strip() == self.accumulated_pankti.strip()
        ].drop_duplicates().to_dict(orient="records")
        current_candidate = chosen_list[0] if chosen_list else None

        preceding_verses_text = ""
        if current_candidate:
            text_set_no = current_candidate.get("Text Set No.")
            try:
                current_verse_no = int(current_candidate.get("Verse No."))
            except (ValueError, TypeError):
                current_verse_no = None

            if current_verse_no is not None:
                filtered_data = existing_data[existing_data["Text Set No."] == text_set_no]
                consecutive_verses = []
                target_verse_no = current_verse_no - 1
                while True:
                    row = filtered_data[filtered_data["Verse No."] == target_verse_no]
                    if row.empty:
                        break
                    row_data = row.iloc[0]
                    consecutive_verses.insert(0, row_data)
                    target_verse_no -= 1

                if consecutive_verses:
                    preceding_verses_text += "\n### Preceding Verses & Translations\n\n"
                    for row_data in consecutive_verses:
                        verse_no = row_data.get("Verse No.", "")
                        verse_text = row_data.get("Verse", "")
                        translation = row_data.get("Translation", "")
                        preceding_verses_text += f"**Verse {verse_no}:** {verse_text}\n"
                        preceding_verses_text += f"**Translation:** {translation}\n\n"

        clipboard_text += preceding_verses_text

        # --- Past Translation of the Current Verse ---
        past_translation_text = ""
        if current_candidate:
            text_set_no = current_candidate.get("Text Set No.")
            try:
                current_verse_no = int(current_candidate.get("Verse No."))
            except (ValueError, TypeError):
                current_verse_no = None

            if current_verse_no is not None:
                filtered_data = existing_data[existing_data["Text Set No."] == text_set_no]
                # Instead of iterating for preceding verses, we directly extract the row for the current verse.
                row = filtered_data[filtered_data["Verse No."] == current_verse_no]
                if not row.empty:
                    row_data = row.iloc[0]
                    verse_text = row_data.get("Verse", "")
                    translation = row_data.get("Translation", "")
                    past_translation_text += "\n### Past Translation of the Current Verse\n\n"
                    past_translation_text += f"**Verse {current_verse_no}:** {verse_text}\n"
                    past_translation_text += f"**Translation:** {translation}\n\n"

        clipboard_text += past_translation_text

        current_verse_words = self.accumulated_pankti.split()

        def find_sublist_index(haystack, needle):
            for i in range(len(haystack) - len(needle) + 1):
                if haystack[i:i + len(needle)] == needle:
                    return i
            return -1

        start_index = find_sublist_index(self.pankti_words, current_verse_words)
        if start_index == -1:
            start_index = 0

        for i, word in enumerate(current_verse_words):
            actual_index = start_index + i
            clipboard_text += f"**Word {i + 1}: {word}**\n"

            acc_entry = self.accumulated_meanings[actual_index] if actual_index < len(self.accumulated_meanings) else {}
            meanings_list = acc_entry.get("meanings", []) if isinstance(acc_entry, dict) else acc_entry
            meanings_str = ", ".join(meanings_list) if meanings_list else "No user-selected meanings available"
            clipboard_text += f"- **User-Selected Meanings:** {meanings_str}\n"

            # --- Past Assessment Details ---
            assessment_details = self.past_word_details.get(actual_index, {})
            if assessment_details:
                clipboard_text += "- **Past Assessment Details:**\n"
                # Display past meanings if available
                past_meanings = assessment_details.get("darpan_meanings", [])
                if past_meanings:
                    clipboard_text += f"   - **Past Meanings:** {', '.join(past_meanings)}\n"
                # Display grammar fields
                clipboard_text += f"   - **Vowel Ending:** {self._norm_get(assessment_details, '\\ufeffVowel Ending') or 'N/A'}\n"
                clipboard_text += f"   - **Number / ‡®µ‡®ö‡®®:** {assessment_details.get('Number / ‡®µ‡®ö‡®®', 'N/A')}\n"
                clipboard_text += f"   - **Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£:** {assessment_details.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', 'N/A')}\n"
                clipboard_text += f"   - **Gender / ‡®≤‡®ø‡©∞‡®ó:** {assessment_details.get('Gender / ‡®≤‡®ø‡©∞‡®ó', 'N/A')}\n"
                clipboard_text += f"   - **Word Root:** {assessment_details.get('Word Root', 'N/A')}\n"
                clipboard_text += f"   - **Word Type:** {self._norm_get(assessment_details, 'Type') or 'N/A'}\n"

            # --- Grammar Options ---
            clipboard_text += "- **Grammar Options:**\n"
            finalized_matches_list = self.accumulated_finalized_matches[actual_index] if actual_index < len(self.accumulated_finalized_matches) else []

            if finalized_matches_list:
                for option_idx, match in enumerate(finalized_matches_list, start=1):
                    clipboard_text += (
                        f"  - **Option {option_idx}:**\n"
                        f"      - **Word:** {self._norm_get(match, 'Word') or 'N/A'}\n"
                        f"      - **Vowel Ending:** {self._norm_get(match, '\\ufeffVowel Ending') or 'N/A'}\n"
                        f"      - **Number / ‡®µ‡®ö‡®®:** {match.get('Number / ‡®µ‡®ö‡®®', 'N/A')}\n"
                        f"      - **Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£:** {match.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', 'N/A')}\n"
                        f"      - **Gender / ‡®≤‡®ø‡©∞‡®ó:** {match.get('Gender / ‡®≤‡®ø‡©∞‡®ó', 'N/A')}\n"
                        f"      - **Word Root:** {match.get('Word Root', 'N/A')}\n"
                        f"      - **Type:** {self._norm_get(match, 'Type') or 'N/A'}\n"
                        f"      - **Literal Translation (Option {option_idx}):** The word '{word}' functions as a "
                        f"'{self._norm_get(match, 'Type') or 'N/A'}' with '{match.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', 'N/A')}' usage, "
                        f"in the '{match.get('Number / ‡®µ‡®ö‡®®', 'N/A')}' form and '{match.get('Gender / ‡®≤‡®ø‡©∞‡®ó', 'N/A')}' gender. Translation: ‚Ä¶\n"
                    )
            else:
                clipboard_text += "  - No finalized grammar options available\n"

            clipboard_text += "\n"

        if '‡••' in current_verse_words:
            clipboard_text += (
                "**Symbol:** ‡••\n"
                "- **Meaning:** End of verse or sentence\n"
                "- **Context:** Denotes the conclusion of the verse.\n\n"
            )

        clipboard_text += "\n### Literal Translation Prompt\n"
        clipboard_text += (
            f"Using the above user-selected meanings and grammar details for the verse '{self.accumulated_pankti}', "
            "please generate a literal translation that adheres strictly to the grammatical structure, "
            "capturing the tense, number, gender, and function accurately."
        )

        return clipboard_text

    def prompt_for_assessment_once_reanalysis(self):
        """Opens a modal prompt for re-analysis of the entire verse and returns the collected assessment data."""
        assessment_win = tk.Toplevel(self.root)
        assessment_win.title(f"Re-Assessment: '{self.accumulated_pankti}'")
        assessment_win.configure(bg='light gray')

        instruction_label = tk.Label(
            assessment_win,
            text="Paste the updated translation or assessment for this verse:",
            font=("Helvetica", 14), bg="light gray"
        )
        instruction_label.pack(pady=10)

        analysis_text = scrolledtext.ScrolledText(
            assessment_win, width=80, height=10,
            font=("Helvetica", 12), wrap=tk.WORD
        )
        analysis_text.pack(padx=20, pady=10)

        cb_frame = tk.Frame(assessment_win, bg="light gray")
        cb_frame.pack(pady=10)

        framework_var = tk.BooleanVar()
        explicit_var = tk.BooleanVar()

        framework_cb = tk.Checkbutton(cb_frame, text="Framework?", variable=framework_var,
                                    font=("Helvetica", 12), bg="light gray")
        framework_cb.pack(side=tk.LEFT, padx=10)

        explicit_cb = tk.Checkbutton(cb_frame, text="Explicit?", variable=explicit_var,
                                    font=("Helvetica", 12), bg="light gray")
        explicit_cb.pack(side=tk.LEFT, padx=10)

        assessment_data = {}

        def on_save():
            translation = analysis_text.get("1.0", tk.END).strip()
            if not translation:
                messagebox.showerror("Error", "Please provide the revised assessment or translation.")
                return
            assessment_data["Translation"] = translation
            assessment_data["Framework?"] = framework_var.get()
            assessment_data["Explicit?"] = explicit_var.get()
            assessment_win.destroy()

        save_btn = tk.Button(
            assessment_win, text="Save Re-Assessment",
            command=on_save, font=("Helvetica", 14, "bold"),
            bg="#2a7b39", fg="white", padx=20, pady=10
        )
        save_btn.pack(pady=20)

        assessment_win.transient(self.root)
        assessment_win.grab_set()
        self.root.wait_window(assessment_win)

        return assessment_data

    def prompt_for_final_grammar_reanalysis(self, word_entries):
        """
        Opens a modal window to finalize grammar during reanalysis.
        Displays a structured prompt for ChatGPT and allows selection from available grammar options.
        """
        final_choice = {}

        final_win = tk.Toplevel(self.root)
        final_win.title(f"Reanalysis: Finalize Grammar for '{word_entries[0]['Word']}'")
        final_win.configure(bg='light gray')

        # --- Build prompt for clipboard ---
        prompt_lines = [
            f"Finalize the applicable grammar for the word: {word_entries[0]['Word']}"
        ]
        prompt_lines.append("The following grammar options are available:")

        fields = [
            "\ufeffVowel Ending", "Number / ‡®µ‡®ö‡®®", "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",
            "Gender / ‡®≤‡®ø‡©∞‡®ó",   "Word Root", "Type"
        ]

        for idx, entry in enumerate(word_entries, start=1):
            # coerce each field to str, converting NaN ‚Üí ""
            parts = []
            for f in fields:
                val = self._norm_get(entry, f) or ""
                if pd.isna(val):
                    val = ""
                parts.append(str(val))
            summary = " | ".join(parts)

            prompt_lines.append(f"Option {idx}: {summary}")

        prompt_text = "\n".join(prompt_lines)

        # --- Prompt display with copy ---
        prompt_frame = tk.Frame(final_win, bg="light gray")
        prompt_frame.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)

        tk.Label(prompt_frame,
                text="ChatGPT Prompt for Grammar Reassessment:",
                font=("Helvetica", 14, "bold"),
                bg="light gray").pack(anchor="w", pady=(0, 5))

        prompt_text_widget = scrolledtext.ScrolledText(prompt_frame, width=80, height=6,
                                                    font=("Helvetica", 12), wrap=tk.WORD)
        prompt_text_widget.pack(fill=tk.BOTH, expand=True)
        prompt_text_widget.insert(tk.END, prompt_text)
        prompt_text_widget.config(state=tk.DISABLED)

        def copy_prompt():
            self.root.clipboard_clear()
            self.root.clipboard_append(prompt_text)
            messagebox.showinfo("Copied", "Prompt text copied to clipboard!")

        tk.Button(prompt_frame, text="Copy Prompt", command=copy_prompt,
                font=("Helvetica", 12, "bold"), bg="#007acc", fg="white", padx=10, pady=5
                ).pack(anchor="e", pady=5)

        # --- Instruction and option selection ---
        tk.Label(final_win,
                text="Please select the correct grammar from the following options:",
                font=("Helvetica", 14), bg="light gray").pack(pady=10)

        choice_var = tk.IntVar(value=0)
        options_container = tk.Frame(final_win, bg="light gray")
        options_container.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)

        canvas = tk.Canvas(options_container, bg="light gray", highlightthickness=0)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb = tk.Scrollbar(options_container, orient="vertical", command=canvas.yview)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.configure(yscrollcommand=vsb.set)

        options_frame = tk.Frame(canvas, bg="light gray")
        canvas.create_window((0, 0), window=options_frame, anchor="nw")

        def on_frame_configure(event):
            canvas.configure(scrollregion=canvas.bbox("all"))
        options_frame.bind("<Configure>", on_frame_configure)

        def as_str(val):
            # turn real NaN ‚Üí "" and everything else ‚Üí string
            return "" if pd.isna(val) else str(val)

        fields = [
            "\ufeffVowel Ending", "Number / ‡®µ‡®ö‡®®", "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",
            "Gender / ‡®≤‡®ø‡©∞‡®ó",   "Word Root", "Type"
        ]

        for idx, entry in enumerate(word_entries):
            summary = " | ".join(
                as_str(self._norm_get(entry, f) or "") for f in fields
            )

            tk.Radiobutton(options_frame,
                        text=f"Option {idx+1}: {summary}",
                        variable=choice_var,
                        value=idx,
                        bg="light gray",
                        font=("Helvetica", 12),
                        anchor='w',
                        justify=tk.LEFT,
                        selectcolor='light blue'
                        ).pack(anchor="w", padx=10, pady=5)

        def on_save():
            selected_index = choice_var.get()
            nonlocal final_choice
            final_choice = word_entries[selected_index]
            final_win.destroy()

        tk.Button(final_win, text="Save Choice",
                command=on_save,
                font=("Helvetica", 14, "bold"),
                bg="#2a7b39", fg="white", padx=20, pady=10).pack(pady=20)

        final_win.transient(self.root)
        final_win.grab_set()
        self.root.wait_window(final_win)
        return final_choice

    def save_assessment_data_reanalysis(self, new_entry):
        """
        Saves a new re-analysis entry to the Excel file.
        Handles grammar and translation revision tracking for specific word occurrences.
        """
        file_path = "1.2.1 assessment_data.xlsx"
        df_existing = self.load_existing_assessment_data(file_path)

        grammar_keys = [
            '\ufeffVowel Ending', 'Number / ‡®µ‡®ö‡®®', 'Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£',
            'Gender / ‡®≤‡®ø‡©∞‡®ó', 'Word Root', 'Type'
        ]

        # Update translation for all entries of the same verse
        df_existing.loc[df_existing["Verse"] == new_entry["Verse"], "Translation"] = new_entry["Translation"]

        # Locate matching entries
        matching_rows = df_existing[
            (df_existing["Word"] == new_entry["Word"]) &
            (df_existing["Verse"] == new_entry["Verse"]) &
            (df_existing["Word Index"] == new_entry["Word Index"])
        ]

        if not matching_rows.empty:
            latest_idx = matching_rows["Grammar Revision"].idxmax()
            latest_row = df_existing.loc[latest_idx]
            differences = any(new_entry.get(key) != self._norm_get(latest_row, key) for key in grammar_keys)

            if differences:
                new_revision = matching_rows["Grammar Revision"].max() + 1
                new_entry["Grammar Revision"] = new_revision

                for idx in matching_rows.index:
                    for key in grammar_keys:
                        df_existing.at[idx, key] = new_entry.get(key)
                    df_existing.at[idx, "Grammar Revision"] = new_revision

                    for key, value in new_entry.items():
                        if key not in grammar_keys and key != "Translation":
                            if key in ("Framework?", "Explicit?"):
                                df_existing.at[idx, key] = int(value)
                            else:
                                df_existing.at[idx, key] = value

                # Update Selected Darpan Meaning
                global_index = new_entry.get("Word Index", 0)
                if len(self.accumulated_meanings) > global_index:
                    acc_entry = self.accumulated_meanings[global_index]
                    selected_meaning = "| ".join(acc_entry.get("meanings", [])) if isinstance(acc_entry, dict) else ", ".join(acc_entry)
                    for idx in matching_rows.index:
                        df_existing.at[idx, "Selected Darpan Meaning"] = selected_meaning

                # Update Translation Revision
                verse_mask = df_existing["Verse"] == new_entry["Verse"]
                latest_revision = df_existing.loc[verse_mask, "Grammar Revision"].max()
                df_existing.loc[verse_mask, "Translation Revision"] = latest_revision
            else:
                return  # No changes, skip saving
        else:
            new_entry["Grammar Revision"] = 1
            current_revision = df_existing[df_existing["Verse"] == new_entry["Verse"]]["Translation Revision"].max()
            new_entry["Translation Revision"] = (current_revision + 1) if not pd.isna(current_revision) else 1

            # Set Selected Darpan Meaning
            global_index = new_entry.get("Word Index", 0)
            if len(self.accumulated_meanings) > global_index:
                acc_entry = self.accumulated_meanings[global_index]
                selected_meaning = "| ".join(acc_entry.get("meanings", [])) if isinstance(acc_entry, dict) else ", ".join(acc_entry)
                new_entry["Selected Darpan Meaning"] = selected_meaning

            df_existing = pd.concat([df_existing, pd.DataFrame([new_entry])], ignore_index=True)

        try:
            df_existing.to_excel(file_path, index=False)
        except Exception as e:
            messagebox.showerror("Error", f"Failed to save reanalysis data: {e}")

    def ensure_meanings_slot_initialized(self, index, word):
        """Ensure self.accumulated_meanings[index] is initialized with a valid structure."""
        while len(self.accumulated_meanings) <= index:
            self.accumulated_meanings.append({"word": None, "meanings": []})

        if not isinstance(self.accumulated_meanings[index], dict) or "word" not in self.accumulated_meanings[index]:
            self.accumulated_meanings[index] = {"word": word, "meanings": []}

    def setup_main_analysis_interface(self):
        """Builds the main analysis interface for Literal Meaning Analysis."""
        # Define a consistent color scheme
        BG_COLOR = "#f0f0f0"        # Light gray background for main area
        HEADER_COLOR = "#2c3e50"    # Dark slate-like header
        HEADER_TEXT_COLOR = "white"
        BUTTON_COLOR = "#007acc"    # A pleasant blue for action buttons
        BUTTON_TEXT_COLOR = "white"
        NAV_BUTTON_COLOR = "#5f9ea0" # CadetBlue for navigation
        LABEL_TEXT_COLOR = "#333333"
        
        # Define fonts
        TITLE_FONT = ("Helvetica", 20, "bold")
        LABEL_FONT = ("Helvetica", 14, "bold")
        ENTRY_FONT = ("Helvetica", 14)
        BUTTON_FONT = ("Helvetica", 14, "bold")
        RESULT_FONT = ("Helvetica", 12)

        # Clear existing widgets in case we re-enter
        for widget in self.root.winfo_children():
            widget.destroy()

        self.root.configure(bg=BG_COLOR)

        # Main frame (entire interface)
        self.main_frame = tk.Frame(self.root, bg=BG_COLOR, padx=20, pady=20)
        self.main_frame.pack(fill=tk.BOTH, expand=True)

        # Header Label
        header_label = tk.Label(
            self.main_frame,
            text="Grammar Analyzer",
            bg=HEADER_COLOR,
            fg=HEADER_TEXT_COLOR,
            font=TITLE_FONT,
            pady=10
        )
        header_label.pack(fill=tk.X, pady=(0, 20))

        # Label and Entry for Pankti
        self.pankti_label = tk.Label(
            self.main_frame,
            text="Please share the Pankti:",
            bg=BG_COLOR,
            fg=LABEL_TEXT_COLOR,
            font=LABEL_FONT
        )
        self.pankti_label.pack(anchor="w", pady=(0, 5))

        self.pankti_entry = tk.Entry(
            self.main_frame,
            width=70,
            font=ENTRY_FONT,
            bd=3,
            relief=tk.GROOVE
        )
        self.pankti_entry.pack(pady=(0, 15))

        # Analyze Button
        self.analyze_button = tk.Button(
            self.main_frame,
            text="Analyze",
            command=self.analyze_pankti,
            font=BUTTON_FONT,
            bg=BUTTON_COLOR,
            fg=BUTTON_TEXT_COLOR,
            relief=tk.RAISED,
            padx=30,
            pady=10
        )
        self.analyze_button.pack(pady=(0, 20))

        # Results Text Area
        self.results_text = scrolledtext.ScrolledText(
            self.main_frame,
            width=90,
            height=20,
            font=RESULT_FONT,
            bd=3,
            relief=tk.SUNKEN,
            wrap=tk.WORD
        )
        self.results_text.pack(pady=(0, 20))

        # Navigation Frame
        nav_frame = tk.Frame(self.main_frame, bg=BG_COLOR)
        nav_frame.pack(fill=tk.X, pady=10)

        # --- New Word Navigation Panel ---
        # A label to display the current word
        self.word_label = tk.Label(nav_frame, text="", font=("Helvetica", 16, "bold"),
                                bg="white", fg="black", width=20, relief=tk.SUNKEN)
        self.word_label.pack(side=tk.LEFT, padx=10)
        self.update_current_word_label()  # Update this label based on self.current_word_index

        # Previous button for word navigation
        self.prev_button = tk.Button(
            nav_frame,
            text="Previous",
            command=self.prev_word,
            state=tk.DISABLED,
            font=BUTTON_FONT,
            bg=NAV_BUTTON_COLOR,
            fg="white",
            padx=20,
            pady=5
        )
        self.prev_button.pack(side=tk.LEFT, padx=(0, 10))

        # Next button for word navigation
        self.next_button = tk.Button(
            nav_frame,
            text="Next",
            command=self.next_word,
            state=tk.DISABLED,
            font=BUTTON_FONT,
            bg=NAV_BUTTON_COLOR,
            fg="white",
            padx=20,
            pady=5
        )
        self.next_button.pack(side=tk.LEFT, padx=(0, 10))

        # Select Word button to analyze the currently displayed word
        select_word_btn = tk.Button(
            nav_frame,
            text="Select Word",
            command=self.select_current_word,
            font=BUTTON_FONT,
            bg=BUTTON_COLOR,
            fg=BUTTON_TEXT_COLOR,
            padx=20,
            pady=5
        )
        select_word_btn.pack(side=tk.LEFT, padx=(0, 10))
        # --- End Word Navigation Panel ---

        # Copy Analysis Button (initially disabled)
        self.copy_button = tk.Button(
            nav_frame,
            text="Copy Analysis",
            command=self.prompt_copy_to_clipboard,
            font=BUTTON_FONT,
            bg="#1b95e0",   # A slightly different blue for variety
            fg="white",
            padx=20,
            pady=5,
            state=tk.DISABLED
        )
        self.copy_button.pack(side=tk.RIGHT, padx=(10, 0))

        # Back to Dashboard Button
        back_dashboard_btn = tk.Button(
            nav_frame,
            text="Back to Dashboard",
            command=self.back_to_dashboard,
            font=BUTTON_FONT,
            bg="red",
            fg="white",
            padx=20,
            pady=5
        )
        back_dashboard_btn.pack(side=tk.RIGHT, padx=(0, 10))
        
        # Create a new "Save Results" button:
        self.save_results_btn = tk.Button(
            nav_frame,
            text="Save Results",
            command=lambda: self.prompt_save_results(self.all_new_entries, skip_copy=True),
            font=BUTTON_FONT,
            bg="#1b95e0",   # Choose a color you like
            fg="white",
            padx=20,
            pady=5,
            state=tk.DISABLED  # Initially disabled
        )
        self.save_results_btn.pack(side=tk.RIGHT, padx=(0, 10))

    def back_to_dashboard(self):
        """Destroy the current main interface and return to the dashboard."""
        # Destroy the main analysis interface
        self.main_frame.destroy()
        # Optionally, reset any state if needed here.
        # Then, show the dashboard again:
        self.show_dashboard()

    def launch_literal_analysis(self):
        """Clears the dashboard and builds the literal meaning analysis interface."""
        # Clear the root window (removes the dashboard)
        for widget in self.root.winfo_children():
            widget.destroy()

        # Optionally update the window title
        self.root.title("Literal Meaning Analysis")

        # Build the main analysis interface
        self.setup_main_analysis_interface()

    def user_input(self, word, pankti):
        print(f"Opening input window for {word}")
        self.input_submitted = False
        # normalize for repeat-note consistency
        verse_key = unicodedata.normalize(
            "NFC", re.sub(r"\s+", " ", pankti.replace('‡••', '').strip())
        )
        raw_tokens = pankti.split()
        word_norm = unicodedata.normalize("NFC", word.strip())
        safe_idx = max(0, min(self.current_word_index, len(raw_tokens)))
        occurrence_idx = sum(
            1
            for tok in raw_tokens[:safe_idx]
            if unicodedata.normalize("NFC", tok.strip().replace('‡••', '')) == word_norm
        )
        if occurrence_idx > 0 and not getattr(self, "_use_inline_literal_banner", True):
            self._maybe_show_repeat_important_note(word_norm, occurrence_idx, verse_key)

        self.input_window = tk.Toplevel(self.root)
        self.input_window.title(f"Input for {word}")
        self.input_window.configure(bg='light gray')
        self.input_window.state('zoomed')
        self.input_window.resizable(True, True)

        # ---------------------------
        # Display the Pankti on top
        # ---------------------------
        pankti_frame = tk.Frame(self.input_window, bg='light gray')
        pankti_frame.pack(fill=tk.X, padx=20, pady=10)

        pankti_display = tk.Text(
            pankti_frame, wrap=tk.WORD, bg='light gray', font=('Arial', 32),
            height=2, padx=5, pady=5
        )
        pankti_display.pack(fill=tk.X, expand=False)
        pankti_display.insert(tk.END, pankti)
        pankti_display.tag_add("center", "1.0", "end")
        pankti_display.tag_configure("center", justify='center')

        # Highlight the word at self.current_word_index
        words = pankti.split()
        start_idx = 0
        for i, w in enumerate(words):
            # When we reach the word at current_word_index, calculate its start/end
            if i == self.current_word_index:
                end_idx = start_idx + len(w)
                pankti_display.tag_add("highlight", f"1.{start_idx}", f"1.{end_idx}")
                pankti_display.tag_config("highlight", foreground="red", font=('Arial', 32, 'bold'))
                break
            # Move the start index past this word plus one space
            start_idx += len(w) + 1

        pankti_display.config(state=tk.DISABLED)

        # ---------------------------
        # Create a horizontal PanedWindow for split layout
        # ---------------------------
        split_pane = tk.PanedWindow(self.input_window, orient=tk.HORIZONTAL, bg='light gray')
        split_pane.pack(fill=tk.BOTH, expand=True)

        # Left pane: Meanings
        self.left_pane = tk.Frame(split_pane, bg='light gray')
        split_pane.add(self.left_pane, stretch="always")
        tk.Label(self.left_pane, text=f"Meanings for {word}:", bg='light gray',
                font=('Arial', 14, 'bold')).pack(anchor='center', pady=(0, 10))
        self.meanings_scrollbar = tk.Scrollbar(self.left_pane, orient=tk.VERTICAL)
        self.meanings_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        self.meanings_canvas = tk.Canvas(self.left_pane, bg='light gray', borderwidth=0,
                                        yscrollcommand=self.meanings_scrollbar.set)
        self.meanings_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        self.meanings_scrollbar.config(command=self.meanings_canvas.yview)
        self.meanings_inner_frame = tk.Frame(self.meanings_canvas, bg='light gray')
        self.meanings_canvas.create_window((0, 0), window=self.meanings_inner_frame, anchor='nw')

        # Right pane: Grammar Options
        right_pane = tk.Frame(split_pane, bg='light gray')
        split_pane.add(right_pane, stretch="always")
        tk.Label(right_pane, text="Select Grammar Options:", bg='light gray',
                font=('Arial', 14, 'bold')).pack(pady=10)
        self.setup_options(
            right_pane,
            "Do you know the Number of the word?",
            [("Singular", "Singular / ‡®á‡®ï"), ("Plural", "Plural / ‡®¨‡®π‡©Å"), ("Not Applicable", "NA")],
            self.number_var
        )
        self.setup_options(
            right_pane,
            "Do you know the Gender of the word?",
            [("Masculine", "Masculine / ‡®™‡©Å‡®≤‡®ø‡©∞‡®ó"), ("Feminine", "Feminine / ‡®á‡®∏‡®§‡®∞‡©Ä"), ("Neutral", "Trans / ‡®®‡®™‡©Å‡®Ç‡®∏‡®ï")],
            self.gender_var
        )
        self.setup_options(
            right_pane,
            "Do you know the Part of Speech for the word?",
            [("Noun", "Noun / ‡®®‡®æ‡®Ç‡®µ"), ("Adjective", "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£"),
            ("Adverb", "Adverb / ‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡©á‡®∂‡®£"), ("Verb", "Verb / ‡®ï‡®ø‡®∞‡®ø‡®Ü"),
            ("Pronoun", "Pronoun / ‡®™‡©ú‡®®‡®æ‡®Ç‡®µ"), ("Postposition", "Postposition / ‡®∏‡©∞‡®¨‡©∞‡®ß‡®ï"),
            ("Conjunction", "Conjunction / ‡®Ø‡©ã‡®ú‡®ï"), ("Interjection", "Interjection / ‡®µ‡®ø‡®∏‡®Æ‡®ø‡®ï")],
            self.pos_var
        )

        # ---------------------------
        # Bottom Button Frame
        # ---------------------------
        button_frame = tk.Frame(self.input_window, bg='light gray')
        button_frame.pack(pady=20)
        tk.Button(button_frame, text="Submit", command=self.submit_input,
                font=('Arial', 12, 'bold'), bg='navy', fg='white',
                padx=30, pady=15).pack(side=tk.LEFT, padx=15)
        tk.Button(button_frame, text="Skip", command=self.skip_input,
                font=('Arial', 12, 'bold'), bg='navy', fg='white',
                padx=30, pady=15).pack(side=tk.LEFT, padx=15)

        # ---------------------------
        # Launch the dictionary lookup in a separate thread
        # ---------------------------
        self.start_progress()
        threading.Thread(target=self.lookup_meanings_thread, args=(word,), daemon=True).start()

        self.input_window.transient(self.root)
        self.input_window.grab_set()
        self.root.wait_window(self.input_window)
        print(f"Input window for {word} closed")

    def lookup_meanings_thread(self, word):
        """
        Performs the dictionary lookup in a separate thread and then
        schedules the update of the meanings UI on the main thread.
        """
        meanings = self.lookup_word_in_dictionary(word)
        # Schedule the UI update on the main thread
        self.root.after(0, lambda: self.update_meanings_ui(meanings))

    def update_meanings_ui(self, meanings):
        """
        Updates the meanings UI with the lookup results.
        Stops the progress bar and populates the meanings section.
        """
        self.stop_progress()  # Stop the progress window now that lookup is complete
        self.accumulate_meanings_data(meanings)
        split_meanings = self.split_meanings_for_display(meanings)
        # Clear any existing widgets in the meanings inner frame
        for widget in self.meanings_inner_frame.winfo_children():
            widget.destroy()
        # Repopulate the meanings UI
        for i, column in enumerate(split_meanings.values()):
            column_frame = tk.Frame(self.meanings_inner_frame, bg='light gray')
            column_frame.grid(row=0, column=i, padx=10, pady=10, sticky='nw')
            for meaning in column:
                tk.Label(column_frame, text=f"- {meaning}", bg='light gray',
                        font=('Arial', 12), wraplength=400, justify=tk.LEFT).pack(anchor='w', padx=15, pady=5)
        self.meanings_inner_frame.update_idletasks()
        self.meanings_canvas.config(scrollregion=self.meanings_inner_frame.bbox("all"))

    def split_meanings_for_display(self, meanings): # Helper function to split meanings into two columns
        # Determine if 'meanings' is a dict or list.
        if isinstance(meanings, dict):
            mlist = meanings.get("meanings", [])
        else:
            mlist = meanings  # Assume it's already a list.
            
        # Now split the list into two halves.
        mid = len(mlist) // 2
        left = mlist[:mid]
        right = mlist[mid:]
        return {"left": left, "right": right}

    def _rule_key_from_entry(self, d):
        return " | ".join([
            d.get("Word",""),
            d.get("\ufeffVowel Ending",""),
            d.get("Number / ‡®µ‡®ö‡®®",""),
            d.get("Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£",""),
            d.get("Gender / ‡®≤‡®ø‡©∞‡®ó",""),
            d.get("Word Root",""),
            d.get("Type",""),
        ]).strip()

    def submit_matches(self):
        any_selection = False
        current_entries = []  # Local list for entries of the current word

        # Process matching rule checkboxes as before
        for var, match in self.match_vars:
            if var.get():
                match_string = match[0]
                self.results_text.insert(tk.END, f"{match_string}\n")
                self.results_text.insert(tk.END, "-" * 50 + "\n")
                data = match_string.split(" | ")
                new_entry = {
                    "Word": data[0],
                    "\ufeffVowel Ending": data[1],
                    "Number / ‡®µ‡®ö‡®®": data[2],
                    "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£": data[3],
                    "Gender / ‡®≤‡®ø‡©∞‡®ó": data[4],
                    "Word Root": data[5],
                    "Type": data[6],
                }
                current_entries.append(new_entry)
                any_selection = True

        # Also update the accumulated meanings for the current word
        selected_meanings = [meaning for var, meaning in self.meaning_vars if var.get()]
        self.accumulate_meanings_data(selected_meanings)

        # --- NEW BLOCK: Check for repeated word and prompt update for previous occurrences ---
        current_word = self.pankti_words[self.current_word_index]
        # Find the first occurrence index for the current word
        first_index = next((i for i, w in enumerate(self.pankti_words) if w == current_word), self.current_word_index)

        # Get the current selection from the UI for this occurrence.
        current_selected = selected_meanings  # already computed

        # If this is a repeated occurrence, merge prior selections for this word only.
        if self.current_word_index != first_index:
            merged_meanings = []
            for idx in range(first_index, self.current_word_index):
                # Only merge if the word at that index matches the current word.
                if idx < len(self.accumulated_meanings) and self.pankti_words[idx] == current_word:
                    entry = self.accumulated_meanings[idx]
                    if isinstance(entry, dict):
                        merged_meanings.extend(entry.get("meanings", []))
                    else:
                        merged_meanings.extend(entry)
            # Remove duplicates while preserving order.
            prior_meanings = list(dict.fromkeys(merged_meanings))
            
            # Compare the current selection to prior selections.
            if set(current_selected) != set(prior_meanings):
                update_prev = messagebox.askyesno(
                    "Update Previous Meanings",
                    f"You have selected different meanings for the word '{current_word}'.\n"
                    "Do you want to update the meanings for all previous occurrences of this word?"
                )
                if update_prev:
                    for idx in range(first_index, self.current_word_index):
                        if idx < len(self.accumulated_meanings) and self.pankti_words[idx] == current_word:
                            self.accumulated_meanings[idx] = {"word": current_word, "meanings": current_selected}

        # Finally, update (or add) the current occurrence's accumulated meanings with only the current selection.
        if self.current_word_index < len(self.accumulated_meanings):
            self.accumulated_meanings[self.current_word_index] = {"word": current_word, "meanings": current_selected}
        else:
            self.accumulated_meanings.append({"word": current_word, "meanings": current_selected})

        # --- End NEW BLOCK ---

        # --- Assign verse using word index and verse boundaries ---
        verse_boundaries = []
        pointer = 0
        for verse in self.selected_verses:
            verse_words = verse.split()
            start = pointer
            end = pointer + len(verse_words)
            verse_boundaries.append((start, end))
            pointer = end

        current_verse = None
        for i, (start, end) in enumerate(verse_boundaries):
            if start <= self.current_word_index < end:
                current_verse = self.selected_verses[i]
                break

        # Attach current verse to each grammar entry
        for entry in current_entries:
            entry["Verse"] = current_verse

        # Process finalized matches (avoiding duplicates)
        finalized_matches = []
        for var, match in self.match_vars:
            if var.get():
                match_word = match[0].split(" | ")[0]
                for entry in current_entries:
                    if entry["Word"] == match_word and entry not in finalized_matches:
                        finalized_matches.append(entry)
        self.accumulate_finalized_matches(finalized_matches)
            
        if not any_selection:
            messagebox.showwarning("No Selection", "No matches were selected. Please select at least one match.")
        else:
            self.match_window.destroy()
            # Add the current word's entries to the global accumulator
            self.all_new_entries.extend(current_entries)
            self.current_word_index += 1
            self.process_next_word()

    def show_matches(self, matches, pankti, meanings, max_display=30):
        # Destroy any existing match window
        if hasattr(self, 'match_window') and self.match_window.winfo_exists():
            self.match_window.destroy()

        # Create the match window
        self.match_window = tk.Toplevel(self.root)
        self.match_window.title("Select Matches and Meanings")
        self.match_window.configure(bg='light gray')
        self.match_window.state('zoomed')
        # New window ‚áí allow a fresh one-time resize binding
        self._inline_resize_bound = False
        try:
            # If the window is destroyed via an atypical path, ensure the next window can rebind
            # and clear any lingering inline banner references.
            def _on_destroy(_e=None):
                try:
                    self._inline_resize_bound = False
                    self.literal_note_frame = None
                    self.literal_note_title = None
                    self.literal_note_body  = None
                except Exception:
                    pass
            self.match_window.bind("<Destroy>", _on_destroy, add="+")
        except Exception:
            pass

        # Reset check-variable lists for matches and meanings
        self.match_vars = []      # For matching rule checkboxes
        self.meaning_vars = []    # For meaning checkboxes
        unique_matches = self.filter_unique_matches(matches)
        self.all_matches.append(unique_matches)

        # ---------------------------
        # Display the complete Pankti at the top
        # ---------------------------
        pankti_frame = tk.Frame(self.match_window, bg='light gray')
        pankti_frame.pack(fill=tk.BOTH, padx=30, pady=20)

        pankti_display = tk.Text(pankti_frame, wrap=tk.WORD, bg='light gray',
                                font=('Arial', 32), height=2, padx=5, pady=5)
        pankti_display.pack(fill=tk.BOTH, expand=False)
        pankti_display.insert(tk.END, f"{pankti}")
        pankti_display.tag_add("center", "1.0", "end")
        pankti_display.tag_configure("center", justify='center')
        pankti_display.config(state=tk.DISABLED)

        # Compute the character offset for the word at self.current_word_index
        words = pankti.split()
        # Align the navigation token stream with the displayed verse tokens so
        # highlighting, indexing, and repeat detection use the same sequence.
        if getattr(self, "pankti_words", None) != words:
            self._norm_words_cache = [self._norm_tok(w) for w in words]
        self.pankti_words = words
        norm_words = getattr(self, "_norm_words_cache", [])
        max_idx = len(words) - 1
        self.current_word_index = max(0, min(self.current_word_index, max_idx))
        idx = self.current_word_index
        start_idx = 0
        for i, w in enumerate(words):
            if i == idx:
                break
            # +1 accounts for the space between words
            start_idx += len(w) + 1
        end_idx = start_idx + len(words[idx])

        pankti_display.tag_add("highlight", f"1.{start_idx}", f"1.{end_idx}")
        pankti_display.tag_config("highlight", foreground="red", font=('Arial', 32, 'bold'))
        pankti_display.config(state=tk.DISABLED)

        # ----- Inline Important Note ‚Äî Literal Analysis (conditional) -----

        verse_text = pankti
        verse_key = self._verse_key(verse_text)
        # Reset per-verse de-duplication so banner shows at most once per (verse_key, word_norm).
        if self._last_literal_verse_key != verse_key:
            self._repeat_note_shown = set()
            self._last_literal_verse_key = verse_key
            # Respect user's prior choice only within the same verse; reset on verse change.
            self._suppress_repeat_notes_for_verse = False

        display_word = (
            self.pankti_words[idx] if idx < len(self.pankti_words) else words[idx]
        )
        word_norm = (
            norm_words[idx] if idx < len(norm_words) else self._norm_tok(display_word)
        )
        # Guard: skip triggering for empty/vanished normalized tokens.
        if word_norm:
            total_occurrences = norm_words.count(word_norm)
            seen_before = norm_words[:idx].count(word_norm)
            trigger_now = (
                self._has_repeat(norm_words, word_norm) and seen_before >= 1
            )
            key = (verse_key, word_norm, "second")
        else:
            trigger_now = False
            key = None

        inline_enabled = getattr(self, "_use_inline_literal_banner", True)
        inline_allowed = inline_enabled and not getattr(self, "_suppress_repeat_notes_for_verse", False)

        if inline_allowed and trigger_now and key and key not in self._repeat_note_shown:
            self._repeat_note_shown.add(key)
            reuse_ok = (
                hasattr(self, "literal_note_frame") and self.literal_note_frame
                and self.literal_note_frame.winfo_exists()
                and self.literal_note_frame.master is self.match_window
            )
            if not reuse_ok:
                if hasattr(self, "literal_note_frame") and self.literal_note_frame and self.literal_note_frame.winfo_exists():
                    self.literal_note_frame.destroy()
                self.literal_note_frame = tk.Frame(self.match_window, bg='AntiqueWhite', relief='groove', bd=2)
                self.literal_note_title = tk.Label(
                    self.literal_note_frame, text="Important Note ‚Äî Literal Analysis",
                    bg='AntiqueWhite', font=('Arial', 14, 'bold')
                )
                self.literal_note_title.pack(anchor='w', padx=10, pady=(5, 0))
                self.literal_note_body = tk.Label(
                    self.literal_note_frame,
                    bg='AntiqueWhite', wraplength=self._banner_wraplength(self.match_window), justify=tk.LEFT, font=('Arial', 12)
                )
            else:
                if not hasattr(self, "literal_note_title") or not self.literal_note_title or not self.literal_note_title.winfo_exists():
                    self.literal_note_title = tk.Label(
                        self.literal_note_frame, text="Important Note ‚Äî Literal Analysis",
                        bg='AntiqueWhite', font=('Arial', 14, 'bold')
                    )
                    self.literal_note_title.pack(anchor='w', padx=10, pady=(5, 0))
                elif not self.literal_note_title.winfo_ismapped():
                    self.literal_note_title.pack(anchor='w', padx=10, pady=(5, 0))
                if not hasattr(self, "literal_note_body") or not self.literal_note_body or not self.literal_note_body.winfo_exists():
                    self.literal_note_body = tk.Label(
                        self.literal_note_frame,
                        bg='AntiqueWhite', wraplength=self._banner_wraplength(self.match_window), justify=tk.LEFT, font=('Arial', 12)
                    )
            # Pack the frame only when we actually have a repeat to show
            if not self.literal_note_frame.winfo_ismapped():
                self.literal_note_frame.pack(fill=tk.X, padx=20, pady=(5, 10))

            # Configure the body text and ensure it's visible
            banner_text = (
                f"In literal analysis: The word ‚Äú{display_word}‚Äù appears multiple times in this verse. "
                "The highlighted grammar options reflect your past selections for this word (or close matches) "
                "to encourage consistency. They‚Äôre suggestions, not mandates‚Äîadjust if the current context differs."
            )
            body = self.literal_note_body
            if body and body.winfo_exists():
                body.config(
                    text=banner_text,
                    wraplength=self._banner_wraplength(self.match_window)
                )
                if not body.winfo_ismapped():
                    body.pack(anchor='w', padx=10, pady=(0, 5))
            # ensure correct wrap on first paint
            try:
                self._on_match_window_resize()
            except Exception:
                pass
            # Reflow text on main window resize (bind once per window)
            try:
                if not hasattr(self, "_inline_resize_bound") or not self._inline_resize_bound:
                    self.match_window.bind("<Configure>", self._on_match_window_resize, add="+")
                    self._inline_resize_bound = True
            except Exception:
                pass
        else:
            # If inline banners are disabled or there is no repeat, clean up any stale frame.
            if hasattr(self, "literal_note_frame") and self.literal_note_frame:
                if (self.literal_note_frame.winfo_exists() or
                        self.literal_note_frame.master is not self.match_window):
                    self.literal_note_frame.destroy()
                self.literal_note_frame = None
                self.literal_note_title = None
                self.literal_note_body  = None
        # ----- end inline note -----

        # ---------------------------
        # Create a main frame to hold both the Meanings and the Matching Rules sections
        # ---------------------------
        main_frame = tk.Frame(self.match_window, bg='light gray')
        main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)

        # ---------------------------
        # Left Pane: Display Meanings as Checkboxes
        # ---------------------------
        meanings_frame = tk.Frame(main_frame, bg='light gray')
        meanings_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=10)

        tk.Label(meanings_frame, text=f"Select Meanings for {self.pankti_words[self.current_word_index]}:",
                bg='light gray', font=('Arial', 14, 'bold')).pack(pady=10)
        # NEW: Add a toggle checkbutton to select/deselect all meanings
        self.select_all_meanings_var = tk.BooleanVar(value=True)
        tk.Checkbutton(meanings_frame, text="Select/Deselect All Meanings",
                    variable=self.select_all_meanings_var, bg='light gray',
                    font=('Arial', 12), command=self.toggle_all_meanings).pack(pady=5)

        meanings_canvas = tk.Canvas(meanings_frame, bg='light gray', borderwidth=0)
        meanings_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        meanings_scrollbar = tk.Scrollbar(meanings_frame, orient=tk.VERTICAL, command=meanings_canvas.yview)
        meanings_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        meanings_canvas.config(yscrollcommand=meanings_scrollbar.set)

        meanings_content = tk.Frame(meanings_canvas, bg='light gray')
        meanings_canvas.create_window((0, 0), window=meanings_content, anchor='nw')

        # Split the meanings into two columns and create a checkbox for each meaning
        split_meanings = self.split_meanings_for_display(meanings)
        # --- Determine if the current word is repeated and merge prior selections ---
        current_word = self.pankti_words[self.current_word_index]
        first_index = next((i for i, w in enumerate(self.pankti_words) if w == current_word), self.current_word_index)

        # Merge meanings from all occurrences from the first occurrence up to (but not including) the current occurrence.
        merged_meanings = []
        for idx in range(first_index, self.current_word_index):
            if idx < len(self.accumulated_meanings):
                entry = self.accumulated_meanings[idx]
                if isinstance(entry, dict):
                    merged_meanings.extend(entry.get("meanings", []))
                else:
                    merged_meanings.extend(entry)
        # Remove duplicates while preserving order
        prior_meanings = list(dict.fromkeys(merged_meanings))

        # If this is a repeated occurrence (current index is not the first occurrence), reorder the meanings.
        if self.current_word_index != first_index:
            # Obtain the current meanings list
            if isinstance(meanings, dict):
                current_meanings = meanings.get("meanings", [])
            else:
                current_meanings = meanings
            # Reorder: meanings from the first occurrence first, then the rest.
            reordered = [m for m in current_meanings if m in prior_meanings]
            reordered += [m for m in current_meanings if m not in prior_meanings]
            if isinstance(meanings, dict):
                meanings["meanings"] = reordered
            else:
                meanings = reordered

        # Now split the meanings for display using the (possibly) reordered meanings.
        split_meanings = self.split_meanings_for_display(meanings)

        # --- Create the checkboxes as before ---
        for i, column in enumerate(split_meanings.values()):
            column_frame = tk.Frame(meanings_content, bg='light gray')
            column_frame.grid(row=0, column=i, padx=10, pady=10, sticky='nw')
            for meaning in column:
                # For repeated occurrences, preselect if the meaning was chosen in the first occurrence,
                # and highlight those checkbuttons with yellow.
                if self.current_word_index != first_index:
                    preselect = meaning in prior_meanings
                    bg_color = "yellow" if preselect else "light gray"
                else:
                    preselect = True
                    bg_color = "light gray"
                var = tk.BooleanVar(value=preselect)
                chk = tk.Checkbutton(
                    column_frame,
                    text=f"- {meaning}",
                    variable=var,
                    bg=bg_color,
                    font=('Arial', 12),
                    wraplength=325,
                    anchor='w',
                    justify=tk.LEFT,
                    selectcolor='light blue'
                )
                chk.pack(anchor='w', padx=15, pady=5)
                self.meaning_vars.append((var, meaning))

        meanings_content.update_idletasks()
        meanings_canvas.config(scrollregion=meanings_canvas.bbox("all"))

        # ---------------------------
        # Right Pane: Display Matching Rules as Checkboxes
        # ---------------------------
        matches_frame = tk.Frame(main_frame, bg='light gray')
        matches_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=10)

        tk.Label(matches_frame, text="Select the matching rules:",
                bg='light gray', font=('Arial', 14, 'bold')).pack(pady=10)

        matches_canvas = tk.Canvas(matches_frame, bg='light gray', borderwidth=0)
        matches_canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        matches_scrollbar = tk.Scrollbar(matches_frame, orient=tk.VERTICAL, command=matches_canvas.yview)
        matches_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        matches_canvas.config(yscrollcommand=matches_scrollbar.set)

        matches_content = tk.Frame(matches_canvas, bg='light gray')
        matches_canvas.create_window((0, 0), window=matches_content, anchor='nw')

        # Determine if the current word is repeated and gather prior grammar rules
        prior_rules = set()
        if self.current_word_index != first_index:
            for idx in range(first_index, self.current_word_index):
                if idx < len(self.accumulated_finalized_matches):
                    prior_rules.update({
                        self._rule_key_from_entry(entry)
                        for entry in self.accumulated_finalized_matches[idx]
                    })

        # Display each match with a checkbox
        for match in unique_matches[:max_display]:
            display_str = match[0]
            core = re.sub(r'\s*\(Matching Characters:\s*\d+\)\s*$', '', display_str).strip()
            if self.current_word_index != first_index:
                preselect = core in prior_rules
                bg_color = "yellow" if preselect else "light gray"
            else:
                preselect = False
                bg_color = "light gray"
            var = tk.BooleanVar(value=preselect)
            text_str = display_str if " (Matching Characters:" in display_str else f"{display_str} (Matching Characters: {match[1]})"
            tk.Checkbutton(
                matches_content,
                text=text_str,
                variable=var,
                bg=bg_color,
                selectcolor='light blue',
                anchor='w'
            ).pack(fill=tk.X, padx=10, pady=5)
            self.match_vars.append((var, match))

        matches_content.update_idletasks()
        matches_canvas.config(scrollregion=matches_canvas.bbox("all"))

        # ---------------------------
        # Bottom Button Frame: Submit and Back
        # ---------------------------
        button_frame = tk.Frame(self.match_window, bg='light gray')
        button_frame.pack(pady=10)
        tk.Button(button_frame, text="Submit", command=self.submit_matches,
                font=('Arial', 12, 'bold'), bg='navy', fg='white', padx=20, pady=10
                ).pack(side=tk.LEFT, padx=5)
        tk.Button(button_frame, text="Back", command=lambda: self.back_to_user_input_with_pankti(pankti),
                font=('Arial', 12, 'bold'), bg='navy', fg='white', padx=20, pady=10
                ).pack(side=tk.LEFT, padx=5)

        print("Match window created and populated.")

    def toggle_all_meanings(self):
        """Toggle all meaning checkboxes based on the select-all checkbutton."""
        new_value = self.select_all_meanings_var.get()
        for var, meaning in self.meaning_vars:
            var.set(new_value)

    def load_grammar_data(self, file_path):
        """
        Loads grammar data from a CSV file.

        Args:
        file_path (str): The path to the CSV file containing grammar data.

        Returns:
        list: A list of dictionaries containing the grammar data.

        Raises:
        FileNotFoundError: If the specified file does not exist.
        IOError: If an error occurs while reading the file.
        """
        try:
            with open(file_path, "r", encoding='utf-8') as data_base:
                return list(csv.DictReader(data_base))
        except FileNotFoundError:
            print(f"Error: The file '{file_path}' does not exist.")
            return []
        except IOError as e:
            print(f"Error reading file '{file_path}': {e}")
            return []

    def break_into_characters(self, word):
        """Breaks a word into individual characters."""
        return [char for char in unicodedata.normalize('NFC', word)]

    def match_inflections(self, word, inflection, pos):
        """
        Determines the number of Matching Characters between the word and the inflection pattern,
        considering the part of speech. Matches are weighted by position and continuity.

        Args:
        word (str): The word to check.
        inflection (str): The inflection pattern to match against.
        pos (str): The part of speech to consider.

        Returns:
        int: The number of Matching Characters, weighted by position and continuity.
        """
        # Break the word and inflection into individual characters
        word_chars = self.break_into_characters(word)
        inflection_chars = self.break_into_characters(inflection)

        # Only consider specified endings for nouns
        if (pos == "Noun / ‡®®‡®æ‡®Ç‡®µ" or pos == "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£") and inflection == '‡®Æ‡©Å‡®ï‡®§‡®æ':
            return 0  # No suffix match needed for this case

        # Initialize the match count
        match_count = 0

        # Determine the minimum length between word and inflection for suffix comparison
        min_length = min(len(word_chars), len(inflection_chars))

        # Iterate through the characters in reverse order (from the end of the word and inflection)
        for i in range(1, min_length + 1):
            if word_chars[-i] == inflection_chars[-i]:
                # Increment the match count with a weight factor based on position
                match_count += i  # Giving more weight to matches that occur further back in the word
            else:
                break  # Stop the loop if a mismatch is found (ensuring continuity)

        return match_count  # Return the weighted match count

    def lookup_word_in_dictionary(self, word):
        """
        Looks up the meanings of a word in the dictionary data.

        Args:
        word (str): The word to look up in the dictionary.

        Returns:
        list: A list of meanings for the word.
        """
        meanings = []
        exact_meanings = []

        # Attempt to find the word directly in the dictionary
        result = self.dictionary_data[self.dictionary_data['Word'] == word]
        if not result.empty:
            exact_meanings = ast.literal_eval(result.iloc[0]['Meanings'])
            print(f"Found exact match for: {word}")

        # Search for the word within combined entries regardless of exact match
        combined_entries = []
        for _, row in self.dictionary_data.iterrows():
            combined_word = row['Word']
            combined_word_list = re.split(r'\s+', combined_word)  # Split by whitespace to handle combined words

            if word in combined_word_list:  # Check if the exact word is in the list of combined words
                print(f"Found exact match in combined entry: {row['Word']}")
                combined_entries.append(row)

        # If combined entries are found, proceed with adjacent word search
        adjacent_word_matches = []
        if combined_entries:
            words_in_pankti = self.accumulated_pankti.split()  # Split pankti into words
            word_index = words_in_pankti.index(word) if word in words_in_pankti else -1

            # Identify adjacent words in the pankti
            adjacent_combinations = []
            if word_index != -1:
                # Two-word combinations (main word + adjacent)
                if word_index > 0:  # Previous word
                    adjacent_combinations.append([words_in_pankti[word_index - 1], word])
                if word_index < len(words_in_pankti) - 1:  # Next word
                    adjacent_combinations.append([word, words_in_pankti[word_index + 1]])

                # Three-word combinations (main word + two adjacents)
                if word_index < len(words_in_pankti) - 2:
                    adjacent_combinations.append([words_in_pankti[word_index], words_in_pankti[word_index + 1], words_in_pankti[word_index + 2]])
                if word_index > 0 and word_index < len(words_in_pankti) - 1:
                    adjacent_combinations.append([words_in_pankti[word_index - 1], words_in_pankti[word_index], words_in_pankti[word_index + 1]])
                if word_index >= 2:
                    adjacent_combinations.append([words_in_pankti[word_index - 2], words_in_pankti[word_index - 1], words_in_pankti[word_index]])

            # Now search within combined entries using adjacent word combinations
            for entry in combined_entries:
                combined_word_list = re.split(r'\s+', entry['Word'])  # Split by whitespace to handle combined words

                for combination in adjacent_combinations:
                    # Convert the combination to possible strings to match
                    combination_strings = [' '.join(combination), ' '.join(combination[::-1])]
                    
                    if any(comb_str in entry['Word'] for comb_str in combination_strings):
                        print(f"Found adjacent match in combined entry: {entry['Word']}")
                        combined_meanings = ast.literal_eval(entry['Meanings'])
                        combined_entry = f"{entry['Word']}: {', '.join(combined_meanings)}"
                        adjacent_word_matches.append(combined_entry)

            # Sort the adjacent matches to prioritize three-word matches
            adjacent_word_matches = sorted(adjacent_word_matches, key=lambda x: len(x.split(' ')), reverse=True)

        # If both exact match and adjacent word matches are found, return adjacent first
        if adjacent_word_matches and exact_meanings:
            return adjacent_word_matches + exact_meanings

        # If only exact match is found, return exact match
        if exact_meanings:
            return exact_meanings

        # If only adjacent word matches are found, return them
        if adjacent_word_matches:
            return adjacent_word_matches

        # If no adjacent word matches are found, return original combined entries meanings
        for entry in combined_entries:
            combined_meanings = ast.literal_eval(entry['Meanings'])
            combined_entry = f"{entry['Word']}: {', '.join(combined_meanings)}"
            meanings.append(combined_entry)

        # If meanings are found, return them
        if meanings:
            return meanings

        # If no meaning is found, return a list with a single string message
        return [f"No meanings found for {word}"]

    def back_to_user_input_with_pankti(self, pankti):
        """
        Allows the user to return to the input stage for the current word, with pankti passed as an argument.
        """
        try:
            if hasattr(self, 'match_window') and self.match_window:
                self.match_window.destroy()  # Close the match window if it exists

            # Ensure the current_word_index is within valid range
            if 0 <= self.current_word_index < len(self.pankti_words):
                word = self.pankti_words[self.current_word_index]  # Get the current word
                self.reset_input_variables()  # Reset selections for the new word
                self.user_input(word, pankti)  # Reopen the input window for the current word and pass pankti
            else:
                messagebox.showerror("Error", "No valid word to return to.")
        except Exception as e:
            messagebox.showerror("Error", f"An error occurred: {e}")

    def fetch_data(self, word, pankti):
        try:
            self.reset_input_variables()  # Reset selections for the new word
            print(f"Opening input window for {word}")

            self.user_input(word, pankti)  # Opens the input window with the Pankti

            # Check if input_window exists before waiting
            if hasattr(self, 'input_window') and self.input_window.winfo_exists():
                self.root.wait_window(self.input_window)  # Wait for the input window to close
            else:
                print(f"Input window for {word} did not open properly.")
                return  # Exit if the input window was not opened

            print(f"Input window for {word} closed")

            if not self.input_submitted:
                print(f"No input submitted for {word}. Skipping to next word.")
                self.current_word_index += 1
                self.process_next_word()
            else:
                # Process the submitted input
                self.handle_submitted_input(word)

        except Exception as e:
            print(f"An error occurred while fetching data for {word}: {str(e)}")
            # Ensure the input window is closed in case of an error
            if hasattr(self, 'input_window') and self.input_window.winfo_exists():
                try:
                    self.input_window.destroy()
                except Exception as close_error:
                    print(f"Failed to close input window: {str(close_error)}")
            # Optionally, log the error to a file for future debugging
            with open("error_log.txt", "a", encoding="utf-8") as log_file:
                log_file.write(f"Error with word '{word}': {str(e)}\n")

    def process_next_word(self):
        """Process the next valid word or prompt for saving if finished."""
        pankti = " ".join(self.pankti_words)
        self.current_pankti = pankti

        if self.current_word_index < len(self.pankti_words):
            word = self.pankti_words[self.current_word_index]
            if self.is_non_word_character(word):
                self.current_word_index += 1  # Skip non-word characters
                self.process_next_word()
            else:
                self.fetch_data(word, pankti)  # Process current word
        else:
            # All words processed‚Äîprompt to save using the global accumulator
            self.save_results_btn.config(state=tk.NORMAL)
            self.prompt_save_results(self.all_new_entries)

    def skip_input(self):
        """
        Handles the action when the user decides to skip the current word.
        """
        try:
            # Ask for user confirmation before skipping
            confirm_skip = messagebox.askyesno("Confirm Skip", "Are you sure you want to skip this word?")
            if not confirm_skip:
                return  # Do nothing if the user cancels the skip

            # Mark input as not submitted and close the input window
            self.input_submitted = False
            self.input_window.destroy()

            # Update the index to move to the next word and process it
            self.current_word_index += 1
            self.process_next_word()
        except Exception as e:
            messagebox.showerror("Error", f"An error occurred while skipping: {e}")

    def submit_input(self):
        self.input_submitted = True
        if hasattr(self, 'input_window') and self.input_window.winfo_exists():
            self.input_window.destroy()

        # Start the progress bar
        self.start_progress()

        # Run the search in a separate thread
        search_thread = threading.Thread(target=self.perform_search_and_finish)
        search_thread.start()

    def perform_search_and_finish(self):
        current_word = self.pankti_words[self.current_word_index]
        number = self.number_var.get()
        gender = self.gender_var.get()
        pos = self.pos_var.get()

        print(f"Processing word: {current_word}, Number: {number}, Gender: {gender}, POS: {pos}")
        
        matches = []
        
        if number == "NA" and gender == "NA" and pos == "NA":
            matches = self.search_by_inflections(current_word)
        else:
            matches = self.search_by_criteria(current_word, number, gender, pos)
            if not matches:
                messagebox.showinfo("No Matches Found", "No matches were found as per the criteria given by you. Now conducting a general search.")
                matches = self.search_by_inflections(current_word)

        # Check if meanings are already accumulated for the current word index
        if len(self.accumulated_meanings) > self.current_word_index:
            entry = self.accumulated_meanings[self.current_word_index]
            # If the entry is a dictionary, extract the meanings list; otherwise assume it's already a list.
            if isinstance(entry, dict):
                meanings = entry.get("meanings", [])
            else:
                meanings = entry
            self.handle_lookup_results(matches, meanings)
        else:
            # Launch the dictionary lookup in a separate thread if no meanings are accumulated.
            self.perform_dictionary_lookup(current_word, lambda meanings: self.handle_lookup_results(matches, meanings))

        # Stop the progress bar once done (use Tkinter's `after` to ensure it runs in the main thread)
        self.root.after(0, self.stop_progress)

        if matches:
            print(f"Found matches for {current_word}: {matches}")
            # Ensure current_pankti and meanings are passed to show_matches
            self.root.after(0, lambda: self.show_matches(matches, self.current_pankti, meanings))
        else:
            self.root.after(0, lambda: messagebox.showinfo("No Matches", f"No matches found for the word: {current_word}"))
            self.current_word_index += 1
            self.root.after(0, self.process_next_word)

    def is_non_word_character(self, word):
        """
        Determines if the given word consists solely of non-word characters.

        Args:
        word (str): The word to be checked.

        Returns:
        bool: True if the word consists only of non-word characters; False otherwise.
        """
        # Regular expression pattern to match non-word characters and digits
        pattern = r"^[^\w\s]*[\d‡••]+[^\w\s]*$"

        # Check if the word matches the pattern
        return re.match(pattern, word) is not None

    def search_by_criteria(self, word, number, gender, pos):
        matches = []
        seen = set()  # To store unique combinations

        # Part of Speech: Noun, Verb
        if pos in ["Noun / ‡®®‡®æ‡®Ç‡®µ", "Verb / ‡®ï‡®ø‡®∞‡®ø‡®Ü"]:
            specified_endings = [
                "‡©å", "‡©ã", "‡©à", "‡©á", "‡©Ç", "‡©Å", "‡©Ä‡®π‡©ã", "‡©Ä‡®π‡©Ç", "‡©Ä‡®è", "‡©Ä‡®à‡®Ç", "‡©Ä‡®à",
                "‡©Ä‡®Ü", "‡©Ä‡®Ö‡©à", "‡©Ä‡®Ö‡®π‡©Å", "‡©Ä‡®ì", "‡©Ä‡®Ç", "‡©Ä", "‡®ø‡®®", "‡®ø‡®π‡©ã", "‡®ø‡®à‡®Ç", "‡®ø‡®Ü‡®Ç",
                "‡®ø‡®Ü", "‡®ø‡®Ö‡®®", "‡®ø‡®Ö‡®π‡©Å", "‡®ø", "‡®æ‡®∞‡©Ç", "‡®æ‡®π‡©Å", "‡®æ‡®π‡®ø", "‡®æ‡®Ç", "‡®æ", "‡®π‡®ø",
                "‡®∏‡©à", "‡®∏", "‡®à‡®¶‡®ø", "‡®à", "‡®â", "‡®π‡®ø‡®â", "‡®ó‡®æ", "‡®Ü", "‡®á"
            ]

            # Determine if the word is truly inflectionless
            is_inflectionless = all(not word.endswith(ending) for ending in specified_endings)

            # Iterate through each rule in the grammar data
            for rule in self.grammar_data:
                current_number = number if number != "NA" else rule['Number / ‡®µ‡®ö‡®®']
                current_gender = gender if gender != "NA" else rule['Gender / ‡®≤‡®ø‡©∞‡®ó']
                current_pos = pos if pos != "NA" else rule['Type']

                # Handle the '‡®Æ‡©Å‡®ï‡®§‡®æ' case
                include_mukta = is_inflectionless and current_pos == "Noun / ‡®®‡®æ‡®Ç‡®µ"

                if include_mukta and rule['\ufeffVowel Ending'] == "‡®Æ‡©Å‡®ï‡®§‡®æ" and rule['Number / ‡®µ‡®ö‡®®'] == current_number and rule['Gender / ‡®≤‡®ø‡©∞‡®ó'] == current_gender and rule['Type'] == current_pos:
                    result = " | ".join([
                        word,
                        rule.get('\ufeffVowel Ending', ""),
                        rule.get('Number / ‡®µ‡®ö‡®®', ""),
                        rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                        rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                        rule.get('Word Root', ""),
                        rule.get('Type', "")
                    ])
                    match_count, match_percentage = (1, 100.0)
                    matches.append((result, match_count, match_percentage))
                elif not include_mukta and rule['Number / ‡®µ‡®ö‡®®'] == current_number and rule['Gender / ‡®≤‡®ø‡©∞‡®ó'] == current_gender and rule['Type'] == current_pos:
                    # Regular inflection matching
                    inflections = rule['\ufeffVowel Ending'].split()
                    for inflection in inflections:
                        match_count, match_percentage = self.calculate_match_metrics(word, inflection)
                        if match_count > 0:
                            result = " | ".join([
                                word,
                                inflection,
                                rule.get('Number / ‡®µ‡®ö‡®®', ""),
                                rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                                rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                                rule.get('Word Root', ""),
                                rule.get('Type', "")
                            ])
                            matches.append((result, match_count, match_percentage))

        # Part of Speech: Adjective (Always perform both searches)
        elif pos == "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£":
            specified_endings = [
                "‡©å", "‡©ã", "‡©à", "‡©á", "‡©Ç", "‡©Å", "‡©Ä‡®π‡©ã", "‡©Ä‡®π‡©Ç", "‡©Ä‡®è", "‡©Ä‡®à‡®Ç", "‡©Ä‡®à",
                "‡©Ä‡®Ü", "‡©Ä‡®Ö‡©à", "‡©Ä‡®Ö‡®π‡©Å", "‡©Ä‡®ì", "‡©Ä‡®Ç", "‡©Ä", "‡®ø‡®®", "‡®ø‡®π‡©ã", "‡®ø‡®à‡®Ç", "‡®ø‡®Ü‡®Ç",
                "‡®ø‡®Ü", "‡®ø‡®Ö‡®®", "‡®ø‡®Ö‡®π‡©Å", "‡®ø", "‡®æ‡®∞‡©Ç", "‡®æ‡®π‡©Å", "‡®æ‡®π‡®ø", "‡®æ‡®Ç", "‡®æ", "‡®π‡®ø",
                "‡®∏‡©à", "‡®∏", "‡®à‡®¶‡®ø", "‡®à", "‡®â", "‡®π‡®ø‡®â", "‡®ó‡®æ", "‡®Ü", "‡®á"
            ]

            # Determine if the word is truly inflectionless
            is_inflectionless = all(not word.endswith(ending) for ending in specified_endings)

            for rule in self.grammar_data:
                current_number = number if number != "NA" else rule['Number / ‡®µ‡®ö‡®®']
                current_gender = gender if gender != "NA" else rule['Gender / ‡®≤‡®ø‡©∞‡®ó']
                current_pos = pos if pos != "NA" else rule['Type']

                # Handle the '‡®Æ‡©Å‡®ï‡®§‡®æ' case
                include_mukta = is_inflectionless and current_pos == "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£"

                # Handle inflections (like Nouns)
                if include_mukta and rule['\ufeffVowel Ending'] == "‡®Æ‡©Å‡®ï‡®§‡®æ" and rule['Number / ‡®µ‡®ö‡®®'] == current_number and rule['Gender / ‡®≤‡®ø‡©∞‡®ó'] == current_gender and rule['Type'] == current_pos:
                    result = " | ".join([
                        word,
                        rule.get('\ufeffVowel Ending', ""),
                        rule.get('Number / ‡®µ‡®ö‡®®', ""),
                        rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                        rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                        rule.get('Word Root', ""),
                        rule.get('Type', "")
                    ])
                    match_count, match_percentage = (1, 100.0)
                    matches.append((result, match_count, match_percentage))
                elif not include_mukta and rule['Number / ‡®µ‡®ö‡®®'] == current_number and rule['Gender / ‡®≤‡®ø‡©∞‡®ó'] == current_gender and rule['Type'] == current_pos:
                    inflections = rule['\ufeffVowel Ending'].split()
                    for inflection in inflections:
                        match_count, match_percentage = self.calculate_match_metrics(word, inflection)
                        if match_count > 0:
                            result = " | ".join([
                                word,
                                inflection,
                                rule.get('Number / ‡®µ‡®ö‡®®', ""),
                                rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                                rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                                rule.get('Word Root', ""),
                                rule.get('Type', "")
                            ])
                            matches.append((result, match_count, match_percentage))

                # Also check for exact matches (like Pronouns)
                if word in rule['\ufeffVowel Ending'] and rule['Number / ‡®µ‡®ö‡®®'] == current_number and rule['Gender / ‡®≤‡®ø‡©∞‡®ó'] == current_gender and rule['Type'] == current_pos:
                    result = " | ".join([
                        word,
                        rule.get('\ufeffVowel Ending', ""),
                        rule.get('Number / ‡®µ‡®ö‡®®', ""),
                        rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                        rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                        rule.get('Word Root', ""),
                        rule.get('Type', "")
                    ])
                    match_count, match_percentage = self.calculate_match_metrics(word, rule['\ufeffVowel Ending'])
                    matches.append((result, match_count, match_percentage))

        # Part of Speech: Pronoun
        elif pos == "Pronoun / ‡®™‡©ú‡®®‡®æ‡®Ç‡®µ":
            for rule in self.grammar_data:
                current_number = number if number != "NA" else rule['Number / ‡®µ‡®ö‡®®']
                current_gender = gender if gender != "NA" else rule['Gender / ‡®≤‡®ø‡©∞‡®ó']

                if word in rule['\ufeffVowel Ending'] and rule['Number / ‡®µ‡®ö‡®®'] == current_number and rule['Gender / ‡®≤‡®ø‡©∞‡®ó'] == current_gender and rule['Type'] == pos:
                    result = " | ".join([
                        word,
                        rule.get('\ufeffVowel Ending', ""),
                        rule.get('Number / ‡®µ‡®ö‡®®', ""),
                        rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                        rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                        rule.get('Word Root', ""),
                        rule.get('Type', "")
                    ])
                    match_count, match_percentage = self.calculate_match_metrics(word, rule['\ufeffVowel Ending'])
                    matches.append((result, match_count, match_percentage))

        # Part of Speech: Adverb, Postposition, Conjunction
        elif pos in ["Adverb / ‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡©á‡®∂‡®£", "Postposition / ‡®∏‡©∞‡®¨‡©∞‡®ß‡®ï", "Conjunction / ‡®Ø‡©ã‡®ú‡®ï", "Interjection / ‡®µ‡®ø‡®∏‡®Æ‡®ø‡®ï"]:
            for rule in self.grammar_data:
                if word in rule['\ufeffVowel Ending'] and rule['Type'] == pos:
                    result = " | ".join([
                        word,
                        rule.get('\ufeffVowel Ending', ""),
                        rule.get('Number / ‡®µ‡®ö‡®®', ""),
                        rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                        rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                        rule.get('Word Root', ""),
                        rule.get('Type', "")
                    ])
                    match_count, match_percentage = self.calculate_match_metrics(word, rule['\ufeffVowel Ending'])
                    matches.append((result, match_count, match_percentage))

        # Use filter_unique_matches to remove duplicates and sort the results
        unique_sorted_matches = self.filter_unique_matches(matches)

        return unique_sorted_matches

    def search_by_inflections(self, word):
        """
        Searches for inflection matches for the given word within the grammar data.

        Args:
        word (str): The word to search for inflections.

        Returns:
        list: A list of tuples representing matched grammatical rules for the word,
            along with match count and match percentage.
        """
        matches = []
        seen = set()  # To store unique combinations

        # Define the specified endings for inflectionless check
        specified_endings = [
            "‡©å", "‡©ã", "‡©à", "‡©á", "‡©Ç", "‡©Å", "‡©Ä‡®π‡©ã", "‡©Ä‡®π‡©Ç", "‡©Ä‡®è", "‡©Ä‡®à‡®Ç", "‡©Ä‡®à",
            "‡©Ä‡®Ü", "‡©Ä‡®Ö‡©à", "‡©Ä‡®Ö‡®π‡©Å", "‡©Ä‡®ì", "‡©Ä‡®Ç", "‡©Ä", "‡®ø‡®®", "‡®ø‡®π‡©ã", "‡®ø‡®à‡®Ç", "‡®ø‡®Ü‡®Ç",
            "‡®ø‡®Ü", "‡®ø‡®Ö‡®®", "‡®ø‡®Ö‡®π‡©Å", "‡®ø", "‡®æ‡®∞‡©Ç", "‡®æ‡®π‡©Å", "‡®æ‡®π‡®ø", "‡®æ‡®Ç", "‡®æ", "‡®π‡®ø",
            "‡®∏‡©à", "‡®∏", "‡®à‡®¶‡®ø", "‡®à", "‡®â", "‡®ì", "‡®π‡®ø‡®â", "‡®ó‡®æ", "‡®Ü", "‡®á"
        ]

        # Determine if the word is truly inflectionless
        try:
            is_inflectionless = all(not word.endswith(ending) for ending in specified_endings)
        except Exception as e:
            print(f"Error determining if the word '{word}' is inflectionless: {str(e)}")
            is_inflectionless = False  # Default to False if there's an error

        for rule in self.grammar_data:
            rule_pos = rule['Type']

            # Noun, Adjective, and Verb processing
            if rule_pos in ["Noun / ‡®®‡®æ‡®Ç‡®µ", "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£", "Verb / ‡®ï‡®ø‡®∞‡®ø‡®Ü"]:
                include_mukta = is_inflectionless and (rule_pos == "Noun / ‡®®‡®æ‡®Ç‡®µ" or rule_pos == "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£")

                if include_mukta and rule['\ufeffVowel Ending'] == "‡®Æ‡©Å‡®ï‡®§‡®æ":
                    # Handle the '‡®Æ‡©Å‡®ï‡®§‡®æ' case
                    result = " | ".join([
                        word,
                        rule.get('\ufeffVowel Ending', ""),
                        rule.get('Number / ‡®µ‡®ö‡®®', ""),
                        rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                        rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                        rule.get('Word Root', ""),
                        rule.get('Type', "")
                    ])
                    match_count, match_percentage = (1, 100.0)
                    matches.append((result, match_count, match_percentage))
                else:
                    # Regular inflection matching
                    inflections = rule['\ufeffVowel Ending'].split()
                    for inflection in inflections:
                        match_count, match_percentage = self.calculate_match_metrics(word, inflection)
                        if match_count > 0:
                            result = " | ".join([
                                word,
                                inflection,
                                rule.get('Number / ‡®µ‡®ö‡®®', ""),
                                rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                                rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                                rule.get('Word Root', ""),
                                rule.get('Type', "")
                            ])
                            matches.append((result, match_count, match_percentage))
                    # Hybrid handling for Adjectives
                    if rule_pos == "Adjectives / ‡®µ‡®ø‡®∂‡©á‡®∂‡®£" and word in rule['\ufeffVowel Ending']:
                        result = " | ".join([
                            word,
                            rule.get('\ufeffVowel Ending', ""),
                            rule.get('Number / ‡®µ‡®ö‡®®', ""),
                            rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                            rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                            rule.get('Word Root', ""),
                            rule.get('Type', "")
                        ])
                        match_count, match_percentage = self.calculate_match_metrics(word, rule['\ufeffVowel Ending'])
                        matches.append((result, match_count, match_percentage))

            # Pronoun processing
            elif rule_pos == "Pronoun / ‡®™‡©ú‡®®‡®æ‡®Ç‡®µ":
                if word in rule['\ufeffVowel Ending']:
                    result = " | ".join([
                        word,
                        rule.get('\ufeffVowel Ending', ""),
                        rule.get('Number / ‡®µ‡®ö‡®®', ""),
                        rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                        rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                        rule.get('Word Root', ""),
                        rule.get('Type', "")
                    ])
                    match_count, match_percentage = self.calculate_match_metrics(word, rule['\ufeffVowel Ending'])
                    matches.append((result, match_count, match_percentage))

            # Adverb, Postposition, and Conjunction processing
            elif rule_pos in ["Adverb / ‡®ï‡®ø‡®∞‡®ø‡®Ü ‡®µ‡®ø‡®∏‡©á‡®∂‡®£", "Postposition / ‡®∏‡©∞‡®¨‡©∞‡®ß‡®ï", "Conjunction / ‡®Ø‡©ã‡®ú‡®ï", "Interjection / ‡®µ‡®ø‡®∏‡®Æ‡®ø‡®ï"]:
                if word in rule['\ufeffVowel Ending']:
                    result = " | ".join([
                        word,
                        rule.get('\ufeffVowel Ending', ""),
                        rule.get('Number / ‡®µ‡®ö‡®®', ""),
                        rule.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', ""),
                        rule.get('Gender / ‡®≤‡®ø‡©∞‡®ó', ""),
                        rule.get('Word Root', ""),
                        rule.get('Type', "")
                    ])
                    match_count, match_percentage = self.calculate_match_metrics(word, rule['\ufeffVowel Ending'])
                    matches.append((result, match_count, match_percentage))

        # Use filter_unique_matches to remove duplicates and sort the results
        unique_sorted_matches = self.filter_unique_matches(matches)

        return unique_sorted_matches

    def analyze_pankti(self):
        """
        1) Get user‚Äôs typed verse/pankti.
        2) Fuzzy-match it against SGGS data.
        3) Show radio buttons for each match so user can select exactly one.
        """
        user_input = self.pankti_entry.get().strip()
        if not user_input:
            messagebox.showerror("Error", "Please enter some text to analyze.")
            return

        # 1) Fuzzy-match the verse in SGGS data (example function)
        candidate_matches = self.match_sggs_verse(user_input)
        if not candidate_matches:
            messagebox.showinfo("No SGGS Match", "No matching verses found in SGGS. Continuing with grammar analysis.")
            # If you still want to do grammar analysis with the typed input:
            self.finish_grammar_analysis(user_input)
            return
        else:
            # Show a new window with radio buttons
            self.show_sggs_matches_option(candidate_matches, user_input)

    def load_sggs_data(self):
        """
        Loads the SGGS data from an Excel file while displaying a modal progress bar.
        The main window is disabled (to prevent user interaction) while the heavy work runs in a background thread.
        The main thread enters a loop to update the UI (so the progress bar animates) until the data is loaded.
        """
        # Disable the main window to prevent interaction
        self.root.attributes("-disabled", True)

        # Show the progress bar modally on the main thread
        self.start_progress()
        self.root.update()  # Ensure the progress window appears immediately

        # This flag will be set when loading is complete.
        self.loading_done = False

        import threading

        def heavy_work():
            # Perform the heavy work (reading and processing the Excel file)
            data = pd.read_excel("1.1.3 sggs_extracted_with_page_numbers.xlsx")
            headers = list(data.columns)
            data['NormalizedVerse'] = (
                data['Verse']
                .astype(str)
                .str.lower()
                .str.strip()
            )
            # Schedule the finalization on the main thread.
            def finish():
                self.sggs_data = data
                self.sggs_headers = headers
                self.loading_done = True
            self.root.after(0, finish)

        # Run the heavy work in a background thread
        threading.Thread(target=heavy_work, daemon=True).start()

        # Process the event loop until loading is done (this allows the progress bar to animate)
        while not self.loading_done:
            self.root.update_idletasks()
            self.root.update()

        # Re-enable the main window now that the heavy work is complete
        self.root.attributes("-disabled", False)
        self.stop_progress()

    def match_sggs_verse(self, user_input, max_results=10, min_score=60):
        """
        Fuzzy-match the user's input (pankti) against the SGGS 'Verse' column.
        Return a tuple (headers, candidate_matches) where headers is the list
        of all column names from the Excel file, and candidate_matches is a list
        (up to max_results) of best matches above the min_score similarity.
        """
        # Ensure we have loaded the data and headers
        if not hasattr(self, 'sggs_data'):
            self.load_sggs_data()
        
        headers = self.sggs_headers
        normalized_input = user_input.lower().strip()
        candidate_matches = []

        # Disable the main window to block user interaction during matching
        self.root.attributes("-disabled", True)
        # Start the modal progress bar
        self.start_progress()
        self.root.update()  # Ensure progress window appears

        total_rows = len(self.sggs_data)
        # Iterate over the rows in SGGS data
        for i, (_, row) in enumerate(self.sggs_data.iterrows()):
            verse_text = row['NormalizedVerse']
            # Remove extra spaces around numbers within "‡••" markers
            verse_text = re.sub(r'‡••\s*(\d+)\s*‡••', r'‡••\1‡••', verse_text)
            score = fuzz.token_sort_ratio(normalized_input, verse_text)
            if score >= min_score:
                candidate_matches.append({
                    'S. No.': row['S. No.'],
                    'Verse': row['Verse'],
                    'Verse No.': row.get('Verse No.'),
                    'Stanza No.': row['Stanza No.'],
                    'Text Set No.': row.get('Text Set No.'),
                    'Raag (Fixed)': row['Raag (Fixed)'],
                    'Sub-Raag': row.get('Sub-Raag'),
                    'Writer (Fixed)': row['Writer (Fixed)'],
                    'Verse Configuration (Optional)': row.get('Verse Configuration (Optional)'),
                    'Stanza Configuration (Optional)': row.get('Stanza Configuration (Optional)'),
                    'Bani Name': row['Bani Name'],
                    'Musical Note Configuration': row.get('Musical Note Configuration'),
                    'Special Type Demonstrator': row.get('Special Type Demonstrator'),
                    'Type': row.get('Type'),
                    'Page Number': row['Page Number'],
                    'Score': score
                })
            if i % 50 == 0:
                self.root.update_idletasks()
                self.root.update()

        # Sort the candidate matches by descending score and select the top results.
        candidate_matches.sort(key=lambda x: x['Score'], reverse=True)

        # Stop the progress bar and re-enable the main window.
        self.stop_progress()
        self.root.attributes("-disabled", False)

        return headers, candidate_matches[:max_results]

    def show_sggs_matches_option(self, candidate_matches, user_input):
        """
        Display fuzzy SGGS matches as radio buttons so the user can choose one.
        The header will also include the user input verse.
        """

        # 1) If the first item is a header list, strip it off:
        if candidate_matches and isinstance(candidate_matches[0], list):
            # This is presumably the header row
            self.header_row = candidate_matches[0]
            # If there's a second element and it's a list of dicts, use that
            if len(candidate_matches) > 1 and isinstance(candidate_matches[1], list):
                candidate_matches = candidate_matches[1]
            else:
                # No real data after the header
                candidate_matches = []

        # 2) Store matches for later reference
        self.candidate_matches = candidate_matches

        # 3) Destroy if there's an existing option window
        if hasattr(self, 'sggs_option_window') and self.sggs_option_window.winfo_exists():
            self.sggs_option_window.destroy()

        # 4) Create a new Toplevel window
        self.sggs_option_window = tk.Toplevel(self.root)
        self.sggs_option_window.title("Select One Matching Verse")
        self.sggs_option_window.configure(bg='light gray')
        self.sggs_option_window.state("zoomed")

        # 5) A Tk variable that holds the index of the selected match
        self.sggs_option_var = tk.IntVar(value=-1)  # -1 = nothing selected

        # 6) Header label
        header_label = tk.Label(
            self.sggs_option_window,
            text=f"Fuzzy Matches Found for '{user_input}'. Please select one:",
            bg='dark slate gray',
            fg='white',
            font=('Arial', 16, 'bold'),
            pady=10
        )
        header_label.pack(fill=tk.X)

        # 7) Scrollable frame for radio buttons
        frame = tk.Frame(self.sggs_option_window, bg='light gray')
        frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)

        canvas = tk.Canvas(frame, bg='light gray', borderwidth=0)
        vsb = tk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scroll_frame = tk.Frame(canvas, bg='light gray')

        canvas.configure(yscrollcommand=vsb.set)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)

        canvas.create_window((0, 0), window=scroll_frame, anchor="nw")

        # 8) Populate the scroll_frame with radio buttons
        for idx, match in enumerate(candidate_matches):
            # We now expect match to be a dict. But if there's a chance
            # it might be something else, you can still do an isinstance check:
            if isinstance(match, dict):
                # Get the score value
                raw_score = match.get('Score', '?')
                # Attempt to convert it to float and format with 2 decimals
                try:
                    score_float = float(raw_score)
                    score_str = f"{score_float:.2f}"
                except ValueError:
                    # If it's not a valid float (e.g. '?'), just show it as is
                    score_str = str(raw_score)
                verse = match.get('Verse', '')
                raag = match.get('Raag (Fixed)', '')
                writer = match.get('Writer (Fixed)', '')
                bani = match.get('Bani Name', '')
                page = match.get('Page Number', '')
            else:
                # Fallback if it's not a dict, adjust indices to your structure
                score = match[7] if len(match) > 7 else '?'
                verse = match[1] if len(match) > 1 else ''
                raag = match[2] if len(match) > 2 else ''
                writer = match[3] if len(match) > 3 else ''
                bani = match[4] if len(match) > 4 else ''
                page = match[5] if len(match) > 5 else ''

            # Convert them to strings and handle 'nan' or empty strings
            def clean_value(val):
                if not val or str(val).lower() == 'nan':
                    return ''
                return str(val)

            raag = clean_value(raag)
            writer = clean_value(writer)
            bani = clean_value(bani)
            page = clean_value(page)

            # Build the info line with only non-empty fields
            info_parts = []
            if raag:
                info_parts.append(f"Raag: {raag}")
            if writer:
                info_parts.append(f"Writer: {writer}")
            if bani:
                info_parts.append(f"Bani: {bani}")
            if page:
                info_parts.append(f"Page: {page}")
            info_line = " | ".join(info_parts)

            # Build the label text
            label_text = f"Match #{idx+1} - Score: {score_str}%\nVerse: {verse}"
            if info_line:
                label_text += f"\n{info_line}"

            rb = tk.Radiobutton(
                scroll_frame, text=label_text,
                variable=self.sggs_option_var, value=idx,
                bg='light gray', anchor='w', justify=tk.LEFT,
                wraplength=800,  # adjust as needed
                font=('Arial', 12), selectcolor='light blue'
            )
            rb.pack(fill=tk.X, padx=10, pady=5)

        scroll_frame.update_idletasks()
        canvas.config(scrollregion=canvas.bbox("all"))

        # 9) Buttons
        btn_frame = tk.Frame(self.sggs_option_window, bg='light gray')
        btn_frame.pack(pady=10)

        tk.Button(
            btn_frame, text="Submit",
            command=self.handle_sggs_option_submit,
            font=('Arial', 12, 'bold'), bg='navy', fg='white',
            padx=20, pady=10
        ).pack(side=tk.LEFT, padx=10)

        tk.Button(
            btn_frame, text="Cancel",
            command=self.sggs_option_window.destroy,
            font=('Arial', 12, 'bold'), bg='red', fg='white',
            padx=20, pady=10
        ).pack(side=tk.LEFT, padx=10)

        print("Match window created and populated.")

    def handle_sggs_option_submit(self):
        idx = self.sggs_option_var.get()
        if idx < 0:
            messagebox.showwarning("No Selection", "Please select one verse match.")
            return

        chosen_match = self.candidate_matches[idx]
        final_input = chosen_match.get('Verse', '')

        if not final_input.strip():
            messagebox.showerror("Error", "The selected verse text is empty. Cannot proceed.")
            return

        # Optionally close the first match window
        self.sggs_option_window.destroy()

        # Now, show the consecutive lines selection
        self.show_consecutive_verses_option(chosen_match, final_input)

    def show_consecutive_verses_option(self, chosen_match, main_verse_text):
        """
        After the user picks the main verse from fuzzy matches,
        let them select additional consecutive lines from the same stanza or text set.
        Ensure the main verse is highlighted and only consecutive verses can be selected.
        """
        # 1) Query your data to find consecutive lines
        stanza_lines = self.fetch_stanza_lines(chosen_match)
        self.chosen_match = stanza_lines
        if not stanza_lines:
            messagebox.showinfo("No Consecutive Lines", "No additional consecutive verses found. Proceeding.")
            self.finish_grammar_analysis(main_verse_text)
            return

        # 2) Store the main user input so that we can return to SGGS matches later.
        self.last_user_input = main_verse_text

        # 3) Create a window to show these lines as checkboxes
        self.consecutive_window = tk.Toplevel(self.root)
        self.consecutive_window.title("Select Consecutive Verses")
        self.consecutive_window.configure(bg='light gray')
        self.consecutive_window.state('zoomed')

        # Adjust header text based on the stored user's choice.
        if hasattr(self, 'verses_choice') and self.verses_choice:
            header_text = f"Select Additional Lines from the '{chosen_match['Special Type Demonstrator']}'"
        else:
            header_text = "Select Additional Lines from the Stanza"

        header_label = tk.Label(
            self.consecutive_window,
            text=header_text,
            bg='dark slate gray',
            fg='white',
            font=('Arial', 16, 'bold'),
            pady=10
        )
        header_label.pack(fill=tk.X)

        frame = tk.Frame(self.consecutive_window, bg='light gray')
        frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=10)

        canvas = tk.Canvas(frame, bg='light gray', borderwidth=0)
        vsb = tk.Scrollbar(frame, orient="vertical", command=canvas.yview)
        scroll_frame = tk.Frame(canvas, bg='light gray')

        canvas.configure(yscrollcommand=vsb.set)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)

        canvas.create_window((0, 0), window=scroll_frame, anchor="nw")

        # Populate with checkboxes for each consecutive line.
        # Highlight the main verse (pre-selected and disabled) and add normal checkboxes for others.
        self.stanza_checkvars = []
        for idx, line_info in enumerate(stanza_lines):
            line_text = line_info.get('Verse', '')
            if line_text.strip() == main_verse_text.strip():
                # Highlight the main verse
                var = tk.BooleanVar(value=True)
                chk = tk.Checkbutton(
                    scroll_frame,
                    text=line_text,
                    variable=var,
                    bg='yellow',           # highlight color
                    font=('Arial', 12, 'bold'),
                    anchor='w',
                    justify=tk.LEFT,
                    wraplength=800,
                    state='disabled',      # force it to remain selected
                    selectcolor='light blue'
                )
            else:
                var = tk.BooleanVar(value=False)
                chk = tk.Checkbutton(
                    scroll_frame,
                    text=line_text,
                    variable=var,
                    bg='light gray',
                    font=('Arial', 12),
                    anchor='w',
                    justify=tk.LEFT,
                    wraplength=800,
                    selectcolor='light blue'
                )
            chk.pack(fill=tk.X, padx=10, pady=5)
            self.stanza_checkvars.append((var, line_text))

        scroll_frame.update_idletasks()
        canvas.config(scrollregion=canvas.bbox("all"))

        # ---------------------------
        # Button Frame: Submit, Cancel, and Back
        # ---------------------------
        btn_frame = tk.Frame(self.consecutive_window, bg='light gray')
        btn_frame.pack(pady=10)

        tk.Button(
            btn_frame,
            text="Submit",
            command=lambda: self.validate_and_submit_consecutive(main_verse_text),
            font=('Arial', 12, 'bold'),
            bg='navy',
            fg='white',
            padx=20,
            pady=10
        ).pack(side=tk.LEFT, padx=10)

        tk.Button(
            btn_frame,
            text="Cancel",
            command=self.consecutive_window.destroy,
            font=('Arial', 12, 'bold'),
            bg='red',
            fg='white',
            padx=20,
            pady=10
        ).pack(side=tk.LEFT, padx=10)

        tk.Button(
            btn_frame,
            text="Back",
            command=self.back_to_sggs_matches_option,
            font=('Arial', 12, 'bold'),
            bg='gray',
            fg='white',
            padx=20,
            pady=10
        ).pack(side=tk.LEFT, padx=10)

        print("Consecutive verses window created and populated.")

    def validate_and_submit_consecutive(self, main_verse_text):
        """
        Validate that the selected verses form a consecutive block that includes the main verse.
        If valid, proceed with handling the submission.
        """
        # Get indices of all selected verses
        selected_indices = [i for i, (var, _) in enumerate(self.stanza_checkvars) if var.get()]
        if not selected_indices:
            messagebox.showwarning("No Selection", "Please ensure at least the main verse is selected.")
            return

        # Find the index of the main verse (which should be pre-selected)
        main_indices = [i for i, (_, line_text) in enumerate(self.stanza_checkvars)
                        if line_text.strip() == main_verse_text.strip()]
        if not main_indices:
            messagebox.showerror("Error", "Main verse not found in the list.")
            return
        main_index = main_indices[0]

        # Check if the selected indices form a consecutive block.
        if max(selected_indices) - min(selected_indices) != len(selected_indices) - 1:
            messagebox.showerror("Non-Consecutive Selection", "Please select only consecutive verses.")
            return

        # Ensure the consecutive block includes the main verse.
        if main_index < min(selected_indices) or main_index > max(selected_indices):
            messagebox.showerror("Selection Error", "The selected consecutive block must include the main verse.")
            return

        # If validation passes, proceed with the submission.
        self.handle_consecutive_submit(main_verse_text)

    def back_to_sggs_matches_option(self):
        """
        Close the consecutive verses window and re-display the SGGS matches option window.
        """
        self.consecutive_window.destroy()
        # Re-open the SGGS matches option window using the stored candidate matches and user input.
        self.show_sggs_matches_option(self.candidate_matches, self.last_user_input)

    def handle_consecutive_submit(self, main_verse_text):
        """
        Combine the main verse text with the lines selected by the user,
        then proceed with grammar analysis. Ensures that the main verse is
        not duplicated if it was also selected by the user.
        Also, store the individual selected verses in self.selected_verses 
        (as a list of tuples: (start_index, end_index, verse_text)) for later use.
        """
        # Strip the main verse text
        main_line = main_verse_text.strip()

        # Gather the lines that were checked; strip extra whitespace
        selected_lines = [line.strip() for var, line in self.stanza_checkvars if var.get()]

        # Store each selected line as an individual verse in self.selected_verses.
        self.selected_verses = list(selected_lines)

        # Keep only matches where the verse is in self.selected_verses.
        self.chosen_match = [match for match in self.chosen_match if match['Verse'] in self.selected_verses]

        # Filter out any line that is exactly the same as the main verse
        extra_lines = [line for line in selected_lines if line != main_line]

        # Combine all selected lines (including the main verse, already in correct order) into a single string,
        # ensuring the original sequence of the verse is maintained.
        if extra_lines:
            combined_text = " ".join(selected_lines)
        else:
            combined_text = main_line
        
        # Close the consecutive window
        self.consecutive_window.destroy()

        # Proceed with grammar analysis using the combined text
        self.finish_grammar_analysis(combined_text)

    def fetch_stanza_lines(self, chosen_match):
        """
        Return a list of dictionaries representing consecutive lines from either the current stanza or the entire text set,
        based on the user's choice.
        This function uses the SGGS data loaded by load_sggs_data() (stored in self.sggs_data).
        """
        # Ensure the SGGS data is loaded
        if not hasattr(self, 'sggs_data'):
            self.load_sggs_data()

        # Retrieve the Stanza No. and Text Set No. from the chosen match.
        stanza_no = chosen_match.get('Stanza No.')
        text_set_no = chosen_match.get('Text Set No.')

        if stanza_no is None or text_set_no is None:
            print("Chosen match does not contain required 'Stanza No.' or 'Text Set No.' information.")
            return []

        # Get the Special Type Demonstrator value.
        special_type = chosen_match.get('Special Type Demonstrator', '')

        # If it's '‡®∂‡®≤‡©ã‡®ï', then we don't ask the user because a ‡®∂‡®≤‡©ã‡®ï is always a stanza.
        if special_type == '‡®∂‡®≤‡©ã‡®ï':
            choice = False
        else:
            choice = messagebox.askyesno(
                "Select Verses",
                f"Do you want to fetch verses from the entire '{special_type}'?\n\n"
                f"(Yes = Entire '{special_type}', No = Only the current Stanza)"
            )
        # Store the user's choice for later use
        self.verses_choice = choice

        if choice:
            # User selected the entire text set.
            subset = self.sggs_data[self.sggs_data['Text Set No.'] == text_set_no]
        else:
            # User selected only the current stanza.
            subset = self.sggs_data[
                (self.sggs_data['Stanza No.'] == stanza_no) &
                (self.sggs_data['Text Set No.'] == text_set_no)
            ]

        # Optionally sort by 'Verse No.' if available
        if 'Verse No.' in subset.columns:
            subset = subset.sort_values(by='Verse No.')

        lines = subset.to_dict(orient='records')

        return lines

    def finish_grammar_analysis(self, user_input):
        """
        Runs grammar analysis after the user has selected or typed a final verse.
        """
        self._repeat_note_shown = set()
        self._suppress_repeat_notes_for_verse = False
        self.pankti_words = user_input.split()
        self.accumulate_pankti_data(user_input)
        self.current_word_index = 0
        self.all_new_entries = []  # Reset global accumulator

        self.update_navigation_buttons()
        self.process_next_word()

    def prompt_for_assessment(self, metadata_entry):
        """
        Opens a modal window that lets the user paste the analysis result (translation)
        and choose options for 'Framework?' and 'Explicit?'.
        The metadata_entry is a dictionary containing the existing metadata (e.g., Word, Vowel Ending, etc.).
        """
        assessment_win = tk.Toplevel(self.root)
        assessment_win.title("Enter Translation Assessment")
        assessment_win.configure(bg='light gray')

        instruction_label = tk.Label(assessment_win, 
                                    text="Paste the analysis result below:",
                                    font=("Helvetica", 14), bg="light gray")
        instruction_label.pack(pady=10)

        analysis_text = scrolledtext.ScrolledText(assessment_win, width=80, height=10,
                                                font=("Helvetica", 12), wrap=tk.WORD)
        analysis_text.pack(padx=20, pady=10)

        cb_frame = tk.Frame(assessment_win, bg="light gray")
        cb_frame.pack(pady=10)

        framework_var = tk.BooleanVar()
        explicit_var = tk.BooleanVar()

        framework_cb = tk.Checkbutton(cb_frame, text="Framework?", variable=framework_var,
                                    font=("Helvetica", 12), bg="light gray")
        framework_cb.pack(side=tk.LEFT, padx=10)

        explicit_cb = tk.Checkbutton(cb_frame, text="Explicit?", variable=explicit_var,
                                    font=("Helvetica", 12), bg="light gray")
        explicit_cb.pack(side=tk.LEFT, padx=10)

        def on_save():
            translation = analysis_text.get("1.0", tk.END).strip()
            if not translation:
                messagebox.showerror("Error", "Please paste the analysis result.")
                return
            # Merge the metadata with the new assessment data
            new_entry = metadata_entry.copy()
            new_entry["\ufeffVowel Ending"] = self._norm_get(new_entry, "\ufeffVowel Ending")
            new_entry.pop("Vowel Ending", None)
            new_entry["Type"] = self._norm_get(new_entry, "Type")
            new_entry.pop("Word Type", None)
            new_entry["Translation"] = translation
            new_entry["Framework?"] = framework_var.get()
            new_entry["Explicit?"] = explicit_var.get()
            # Revision is computed in save_assessment_data
            self.save_assessment_data(new_entry)
            assessment_win.destroy()

        save_btn = tk.Button(assessment_win, text="Save Assessment",
                            command=on_save, font=("Helvetica", 14, "bold"),
                            bg="#007acc", fg="white", padx=20, pady=10)
        save_btn.pack(pady=20)

        assessment_win.transient(self.root)
        assessment_win.grab_set()
        self.root.wait_window(assessment_win)

    def load_existing_assessment_data(self, file_path):
        expected_columns = [
            "Verse", "Translation", "Translation Revision",
            "Word", "Selected Darpan Meaning", "\ufeffVowel Ending", "Number / ‡®µ‡®ö‡®®", "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£", "Gender / ‡®≤‡®ø‡©∞‡®ó", "Word Root", "Type", "Grammar Revision", "Word Index",
            "S. No.", "Verse No.", "Stanza No.", "Text Set No.", "Raag (Fixed)", "Sub-Raag", "Writer (Fixed)",
            "Verse Configuration (Optional)", "Stanza Configuration (Optional)", "Bani Name", "Musical Note Configuration",
            "Special Type Demonstrator", "Verse Type", "Page Number",
            "Framework?", "Explicit?"
        ]
        if os.path.exists(file_path):
            try:
                df = pd.read_excel(file_path)
                df.rename(columns={"Vowel Ending": "\ufeffVowel Ending", "Word Type": "Type"}, inplace=True)
                if df.empty or len(df.columns) == 0:
                    df = pd.DataFrame(columns=expected_columns)
                else:
                    # Ensure the DataFrame has exactly the expected columns.
                    df = df.reindex(columns=expected_columns)
                return df
            except Exception as e:
                print(f"Error reading {file_path}: {e}")
        return pd.DataFrame(columns=expected_columns)

    def save_assessment_data(self, new_entry):
        """
        Saves a new assessment entry to an Excel file with the following behavior:
        - For a given verse, update the "Translation" field for all entries.
        - For the specific word being assessed in that verse, group all matching rows.
        - If any grammar field differs from the new entry, increment the Grammar Revision once for the group
            and update all matching rows with the new grammar details.
        - Update "Selected Darpan Meaning" for the specific word occurrence.
        - If any word's grammar is revised in the verse, increment the "Translation Revision" for all words of that verse.
        - If no matching entry exists for the word in that verse, append the new entry with a Grammar Revision of 1,
            and initialize the Translation Revision.
        """
        file_path = "1.2.1 assessment_data.xlsx"
        df_existing = self.load_existing_assessment_data(file_path)
        
        # Define the grammar keys to compare (excluding Translation, which is updated separately).
        grammar_keys = [
            '\ufeffVowel Ending', 'Number / ‡®µ‡®ö‡®®', 'Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£',
            'Gender / ‡®≤‡®ø‡©∞‡®ó', 'Word Root', 'Type'
        ]
        
        # Update the Translation for all rows in the same verse.
        df_existing.loc[df_existing["Verse"] == new_entry["Verse"], "Translation"] = new_entry["Translation"]
        
        # Filter for rows with the same word, same verse, and same word index.
        matching_rows = df_existing[
            (df_existing["Word"] == new_entry["Word"]) &
            (df_existing["Verse"] == new_entry["Verse"]) &
            (df_existing["Word Index"] == new_entry["Word Index"])
        ]
        
        if not matching_rows.empty:
            # For repeated occurrences, pick the highest Grammar Revision as a representative.
            latest_idx = matching_rows["Grammar Revision"].idxmax()
            latest_row = df_existing.loc[latest_idx]
            
            # Check if any grammar field is different.
            differences = any(new_entry.get(key) != self._norm_get(latest_row, key) for key in grammar_keys)
            
            if differences:
                # Compute new Grammar Revision number for the group.
                new_grammar_revision = matching_rows["Grammar Revision"].max() + 1
                new_entry["Grammar Revision"] = new_grammar_revision
                
                # Update all matching rows with new grammar values and new Grammar Revision.
                for idx in matching_rows.index:
                    for key in grammar_keys:
                        df_existing.at[idx, key] = new_entry.get(key)
                    df_existing.at[idx, "Grammar Revision"] = new_grammar_revision
                    # Update additional fields from new_entry if needed.
                    for key, value in new_entry.items():
                        if key not in grammar_keys and key not in ["Translation"]:
                            if key in ("Framework?", "Explicit?"):
                                df_existing.at[idx, key] = int(value)  # Cast Boolean to int (False->0, True->1)
                            else:
                                df_existing.at[idx, key] = value
                # Update Selected Darpan Meaning for these rows.
                for idx in matching_rows.index:
                    # --- Compute correct global index for the current word using the current verse ---
                    current_verse_words = self.accumulated_pankti.split()

                    def find_sublist_index(haystack, needle):
                        for i in range(len(haystack) - len(needle) + 1):
                            if haystack[i:i+len(needle)] == needle:
                                return i
                        return -1

                    start_index = find_sublist_index(self.pankti_words, current_verse_words)
                    if start_index == -1:
                        start_index = 0

                    # Assume new_entry["Word Index"] is the local index within the verse.
                    global_index = start_index + new_entry.get("Word Index", 0)

                    # Retrieve the selected Darpan meaning using the global index.
                    if len(self.accumulated_meanings) > global_index:
                        acc_entry = self.accumulated_meanings[global_index]
                        if isinstance(acc_entry, dict):
                            selected_meaning = "| ".join(acc_entry.get("meanings", []))
                        else:
                            selected_meaning = "| ".join(acc_entry)
                    else:
                        selected_meaning = ""

                    df_existing.at[idx, "Selected Darpan Meaning"] = selected_meaning

                # Now update the Translation Revision for all rows in the verse 
                # to reflect the latest Grammar Revision (without extra +1).
                verse_mask = df_existing["Verse"] == new_entry["Verse"]
                latest_grammar_revision = df_existing.loc[verse_mask, "Grammar Revision"].max()
                df_existing.loc[verse_mask, "Translation Revision"] = latest_grammar_revision
            else:
                # No differences detected; no update necessary.
                return
        else:
            # No existing entry for this word in the verse; add it with Grammar Revision 1 as well as Translation Revision 1.
            new_entry["Grammar Revision"] = 1
            new_entry["Translation Revision"] = 1

            # --- Compute correct global index for the current word using the current verse ---
            current_verse_words = self.accumulated_pankti.split()
            
            def find_sublist_index(haystack, needle):
                for i in range(len(haystack) - len(needle) + 1):
                    if haystack[i:i+len(needle)] == needle:
                        return i
                return -1

            start_index = find_sublist_index(self.pankti_words, current_verse_words)
            if start_index == -1:
                start_index = 0

            # Assume new_entry["Word Index"] holds the local index within the current verse.
            global_index = start_index + new_entry.get("Word Index", 0)

            # Retrieve the selected Darpan meaning using the global index.
            if len(self.accumulated_meanings) > global_index:
                acc_entry = self.accumulated_meanings[global_index]
                if isinstance(acc_entry, dict):
                    selected_meaning = "| ".join(acc_entry.get("meanings", []))
                else:
                    selected_meaning = "| ".join(acc_entry)
            else:
                selected_meaning = ""

            new_entry["Selected Darpan Meaning"] = selected_meaning

            # Determine Translation Revision for the verse.
            current_translation_revision = df_existing[df_existing["Verse"] == new_entry["Verse"]]["Translation Revision"].max()
            new_entry["Translation Revision"] = (current_translation_revision + 1) if current_translation_revision is not None else 1

            # Append new_entry to the DataFrame.
            df_existing = pd.concat([df_existing, pd.DataFrame([new_entry])], ignore_index=True)


        try:
            df_existing.to_excel(file_path, index=False)
        except Exception as e:
            messagebox.showerror("Error", f"Failed to save assessment data: {e}")

    def setup_options(self, parent_frame, label_text, options, variable):
        """
        Sets up radio button options in the specified parent frame.

        Args:
        parent_frame (tk.Frame): The frame in which to pack the radio buttons.
        label_text (str): The label text to display above the radio buttons.
        options (list of tuple): A list of tuples where each tuple contains the option text and the value to set in the variable.
        variable (tk.StringVar): The control variable for the group of radio buttons.
        """
        # Create a label for the option group with styling
        tk.Label(parent_frame, text=label_text, bg='light gray', font=('Arial', 12)).pack(pady=(10, 5))

        # Create radio buttons for each option
        for opt_text, opt_value in options:
            tk.Radiobutton(
                parent_frame,
                text=opt_text,
                variable=variable,
                value=opt_value,
                bg='light gray',
                selectcolor='light blue',
                anchor='w',
                font=('Arial', 11)
            ).pack(anchor='w', padx=20, pady=2)

    def navigation_controls(self, parent_frame):
        """
        Add navigation controls to the parent frame to navigate through words.
        """
        nav_frame = tk.Frame(parent_frame, bg='light gray')
        nav_frame.pack(pady=10)

        tk.Button(nav_frame, text="Previous", command=self.previous_word, bg='dark gray', fg='white').pack(side='left', padx=5)
        tk.Button(nav_frame, text="Next", command=self.next_word, bg='dark gray', fg='white').pack(side='left', padx=5)

    def update_navigation_buttons(self):
        """Enable or disable navigation buttons based on the current word index."""
        print(f"Updating buttons, current index: {self.current_word_index}, total words: {len(self.pankti_words)}")
        
        # Disable 'Previous' button if at the start
        if self.current_word_index <= 0:
            self.prev_button.config(state=tk.DISABLED)
        else:
            self.prev_button.config(state=tk.NORMAL)
        
        # Disable 'Next' button if at the end
        if self.current_word_index >= len(self.pankti_words) - 1:
            self.next_button.config(state=tk.DISABLED)
        else:
            self.next_button.config(state=tk.NORMAL)

    def update_current_word_label(self):
        """Update the word label to display the current word."""
        if hasattr(self, 'pankti_words') and self.pankti_words:
            # Clamp the index to valid range:
            if self.current_word_index < 0:
                self.current_word_index = 0
            if self.current_word_index >= len(self.pankti_words):
                self.current_word_index = len(self.pankti_words) - 1
            current_word = self.pankti_words[self.current_word_index]
            self.word_label.config(text=current_word)
        else:
            self.word_label.config(text="No Word Available")

    def prev_word(self):
        if self.current_word_index > 0:
            self.current_word_index -= 1
            self.update_current_word_label()
        self.update_navigation_buttons()

    def next_word(self):
        if self.current_word_index < len(self.pankti_words) - 1:
            self.current_word_index += 1
            self.update_current_word_label()
        self.update_navigation_buttons()

    def select_current_word(self):
        """Trigger analysis for the currently displayed word."""
        if hasattr(self, 'pankti_words') and self.pankti_words:
            # Ensure current_word_index is within range:
            if self.current_word_index >= len(self.pankti_words):
                self.current_word_index = len(self.pankti_words) - 1
            word = self.pankti_words[self.current_word_index]
            self.fetch_data(word, " ".join(self.pankti_words))
        else:
            print("No word available for selection.")

    def close_window(self, window):
        """Closes the given Tkinter window."""
        if window and window.winfo_exists():
            window.destroy()

    def reset_input_variables(self):
        """Reset input variables for number, gender, and part of speech."""
        self.number_var.set("NA")
        self.gender_var.set("NA")
        self.pos_var.set("NA")

    def compose_clipboard_text_for_chatgpt(self):
        clipboard_text = "### Detailed Analysis & Literal Translation\n\n"
        clipboard_text += (
            f"The verse **'{self.accumulated_pankti}'** holds deep meaning. Below is a breakdown of each word with "
            "user-selected meanings and grammar details, which together form the basis for a literal translation prompt.\n\n"
        )
        
        # --- Preceding Verses & Translations ---
        # Load existing assessment data.
        existing_data = self.load_existing_assessment_data("1.2.1 assessment_data.xlsx")
        
        # Identify the candidate matching the current verse.
        current_candidate = next((cand for cand in self.chosen_match 
                                if cand.get("Verse", "").strip() == self.accumulated_pankti.strip()), None)
        
        preceding_verses_text = ""
        if current_candidate:
            text_set_no = current_candidate.get("Text Set No.")
            try:
                current_verse_no = int(current_candidate.get("Verse No."))
            except (ValueError, TypeError):
                current_verse_no = None
            
            if current_verse_no is not None:
                # Filter for the same Text Set No.
                filtered_data = existing_data[existing_data["Text Set No."] == text_set_no]
                consecutive_verses = []
                target_verse_no = current_verse_no - 1
                # Collect consecutive preceding verses.
                while True:
                    row = filtered_data[filtered_data["Verse No."] == target_verse_no]
                    if row.empty:
                        break
                    row_data = row.iloc[0]
                    consecutive_verses.insert(0, row_data)  # earlier verses first
                    target_verse_no -= 1
                
                if consecutive_verses:
                    preceding_verses_text += "\n### Preceding Verses & Translations\n\n"
                    for row_data in consecutive_verses:
                        verse_no = row_data.get("Verse No.", "")
                        verse_text = row_data.get("Verse", "")
                        translation = row_data.get("Translation", "")
                        preceding_verses_text += f"**Verse {verse_no}:** {verse_text}\n"
                        preceding_verses_text += f"**Translation:** {translation}\n\n"
        
        clipboard_text += preceding_verses_text
        
        # --- Current Verse Analysis ---
        current_verse_words = self.accumulated_pankti.split()
        
        def find_sublist_index(haystack, needle):
            # Find a consecutive occurrence of 'needle' in 'haystack'
            for i in range(len(haystack) - len(needle) + 1):
                if haystack[i:i+len(needle)] == needle:
                    return i
            return -1

        start_index = find_sublist_index(self.pankti_words, current_verse_words)
        if start_index == -1:
            start_index = 0  # Fallback if no match is found

        for i, word in enumerate(current_verse_words):
            actual_index = start_index + i
            clipboard_text += f"**Word {i+1}: {word}**\n"
            
            # Retrieve the entry for the current word (if available)
            acc_entry = self.accumulated_meanings[actual_index] if actual_index < len(self.accumulated_meanings) else {}
            # If the entry is a dictionary, extract the 'meanings' list; otherwise, assume it's already a list
            if isinstance(acc_entry, dict):
                meanings_list = acc_entry.get("meanings", [])
            else:
                meanings_list = acc_entry
            # Create a string of meanings, or a default message if none are available
            meanings_str = ", ".join(meanings_list) if meanings_list else "No user-selected meanings available"
            clipboard_text += f"- **User-Selected Meanings:** {meanings_str}\n"
            
            clipboard_text += "- **Grammar Options:**\n"
            finalized_matches_list = self.accumulated_finalized_matches[actual_index] if actual_index < len(self.accumulated_finalized_matches) else []
            
            if finalized_matches_list:
                for option_idx, match in enumerate(finalized_matches_list, start=1):
                    clipboard_text += (
                        f"  - **Option {option_idx}:**\n"
                        f"      - **Word:** {self._norm_get(match, 'Word') or 'N/A'}\n"
                        f"      - **Vowel Ending:** {self._norm_get(match, '\\ufeffVowel Ending') or 'N/A'}\n"
                        f"      - **Number / ‡®µ‡®ö‡®®:** {match.get('Number / ‡®µ‡®ö‡®®', 'N/A')}\n"
                        f"      - **Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£:** {match.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', 'N/A')}\n"
                        f"      - **Gender / ‡®≤‡®ø‡©∞‡®ó:** {match.get('Gender / ‡®≤‡®ø‡©∞‡®ó', 'N/A')}\n"
                        f"      - **Word Root:** {match.get('Word Root', 'N/A')}\n"
                        f"      - **Type:** {self._norm_get(match, 'Type') or 'N/A'}\n"
                    )
                    clipboard_text += (
                        f"      - **Literal Translation (Option {option_idx}):** The word '{word}' functions as a "
                        f"'{self._norm_get(match, 'Type') or 'N/A'}' with '{match.get('Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£', 'N/A')}' usage, in the "
                        f"'{match.get('Number / ‡®µ‡®ö‡®®', 'N/A')}' form and '{match.get('Gender / ‡®≤‡®ø‡©∞‡®ó', 'N/A')}' gender. Translation: ‚Ä¶\n"
                    )
            else:
                clipboard_text += "  - No finalized grammar options available\n"
            
            clipboard_text += "\n"
        
        if '‡••' in current_verse_words:
            clipboard_text += (
                "**Symbol:** ‡••\n"
                "- **Meaning:** End of verse or sentence\n"
                "- **Context:** Denotes the conclusion of the verse.\n\n"
            )
        
        clipboard_text += "\n### Literal Translation Prompt\n"
        clipboard_text += (
            f"Using the above user-selected meanings and grammar details for the verse '{self.accumulated_pankti}', "
            "please generate a literal translation that adheres strictly to the grammatical structure, "
            "capturing the tense, number, gender, and function accurately."
        )
        
        return clipboard_text

    def prompt_copy_to_clipboard(self):
        print("Prompting to copy text to clipboard...")
        copy_prompt = messagebox.askyesno(
            "Copy to Clipboard", 
            f"Would you like to copy the detailed analysis for the verse '{self.accumulated_pankti}' to your clipboard?"
        )
        if copy_prompt:
            if not self.accumulated_pankti or not self.accumulated_meanings or not self.accumulated_grammar_matches or not self.accumulated_finalized_matches:
                print("Error: Accumulated data is not populated.")
                messagebox.showerror("Error", "Failed to copy data to clipboard: Data not populated.")
            else:
                clipboard_text = self.compose_clipboard_text_for_chatgpt()
                pyperclip.copy(clipboard_text)
                messagebox.showinfo("Copied", "The analysis has been copied to the clipboard!")
                print("Clipboard text copied successfully!")

    def prompt_for_final_grammar(self, word_entries):
        """
        Opens a modal window showing all grammar options for a given word.
        Generates a prompt text for ChatGPT to help finalize the grammar choice,
        copies it to the clipboard (with a button to re-copy if needed), and then
        allows the user to select the final applicable grammar.
        
        word_entries: a list of dictionaries (each corresponding to one grammar option for the word)
        """
        final_choice = {}

        final_win = tk.Toplevel(self.root)
        final_win.title(f"Finalize Grammar for '{word_entries[0]['Word']}'")
        final_win.configure(bg='light gray')

        # --- Build the ChatGPT prompt text ---
        prompt_lines = []
        prompt_lines.append(f"Finalise the applicable grammar for the word: {word_entries[0]['Word']}")
        prompt_lines.append("The following grammar options are available:")
        for idx, entry in enumerate(word_entries, start=1):
            summary = " | ".join([
                self._norm_get(entry, "\ufeffVowel Ending") or "",
                self._norm_get(entry, "Number / ‡®µ‡®ö‡®®") or "",
                self._norm_get(entry, "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£") or "",
                self._norm_get(entry, "Gender / ‡®≤‡®ø‡©∞‡®ó") or "",
                self._norm_get(entry, "Word Root") or "",
                self._norm_get(entry, "Type") or ""
            ])
            prompt_lines.append(f"Option {idx}: {summary}")
        prompt_text = "\n".join(prompt_lines)
        # --- End building prompt text ---

        # --- Display the prompt text and add a copy button ---
        prompt_frame = tk.Frame(final_win, bg="light gray")
        prompt_frame.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)

        prompt_label = tk.Label(prompt_frame, 
                                text="ChatGPT Prompt for Grammar Finalisation:",
                                font=("Helvetica", 14, "bold"),
                                bg="light gray")
        prompt_label.pack(anchor="w", pady=(0,5))

        prompt_text_widget = scrolledtext.ScrolledText(prompt_frame, width=80, height=6,
                                                        font=("Helvetica", 12), wrap=tk.WORD)
        prompt_text_widget.pack(fill=tk.BOTH, expand=True)
        prompt_text_widget.insert(tk.END, prompt_text)
        prompt_text_widget.config(state=tk.DISABLED)

        def copy_prompt():
            self.root.clipboard_clear()
            self.root.clipboard_append(prompt_text)
            messagebox.showinfo("Copied", "Prompt text copied to clipboard!")

        copy_btn = tk.Button(prompt_frame, text="Copy Prompt", command=copy_prompt,
                            font=("Helvetica", 12, "bold"),
                            bg="#007acc", fg="white", padx=10, pady=5)
        copy_btn.pack(anchor="e", pady=5)
        # --- End prompt display ---

        # Instruction for selecting final grammar
        instruction = tk.Label(final_win,
                            text="Multiple grammar options found for this word.\nPlease select the final applicable grammar:",
                            font=("Helvetica", 14),
                            bg="light gray")
        instruction.pack(pady=10)

        # Tk variable to hold the selected option index
        choice_var = tk.IntVar(value=0)

        # --- Create a scrollable area for the radio buttons ---
        options_container = tk.Frame(final_win, bg="light gray")
        options_container.pack(padx=20, pady=10, fill=tk.BOTH, expand=True)

        # Create a canvas in the container
        canvas = tk.Canvas(options_container, bg="light gray", highlightthickness=0)
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        # Add a vertical scrollbar to the container
        vsb = tk.Scrollbar(options_container, orient="vertical", command=canvas.yview)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)
        canvas.configure(yscrollcommand=vsb.set)

        # Create a frame inside the canvas to hold the radio buttons
        options_frame = tk.Frame(canvas, bg="light gray")
        canvas.create_window((0,0), window=options_frame, anchor="nw")

        # Update the scroll region when the frame's size changes
        def on_frame_configure(event):
            canvas.configure(scrollregion=canvas.bbox("all"))
        options_frame.bind("<Configure>", on_frame_configure)
        # --- End scrollable area creation ---

        # Create radio buttons with option text as "Option {number}: {summary}"
        for idx, entry in enumerate(word_entries):
            summary = " | ".join([
                self._norm_get(entry, "\ufeffVowel Ending") or "",
                self._norm_get(entry, "Number / ‡®µ‡®ö‡®®") or "",
                self._norm_get(entry, "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£") or "",
                self._norm_get(entry, "Gender / ‡®≤‡®ø‡©∞‡®ó") or "",
                self._norm_get(entry, "Word Root") or "",
                self._norm_get(entry, "Type") or ""
            ])
            rb_text = f"Option {idx+1}: {summary}"
            rb = tk.Radiobutton(options_frame,
                                text=rb_text,
                                variable=choice_var,
                                value=idx,
                                bg="light gray",
                                font=("Helvetica", 12),
                                anchor='w',
                                justify=tk.LEFT,
                                selectcolor='light blue')
            rb.pack(anchor="w", padx=10, pady=5)

        def on_save():
            selected_index = choice_var.get()
            nonlocal final_choice
            final_choice = word_entries[selected_index]
            final_win.destroy()

        save_btn = tk.Button(final_win, text="Save Choice",
                            command=on_save,
                            font=("Helvetica", 14, "bold"),
                            bg="#007acc", fg="white", padx=20, pady=10)
        save_btn.pack(pady=20)

        final_win.transient(self.root)
        final_win.grab_set()
        self.root.wait_window(final_win)
        return final_choice

    def prompt_save_results(self, new_entries, skip_copy=False):
        """
        For each verse in self.selected_verses, prompts the user to save new entries (accumulated from all words),
        checking for duplicates first. Then opens a modal prompt for assessment and saves the finalized data
        (including verse-level metadata) to an Excel file.
        """
        def _s(val):
            if val is None:
                return ""
            return unicodedata.normalize("NFC", str(val).strip())

        file_path = "1.2.1 assessment_data.xlsx"
        existing_data = self.load_existing_assessment_data(file_path)
        
        # Save the original accumulated_pankti so it can be restored later.
        original_accumulated_pankti = self.accumulated_pankti

        # Process each verse in the selected verses
        for verse in self.selected_verses:
            # Update the current verse for processing.
            self.accumulated_pankti = verse
            verse_norm = _s(verse)

            # reset repeat-note tracking for this verse
            self._repeat_note_shown = set()
            self._suppress_repeat_notes_for_verse = False

            # normalize for repeat-note consistency
            verse_key = unicodedata.normalize(
                "NFC", re.sub(r"\s+", " ", verse_norm.replace('‡••', '').strip())
            )
            current_verse_words = verse_key.split()

            # Normalize tokens once and use everywhere to avoid Unicode/spacing mismatches
            normalized_tokens = [_s(w) for w in current_verse_words]
            normalized_words = set(normalized_tokens)

            from collections import Counter
            word_counts = Counter(normalized_tokens)

            # Filter new_entries to only those whose "Word" is present in the current verse.
            filtered_new_entries = [
                entry for entry in new_entries
                if _s(entry.get("Word")) in normalized_words and _s(entry.get("Verse")) == verse_norm
            ]

            duplicate_entries = []
            unique_entries = []

            # Duplicate check: for each filtered entry, compare against existing data.
            for new_entry in filtered_new_entries:
                new_word = self._norm_get(new_entry, "Word")
                new_ve = self._norm_get(new_entry, "\ufeffVowel Ending")
                new_num = self._norm_get(new_entry, "Number / ‡®µ‡®ö‡®®")
                new_grammar = self._norm_get(new_entry, "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£")
                new_gender = self._norm_get(new_entry, "Gender / ‡®≤‡®ø‡©∞‡®ó")
                new_root = self._norm_get(new_entry, "Word Root")
                new_type = self._norm_get(new_entry, "Type")
                new_verse = self._norm_get(new_entry, "Verse")

                if any(
                    new_word == self._norm_get(existing_entry, "Word") and
                    new_ve == self._norm_get(existing_entry, "\ufeffVowel Ending") and
                    new_num == self._norm_get(existing_entry, "Number / ‡®µ‡®ö‡®®") and
                    new_grammar == self._norm_get(existing_entry, "Grammar / ‡®µ‡®Ø‡®æ‡®ï‡®∞‡®£") and
                    new_gender == self._norm_get(existing_entry, "Gender / ‡®≤‡®ø‡©∞‡®ó") and
                    new_root == self._norm_get(existing_entry, "Word Root") and
                    new_type == self._norm_get(existing_entry, "Type") and
                    new_verse == self._norm_get(existing_entry, "Verse")
                    for existing_entry in existing_data.to_dict('records')
                ):
                    duplicate_entries.append(new_entry)
                else:
                    unique_entries.append(new_entry)

            if duplicate_entries:
                duplicate_message = "Some entries are already present:\n" + "\n".join(map(str, duplicate_entries))
                messagebox.showinfo("Duplicates Found", duplicate_message)

            if not skip_copy:
                self.prompt_copy_to_clipboard()

            if unique_entries:
                save = messagebox.askyesno(
                    "Save Results",
                    f"Would you like to save the new entries for the following verse?\n\n{verse_norm}"
                )
                if save:
                    # Open one assessment prompt for the current verse.
                    assessment_data = self.prompt_for_assessment_once()

                    # --- Extract verse metadata from candidate matches ---
                    verse_to_match = _s(self.accumulated_pankti)
                    candidate = None
                    if hasattr(self, 'candidate_matches') and hasattr(self, 'chosen_match') and self.chosen_match:
                        for cand in self.chosen_match:
                            if _s(cand.get("Verse")) == verse_to_match:
                                candidate = cand
                                break
                        if candidate is None:
                            candidate = self.chosen_match[0]
                        verse_metadata = {
                            "Verse": verse_to_match,
                            "S. No.": candidate.get("S. No.", ""),
                            "Verse No.": candidate.get("Verse No.", ""),
                            "Stanza No.": candidate.get("Stanza No.", ""),
                            "Text Set No.": candidate.get("Text Set No.", ""),
                            "Raag (Fixed)": candidate.get("Raag (Fixed)", ""),
                            "Sub-Raag": candidate.get("Sub-Raag", ""),
                            "Writer (Fixed)": candidate.get("Writer (Fixed)", ""),
                            "Verse Configuration (Optional)": candidate.get("Verse Configuration (Optional)", ""),
                            "Stanza Configuration (Optional)": candidate.get("Stanza Configuration (Optional)", ""),
                            "Bani Name": candidate.get("Bani Name", ""),
                            "Musical Note Configuration": candidate.get("Musical Note Configuration", ""),
                            "Special Type Demonstrator": candidate.get("Special Type Demonstrator", ""),
                            "Verse Type": candidate.get("Type", ""),
                            "Page Number": candidate.get("Page Number", "")
                        }
                    else:
                        verse_metadata = {}
                    # -------------------------------------------------------

                    # --- Finalize grammar options per word (handling repeated words by occurrence order via clustering) ---
                    # --- Group unique_entries by word ---
                    word_groups = {}
                    for entry in unique_entries:
                        word = _s(entry["Word"])
                        word_groups.setdefault(word, []).append(entry)
                        
                    final_entries = []
                    occurrence_mapping = {}  # Mapping from (word, occurrence_position) to list of entries (options)

                    # For each unique word in the current verse, partition its entries into clusters based on occurrence count.
                    for word in set(normalized_tokens):
                        count = word_counts[word]
                        if word not in word_groups:
                            continue
                        entries_list = word_groups[word]  # all unique entries for that word (in sequence)
                        n = len(entries_list)
                        k = count  # expected number of clusters
                        groups = []
                        start = 0
                        # Partition entries_list into k groups using a chunking method.
                        # If n isn't exactly divisible by k, distribute the remainder to the first few groups.
                        group_size = n // k
                        remainder = n % k
                        for i in range(k):
                            size = group_size + (1 if i < remainder else 0)
                            group = entries_list[start:start+size]
                            groups.append(group)
                            start += size
                        # Find the indices (positions) in normalized_tokens where this word occurs, in order.
                        occurrence_positions = [i for i, w in enumerate(normalized_tokens) if w == word]
                        for occ, pos in zip(range(k), occurrence_positions):
                            occurrence_mapping[(word, pos)] = groups[occ]

                    occurrence_counters = {}
                    # Now, iterate over normalized_tokens (which are in order) and process each occurrence.
                    for idx, word in enumerate(normalized_tokens):
                        occ_idx = occurrence_counters.get(word, 0)
                        occurrence_counters[word] = occ_idx + 1
                        key = (word, idx)  # Unique key for the occurrence at position idx.
                        entries = occurrence_mapping.get(key, [])
                        if not entries:
                            continue  # No entries for this occurrence.

                        if word_counts.get(word, 0) > 1 and occ_idx > 0:
                            if not getattr(self, "_use_inline_literal_banner", True):
                                self._maybe_show_repeat_important_note(word, occ_idx, verse_key)
                            else:
                                self._repeat_note_shown.add((verse_key, word, "second"))

                        # Remove duplicate entries within this occurrence group (local deduplication).
                        dedup_entries = []
                        seen = set()
                        for entry in entries:
                            entry_tuple = tuple(sorted(entry.items()))
                            if entry_tuple not in seen:
                                seen.add(entry_tuple)
                                dedup_entries.append(entry)
                        entries = dedup_entries

                        # If more than one unique option exists for this occurrence, prompt the user to choose.
                        if len(entries) > 1:
                            chosen_entry = self.prompt_for_final_grammar(entries)
                        else:
                            chosen_entry = entries[0]

                        # Capture the occurrence index.
                        chosen_entry['Word Index'] = idx

                        # Ensure self.accumulated_finalized_matches is long enough.
                        if len(self.accumulated_finalized_matches) <= idx:
                            self.accumulated_finalized_matches.extend([[]] * (idx - len(self.accumulated_finalized_matches) + 1))
                        self.accumulated_finalized_matches[idx] = [chosen_entry]
                        final_entries.append(chosen_entry)
                    # -------------------------------------------------------

                    # Now update each finalized entry with the assessment and verse metadata, then save.
                    for entry in final_entries:
                        entry.update(assessment_data)
                        entry.update(verse_metadata)
                        self.save_assessment_data(entry)
                    messagebox.showinfo("Saved", "Assessment data saved successfully for verse:\n" + verse_norm)

        # Restore the original accumulated_pankti after processing all verses.
        self.accumulated_pankti = original_accumulated_pankti

        if hasattr(self, 'copy_button') and self.copy_button.winfo_exists():
            self.copy_button.config(state=tk.NORMAL)

    def prompt_for_assessment_once(self):
        """Opens a modal prompt for the entire verse assessment and returns the collected data."""
        assessment_win = tk.Toplevel(self.root)
        assessment_win.title(f"Enter Translation Assessment for: '{self.accumulated_pankti}'")
        assessment_win.configure(bg='light gray')

        instruction_label = tk.Label(
            assessment_win, 
            text="Paste the analysis result for the entire verse below:",
            font=("Helvetica", 14), bg="light gray"
        )
        instruction_label.pack(pady=10)

        analysis_text = scrolledtext.ScrolledText(assessment_win, width=80, height=10,
                                                font=("Helvetica", 12), wrap=tk.WORD)
        analysis_text.pack(padx=20, pady=10)

        cb_frame = tk.Frame(assessment_win, bg="light gray")
        cb_frame.pack(pady=10)

        framework_var = tk.BooleanVar()
        explicit_var = tk.BooleanVar()

        framework_cb = tk.Checkbutton(cb_frame, text="Framework?", variable=framework_var,
                                    font=("Helvetica", 12), bg="light gray")
        framework_cb.pack(side=tk.LEFT, padx=10)

        explicit_cb = tk.Checkbutton(cb_frame, text="Explicit?", variable=explicit_var,
                                    font=("Helvetica", 12), bg="light gray")
        explicit_cb.pack(side=tk.LEFT, padx=10)

        assessment_data = {}

        def on_save():
            translation = analysis_text.get("1.0", tk.END).strip()
            if not translation:
                messagebox.showerror("Error", "Please paste the analysis result.")
                return
            assessment_data["Translation"] = translation
            assessment_data["Framework?"] = framework_var.get()
            assessment_data["Explicit?"] = explicit_var.get()
            assessment_win.destroy()

        save_btn = tk.Button(assessment_win, text="Save Assessment",
                            command=on_save, font=("Helvetica", 14, "bold"),
                            bg="#007acc", fg="white", padx=20, pady=10)
        save_btn.pack(pady=20)

        assessment_win.transient(self.root)
        assessment_win.grab_set()
        self.root.wait_window(assessment_win)

        return assessment_data

    def accumulate_pankti_data(self, pankti):
        self.accumulated_pankti = pankti

    def accumulate_meanings_data(self, meanings):
        """
        Accumulate the meanings for the current word, along with the word itself and its index.
        This creates a mapping for each word occurrence.
        """
        # Ensure the list is long enough for the current word index.
        while len(self.accumulated_meanings) <= self.current_word_index:
            self.accumulated_meanings.append({"word": None, "meanings": []})
        
        # Store the word if it hasn't been stored yet.
        if self.accumulated_meanings[self.current_word_index]["word"] is None:
            # Assuming self.pankti_words is defined and holds the current verse's words.
            self.accumulated_meanings[self.current_word_index]["word"] = self.pankti_words[self.current_word_index]
        
        # Update the meanings for the current word occurrence.
        self.accumulated_meanings[self.current_word_index]["meanings"] = meanings

    def accumulate_grammar_matches(self, matches):
        self.accumulated_grammar_matches.append(matches)

    def accumulate_finalized_matches(self, finalized_matches):
        # Ensure the list is long enough to hold the current index
        if len(self.accumulated_finalized_matches) <= self.current_word_index:
            # Expand the list to the required length with empty lists
            self.accumulated_finalized_matches.extend([[]] * (self.current_word_index - len(self.accumulated_finalized_matches) + 1))
        
        # Store the finalized matches at the correct index
        self.accumulated_finalized_matches[self.current_word_index] = finalized_matches

    def start_progress(self):
        self.progress_window = tk.Toplevel(self.root)
        self.progress_window.title("Please Wait...")
        self.progress_window.geometry("350x120")
        self.progress_window.resizable(False, False)
        self.progress_window.attributes("-topmost", True)
        self.progress_window.configure(bg="#f0f0f0")
        self.progress_window.attributes('-alpha', 0.0)  # Start fully transparent

        # Center the progress window
        self.progress_window.update_idletasks()
        x = (self.progress_window.winfo_screenwidth() - self.progress_window.winfo_width()) // 2
        y = (self.progress_window.winfo_screenheight() - self.progress_window.winfo_height()) // 3
        self.progress_window.geometry(f"+{x}+{y}")

        # Fade-in effect
        def fade_in(window, alpha=0.0):
            alpha = round(alpha + 0.05, 2)
            if alpha <= 1.0:
                window.attributes('-alpha', alpha)
                window.after(30, lambda: fade_in(window, alpha))

        fade_in(self.progress_window)

        # Custom style
        style = ttk.Style(self.progress_window)
        style.theme_use('default')

        style.configure("Custom.Horizontal.TProgressbar",
                        troughcolor="#e0e0e0",
                        bordercolor="#e0e0e0",
                        background="#4a90e2",
                        lightcolor="#4a90e2",
                        darkcolor="#4a90e2",
                        thickness=20)

        label = ttk.Label(self.progress_window, text="Processing, please wait...", background="#f0f0f0", font=("Segoe UI", 10))
        label.pack(pady=(20, 5))

        self.progress_bar = ttk.Progressbar(self.progress_window,
                                            mode='indeterminate',
                                            style="Custom.Horizontal.TProgressbar")
        self.progress_bar.pack(padx=30, fill=tk.X)
        self.progress_bar.start(7)

        self.root.update_idletasks()

    def stop_progress(self):
        self.progress_bar.stop()
        self.progress_window.destroy()

    def handle_submitted_input(self, word):
        # Implement your logic for handling the submitted input here
        print(f"Handling submitted input for: {word}")
        # You can add your logic here, such as saving or processing the input further

    def filter_unique_matches(self, matches):
        """
        Eliminate duplicates based on the grammatical parts and return unique matches.

        Args:
        matches (list): The list of matches to filter.

        Returns:
        list: The list of unique matches.
        """
        # Sort matches by match_count first and then by match_percentage, both in descending order
        matches_sorted = sorted(matches, key=lambda x: (x[1], x[2]), reverse=True)

        unique_matches = []
        seen_grammar = set()
        for match in matches_sorted:
            grammar_info = match[0].split(" | ")[2:]  # Extract grammar info (e.g., Number, Gender, POS, etc.)
            grammar_tuple = tuple(grammar_info)
            if grammar_tuple not in seen_grammar:
                unique_matches.append(match)
                seen_grammar.add(grammar_tuple)

        # Accumulate Grammar Matches data
        self.accumulate_grammar_matches(unique_matches)
        return unique_matches

    def perform_dictionary_lookup(self, word, callback):
        """
        Runs the dictionary lookup (with its progress bar) in a separate thread.
        Once the lookup completes, the callback is called on the main thread with the lookup result.
        """
        def lookup_worker():
            # Call your lookup function (which already shows the progress bar)
            meanings = self.lookup_word_in_dictionary(word)
            # Schedule the callback on the main thread with the obtained meanings
            self.root.after(0, lambda: callback(meanings))
        threading.Thread(target=lookup_worker, daemon=True).start()

    def handle_lookup_results(self, matches, meanings):
        """
        Called when the dictionary lookup is finished.
        'matches' are those computed earlier and 'meanings' is the result from the lookup.
        This method updates the UI by calling show_matches.
        """
        print(f"Lookup completed for {self.pankti_words[self.current_word_index]}. Meanings: {meanings}")
        self.show_matches(matches, self.current_pankti, meanings)

    def calculate_match_metrics(self, word, vowel_ending):
        """
        Calculates the number of matching characters from the end of vowel_ending and word, 
        and the percentage of matching characters with respect to the matched part of vowel_ending.
        
        Parameters:
        word (str): The word to be compared.
        vowel_ending (str): The vowel ending to be compared, possibly containing multiple parts.
        
        Returns:
        tuple: A tuple containing:
            - match_count (int): Number of matching characters from the end.
            - match_percentage (float): Percentage of matching characters based on the matched part of vowel_ending.
        """
        word_chars = list(word)  # Convert word to a list of characters
        vowel_parts = vowel_ending.split()  # Split vowel ending into parts

        total_match_count = 0
        max_match_percentage = 0.0

        # Iterate through each part of the vowel ending
        for part in vowel_parts:
            part_chars = list(part)  # Convert each part to a list of characters

            match_count = 0

            # Reverse iterate through both word and part to compare characters from the end
            for i in range(1, min(len(word_chars), len(part_chars)) + 1):
                if word_chars[-i] == part_chars[-i]:
                    match_count += 1
                else:
                    break  # Stop when characters no longer match

            if match_count > 0:
                # Calculate match percentage based on the length of the matched part
                match_percentage = (match_count / len(part_chars)) * 100
                total_match_count += match_count
                
                # Track the highest match percentage found
                if match_percentage > max_match_percentage:
                    max_match_percentage = match_percentage

            # Stop if there was any match in the current part
            if match_count > 0:
                break

        return total_match_count, max_match_percentage


if __name__ == "__main__":
    root = tk.Tk()
    app = GrammarApp(root)
    root.mainloop()